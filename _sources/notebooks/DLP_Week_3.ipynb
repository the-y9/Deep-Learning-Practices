{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3 - Large Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1\n",
    "<b>Download the Yelp review dataset “Yelp/yelp_review_full”. Split each sample by calling the string method “.split()” and choose the correct statements about the dataset.\n",
    "\n",
    "- A. The dataset contains close to 99 million words\n",
    "- B. There are more than 300 samples that contain a single word\n",
    "- C. There are less than 300 samples that contain only a single word\n",
    "- D. “Cheesy-melty-roasted-cauliflower-with-fresh-bread-crumbs-on top.\\\\nTo-die-for.” is one of the single words in the dataset\n",
    "- E. The average length of a sample is 134.1\n",
    "- F. The distribution of the length of the samples is right skewed\n",
    "<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['label', 'text'],\n",
      "    num_rows: 700000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "try:\n",
    "    ds = load_dataset(\"Yelp/yelp_review_full\", split=\"all\")\n",
    "    print(ds)\n",
    "except Exception as e:\n",
    "    print(\"ERROR:\",e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of processors = 12\n",
      "Dataset({\n",
      "    features: ['label', 'text', 'num_word'],\n",
      "    num_rows: 700000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "procs = os.cpu_count()\n",
    "print(\"No. of processors =\", os.cpu_count())\n",
    "\n",
    "def modify(sample):\n",
    "    sample['num_word'] = len(sample['text'].split())\n",
    "    return sample\n",
    "ds = ds.map(modify, batch_size=7000, num_proc=procs)    \n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total = 93878307\n",
      "samples > 1 word = 355\n",
      "average sample length = 134.11186714285714\n",
      "has Cheesy-melty-roasted-cauliflower-with-fresh-bread-crumbs-on-top.\\nTo-die-for. = 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "try:\n",
    "    num_words = ds['num_word']\n",
    "    total_words = sum(num_words)\n",
    "    print(f\"total = {total_words}\")\n",
    "    print(f\"samples > 1 word = {np.sum(np.array(num_words)==1)}\")\n",
    "    print(f\"average sample length = {total_words/ds.num_rows}\")\n",
    "    st = 'Cheesy-melty-roasted-cauliflower-with-fresh-bread-crumbs-on-top.\\\\nTo-die-for.'\n",
    "    fil_ds = ds.filter(lambda sample, st=st: sample['text'] == st, num_proc=procs)\n",
    "    print(f\"has {st} = {len(fil_ds)}\")\n",
    "    plt.figure(figsize=(5,2))\n",
    "    plt.hist(num_words)\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(\"ERROR:\",e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2\n",
    "<b>Load the “bert-base-uncased” pre-trained tokenizer and choose the correct statements about the tokenizer.\n",
    "- A. The tokenizer is used for the BERT model with the context length of 512\n",
    "- B. The tokenizer has 5 special tokens\n",
    "- C. Tokenizing a sample that contains more than 512 words would result in truncation of all tokens beyond the length 512\n",
    "- D. Tokenizer inserts all the special tokens when it processes a single sample as an input\n",
    "- E. Tokenizer inserts [CLS] and [SEP] special tokens when it processes a single sample as an input\n",
    "- F. Tokenizer inserts only [CLS]special token when it processes a single sample as an input\n",
    "<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")\n",
      "B. True\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from pprint import pprint\n",
    "try:\n",
    "    bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "    pprint(bert_tokenizer)\n",
    "    print(f\"B. {len(bert_tokenizer.all_special_tokens) == 5}\")\n",
    "except Exception as e:\n",
    "    print(\"ERROR:\",e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1019 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['label', 'text', 'num_word'],\n",
      "    num_rows: 12441\n",
      "})\n",
      "{'label': 4, 'text': \"After a morning of Thrift Store hunting, a friend and I were thinking of lunch, and he suggested Emil's after he'd seen Chris Sebak do a bit on it and had tried it a time or two before, and I had not. He said they had a decent Reuben, but to be prepared to step back in time.\\\\n\\\\nWell, seeing as how I'm kind of addicted to late 40's and early 50's, and the whole Rat Pack scene, stepping back in time is a welcomed change in da burgh...as long as it doesn't involve 1979, which I can see all around me every day.\\\\n\\\\nAnd yet another shot at finding a decent Reuben in da burgh...well, that's like hunting the Holy Grail. So looking under one more bush certainly wouldn't hurt.\\\\n\\\\nSo off we go right at lunchtime in the middle of...where exactly were we? At first I thought we were lost, driving around a handful of very rather dismal looking blocks in what looked like a neighborhood that had been blighted by the building of a highway. And then...AHA! Here it is! And yep, there it was. This little unassuming building with an add-on entrance with what looked like a very old hand painted sign stating quite simply 'Emil's. \\\\n\\\\nWe walked in the front door, and entered another world. Another time, and another place. Oh, and any Big Burrito/Sousa foodies might as well stop reading now. I wouldn't want to see you walk in, roll your eyes and say 'Reaaaaaalllly?'\\\\n\\\\nThis is about as old world bar/lounge/restaurant as it gets. Plain, with a dark wood bar on one side, plain white walls with no yinzer pics, good sturdy chairs and actual white linens on the tables. This is the kind of neighborhood dive that I could see Frank and Dino pulling a few tables together for some poker, a fish sammich, and some cheap scotch. And THAT is exactly what I love.\\\\n\\\\nOh...but good food counts too. \\\\n\\\\nWe each had a Reuben, and my friend had a side of fries. The Reubens were decent, but not NY awesome. A little too thick on the bread, but overall, tasty and definitely filling. Not too skimpy on the meat. I seriously CRAVE a true, good NY Reuben, but since I can't afford to travel right now, what I find in da burgh will have to do. But as we sat and ate, burgers came out to an adjoining table. Those were some big thick burgers. A steak went past for the table behind us. That was HUGE! And when we asked about it, the waitress said 'Yeah, it's huge and really good, and he only charges $12.99 for it, ain't that nuts?' Another table of five came in, and wham. Fish sandwiches PILED with breaded fish that looked amazing. Yeah, I want that, that, that and THAT!\\\\n\\\\nMy friend also mentioned that they have a Chicken Parm special one day of the week that is only served UNTIL 4 pm, and that it is fantastic. If only I could GET there on that week day before 4...\\\\n\\\\nThe waitress did a good job, especially since there was quite a growing crowd at lunchtime on a Saturday, and only one of her. She kept up and was very friendly. \\\\n\\\\nThey only have Pepsi products, so I had a brewed iced tea, which was very fresh, and she did pop by to ask about refills as often as she could. As the lunch hour went on, they were getting busy.\\\\n\\\\nEmil's is no frills, good portions, very reasonable prices, VERY comfortable neighborhood hole in the wall...kind of like Cheers, but in a blue collar neighborhood in the 1950's. Fan-freakin-tastic! I could feel at home here.\\\\n\\\\nYou definitely want to hit Mapquest or plug in your GPS though. I am not sure that I could find it again on my own...it really is a hidden gem. I will be making my friend take me back until I can memorize where the heck it is.\\\\n\\\\nAddendum: 2nd visit for the fish sandwich. Excellent. Truly. A pound of fish on a fish-shaped bun (as opposed to da burgh's seemingly popular hamburger bun). The fish was flavorful, the batter excellent, and for just $8. This may have been the best fish sandwich I've yet to have in da burgh.\", 'num_word': 712}\n",
      "1019\n"
     ]
    }
   ],
   "source": [
    "s_ds = ds.filter(lambda sample: sample['num_word'] > 512)\n",
    "print(s_ds)\n",
    "\n",
    "example = s_ds[0]\n",
    "print(example)\n",
    "\n",
    "tokenized_sample = bert_tokenizer(example['text'])\n",
    "print(len(tokenized_sample['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'single', 'sample', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "example = \"single sample\"\n",
    "tokens = bert_tokenizer.convert_ids_to_tokens(bert_tokenizer(example)['input_ids'])\n",
    "print(tokens)                                         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3\n",
    "<b>Use “BertConfig” and “BertForMaskedLM” to construct the default (original) BERT model. Choose the correct statements\n",
    "- A. The model has 12 Bert layers\n",
    "- B. The model has 6 Bert layers\n",
    "- C. The model uses absolute position embeddings\n",
    "- D. The word embedding (token embedding) layer has about 23 million learnable parameters\n",
    "- E. The total number of parameters in the model is close to 110 million<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.48.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "BertForMaskedLM(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (cls): BertOnlyMLMHead(\n",
      "    (predictions): BertLMPredictionHead(\n",
      "      (transform): BertPredictionHeadTransform(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (transform_act_fn): GELUActivation()\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig, BertForMaskedLM\n",
    "config = BertConfig()\n",
    "pprint(config)\n",
    "print('-'*100)\n",
    "model = BertForMaskedLM(config)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109514298\n"
     ]
    }
   ],
   "source": [
    "params = 0\n",
    "for param in model.parameters():\n",
    "    params += param.numel()\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132801024"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(30522*768 + 512*768 + 2*768) + (3*768*768 + 768*768 + 768*3072 + 3072*768)*12 + (768*768 + 768*30522)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4\n",
    "<b>Double the context length from 512 to 1024 (you can change it in the configuration). Count the number of parameters and enter the change in the number of parameters (in millions) compared to the default configuration.<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109907514\n",
      "0.393216\n"
     ]
    }
   ],
   "source": [
    "config2 = BertConfig(max_position_embeddings=1024)\n",
    "model2 = BertForMaskedLM(config2)\n",
    "params2 = 0\n",
    "for param in model2.parameters():\n",
    "    params2 += param.numel()\n",
    "print(params2)\n",
    "print((params2-params)/10**6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5\n",
    "<b>Pack (chunk) the samples such that the length of all the samples in the dataset is 512 (for efficient training). Define a mapping function that implements the following procedure\n",
    " 1. Take a batch of 1000 samples\n",
    " 2. Tokenize it to get input IDs and attention mask\n",
    " 3. Concatenate all the input IDs\n",
    " 4. Chunk the concatenated IDs into a size of 512\n",
    " 5. Drop the last chunk if its length is less than 512\n",
    " 6. Pack all the chunks\n",
    " 7. Iterate over all the batches in the dataset \n",
    " \n",
    "Store the resulting dataset in the variable “ds_chunked”. Enter the total number of samples in the new dataset.\n",
    "Note: the batch size should be kept at 1000 while calling \"ds.map()\" for theanswer to match.\n",
    "<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=12): 100%|██████████| 700000/700000 [10:51<00:00, 1074.69 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def chunk(samples, tokenizer = bert_tokenizer):\n",
    "    samples = tokenizer(samples['text'])\n",
    "    concat_inps = sum([seq for seq in samples['input_ids']],[])\n",
    "    total_tokens = len(concat_inps)\n",
    "    num_contx_samples = total_tokens // 512\n",
    "\n",
    "    inp_id_list, att_mask_list = [], []\n",
    "    for i in range(0, num_contx_samples):\n",
    "        inp_ids = concat_inps[i*512:(i+1)*512]\n",
    "        att_masks = [1]*512\n",
    "        inp_id_list.append(inp_ids)\n",
    "        att_mask_list.append(att_masks)\n",
    "    \n",
    "    return {'input_ids': inp_id_list, 'attention_mask': att_mask_list}\n",
    "\n",
    "ds_chunked = ds.map(chunk,\n",
    "    batch_size=1000,\n",
    "    batched = True,\n",
    "    num_proc = procs,\n",
    "    remove_columns = ['label', 'text', 'num_word'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 246703\n",
       "})"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_chunked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6\n",
    "<b>\n",
    "\n",
    "- Split the new dataset into training and test sets with the test_size=0.05 and seed=42.\n",
    "- Use the appropriate data collator function for the MLM objective and set the masking probability to 0.2. \n",
    "- Use the data loader from PyTorch to load a batch of samples, and enter the token ID corresponding to the unmasked token\n",
    "</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]),\n",
      " 'input_ids': tensor([[ 103, 2070, 2052,  ..., 2015, 1010,  103],\n",
      "        [2074,  103, 2001,  ..., 1013, 7479, 1012]]),\n",
      " 'labels': tensor([[3241, -100, -100,  ..., -100, -100, 5404],\n",
      "        [-100, 2003, -100,  ..., -100, -100, -100]])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "ds_split = ds_chunked.train_test_split(test_size = 0.5, seed=42)\n",
    "data_collator = DataCollatorForLanguageModeling(bert_tokenizer,\n",
    "                mlm = True,\n",
    "                mlm_probability = 0.2)\n",
    "dataloader = DataLoader(dataset = ds_split['train'],\n",
    "                collate_fn = data_collator,\n",
    "                batch_size=2)\n",
    "pprint(next(iter(dataloader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7\n",
    "<b>Create a small BERT model by changing the following hyper-parameters and keeping the other hyper-parameters as it is\n",
    "- num_hidden_layers = 6\n",
    "- hidden size: 384\n",
    "- intermediate_size: 1536\n",
    "\n",
    "and start training the model with a batch of size 8 for an epoch. What is the loss value at the end of the training?\n",
    "\n",
    "Note: You may optionally save the checkpoints for every N-th step.<b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Define bert Config\n",
    "2. load bert\n",
    "3. tokenize ds\n",
    "4. Data collator\n",
    "5. Data Loader\n",
    "6. Setup (optimize)\n",
    "7. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config_small = BertConfig(num_hidden_layers=6,\n",
    "#                             hidden_size = 384,\n",
    "#                             intermediate_size=1536)\n",
    "\n",
    "# bert_small = BertForMaskedLM(config_small)\n",
    "# print(bert_small)\n",
    "\n",
    "# def tokenize(samples, tokenizer=bert_tokenizer):\n",
    "#     return tokenizer(samples['text'])\n",
    "\n",
    "# train_ds = ds.map(tokenize, batch_size=8, batched=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'text', 'num_word'],\n",
       "    num_rows: 700000\n",
       "})"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
