
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Week 2 &#8212; Deep Learning Practices</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'notebooks/DLP_Week_2';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="Week 1" href="DLP_Week_1.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../README.html">
  
  
  
  
  
  
    <p class="title logo__title">Deep Learning Practices</p>
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../README.html">
                    Deep-Learning-Practices Notes
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="DLP_Week_1.html">Week 1</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Week 2</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/the-y9/Deep-Learning-Practices" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/the-y9/Deep-Learning-Practices/issues/new?title=Issue%20on%20page%20%2Fnotebooks/DLP_Week_2.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/notebooks/DLP_Week_2.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Week 2</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tokenization">Tokenization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#byte-pair-encoding">Byte Pair Encoding</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#word-piece-tokenizer">Word-Piece Tokenizer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#practice">Practice</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#load-dataset">Load Dataset</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tokenize">Tokenize</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vocabulary">Vocabulary</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#encoding">Encoding</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#batch-encoding">Batch Encoding</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#testing">Testing</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#assignment">Assignment</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="week-2">
<h1>Week 2<a class="headerlink" href="#week-2" title="Link to this heading">#</a></h1>
<section id="tokenization">
<h2>Tokenization<a class="headerlink" href="#tokenization" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>Tokenizer → 2. Token to Id → 3. embeddings → 4. Language Model → 5. Id to Token → 6. Token to words</p></li>
</ol>
<p>Encoder [1-2]
Decoder [5-6]</p>
</section>
<section id="byte-pair-encoding">
<h2>Byte Pair Encoding<a class="headerlink" href="#byte-pair-encoding" title="Link to this heading">#</a></h2>
<p>Small Vocabulary → Large Sequence Length
Large Vocabulary → Problem in Computing Softmax
Encodes language without spaces
→ Based on Frequency
→ Fertility → No. of subwords broken out from a word
→ 1 Merge = 1 Addition to Vocabulary</p>
</section>
<section id="word-piece-tokenizer">
<h2>Word-Piece Tokenizer<a class="headerlink" href="#word-piece-tokenizer" title="Link to this heading">#</a></h2>
</section>
<section id="practice">
<h2>Practice<a class="headerlink" href="#practice" title="Link to this heading">#</a></h2>
<section id="load-dataset">
<h3>Load Dataset<a class="headerlink" href="#load-dataset" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_dataset</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
  from .autonotebook import tqdm as notebook_tqdm
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ds</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s1">&#39;bookcorpus&#39;</span><span class="p">,</span><span class="n">split</span><span class="o">=</span><span class="s1">&#39;all&#39;</span><span class="p">,</span> <span class="n">trust_remote_code</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ds</span><span class="p">)</span>
<span class="k">for</span> <span class="n">idx</span><span class="p">,</span><span class="n">sample</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">ds</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">6</span><span class="p">][</span><span class="s1">&#39;text&#39;</span><span class="p">]):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">idx</span><span class="si">}</span><span class="s1"> : </span><span class="si">{</span><span class="n">sample</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:   0%|          | 0.00/1.18G [00:00&lt;?, ?B/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:   0%|          | 49.3k/1.18G [00:00&lt;42:45, 460kB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:   0%|          | 144k/1.18G [00:00&lt;26:48, 733kB/s] 
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:   0%|          | 359k/1.18G [00:00&lt;14:19, 1.37MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:   0%|          | 1.00M/1.18G [00:00&lt;05:54, 3.33MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:   0%|          | 3.15M/1.18G [00:00&lt;02:00, 9.78MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:   1%|          | 8.93M/1.18G [00:00&lt;00:45, 25.8MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:   1%|▏         | 15.8M/1.18G [00:00&lt;00:29, 39.6MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:   2%|▏         | 22.4M/1.18G [00:00&lt;00:24, 47.5MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:   2%|▏         | 29.0M/1.18G [00:00&lt;00:21, 53.1MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:   3%|▎         | 35.4M/1.18G [00:01&lt;00:20, 55.9MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:   4%|▎         | 42.7M/1.18G [00:01&lt;00:18, 60.4MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:   4%|▍         | 49.2M/1.18G [00:01&lt;00:18, 61.8MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:   5%|▍         | 56.0M/1.18G [00:01&lt;00:17, 62.8MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:   5%|▌         | 62.7M/1.18G [00:01&lt;00:17, 63.4MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:   6%|▌         | 69.5M/1.18G [00:01&lt;00:17, 64.5MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:   6%|▋         | 76.0M/1.18G [00:01&lt;00:17, 64.5MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:   7%|▋         | 82.7M/1.18G [00:01&lt;00:16, 64.7MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:   8%|▊         | 89.5M/1.18G [00:01&lt;00:16, 65.0MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:   8%|▊         | 96.3M/1.18G [00:01&lt;00:16, 65.6MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:   9%|▊         | 103M/1.18G [00:02&lt;00:16, 65.5MB/s] 
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:   9%|▉         | 109M/1.18G [00:02&lt;00:16, 65.4MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  10%|▉         | 116M/1.18G [00:02&lt;00:16, 65.3MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  10%|█         | 123M/1.18G [00:02&lt;00:15, 66.1MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  11%|█         | 130M/1.18G [00:02&lt;00:15, 65.9MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  12%|█▏        | 136M/1.18G [00:02&lt;00:15, 65.2MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  12%|█▏        | 143M/1.18G [00:02&lt;00:15, 65.6MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  13%|█▎        | 150M/1.18G [00:02&lt;00:15, 66.3MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  13%|█▎        | 157M/1.18G [00:02&lt;00:15, 66.1MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  14%|█▍        | 163M/1.18G [00:02&lt;00:15, 64.8MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  14%|█▍        | 170M/1.18G [00:03&lt;00:15, 65.4MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  15%|█▌        | 177M/1.18G [00:03&lt;00:14, 67.1MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  16%|█▌        | 184M/1.18G [00:03&lt;00:15, 66.0MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  16%|█▌        | 191M/1.18G [00:03&lt;00:15, 65.5MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  17%|█▋        | 197M/1.18G [00:03&lt;00:15, 65.3MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  17%|█▋        | 204M/1.18G [00:03&lt;00:14, 65.9MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  18%|█▊        | 211M/1.18G [00:03&lt;00:14, 66.1MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  18%|█▊        | 218M/1.18G [00:03&lt;00:14, 65.5MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  19%|█▉        | 224M/1.18G [00:03&lt;00:14, 65.5MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  20%|█▉        | 231M/1.18G [00:03&lt;00:14, 66.1MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  20%|██        | 238M/1.18G [00:04&lt;00:14, 66.6MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  21%|██        | 245M/1.18G [00:04&lt;00:14, 64.8MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  21%|██▏       | 251M/1.18G [00:04&lt;00:14, 65.5MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  22%|██▏       | 258M/1.18G [00:04&lt;00:14, 65.1MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  22%|██▏       | 265M/1.18G [00:04&lt;00:14, 63.9MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  23%|██▎       | 273M/1.18G [00:04&lt;00:14, 64.0MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  24%|██▍       | 280M/1.18G [00:04&lt;00:13, 64.4MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  24%|██▍       | 288M/1.18G [00:04&lt;00:13, 65.0MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  25%|██▌       | 296M/1.18G [00:04&lt;00:13, 65.2MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  26%|██▌       | 303M/1.18G [00:05&lt;00:13, 65.2MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  26%|██▋       | 311M/1.18G [00:05&lt;00:13, 65.5MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  27%|██▋       | 319M/1.18G [00:05&lt;00:12, 68.4MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  28%|██▊       | 326M/1.18G [00:05&lt;00:12, 65.7MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  28%|██▊       | 332M/1.18G [00:05&lt;00:13, 65.0MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  29%|██▉       | 339M/1.18G [00:05&lt;00:12, 66.9MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  29%|██▉       | 346M/1.18G [00:05&lt;00:12, 65.1MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  30%|██▉       | 354M/1.18G [00:05&lt;00:12, 64.3MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  31%|███       | 361M/1.18G [00:05&lt;00:12, 64.8MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  31%|███▏      | 369M/1.18G [00:06&lt;00:12, 65.1MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  32%|███▏      | 377M/1.18G [00:06&lt;00:12, 65.5MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  33%|███▎      | 384M/1.18G [00:06&lt;00:12, 65.6MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  33%|███▎      | 392M/1.18G [00:06&lt;00:11, 65.7MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  34%|███▍      | 400M/1.18G [00:06&lt;00:11, 65.8MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  35%|███▍      | 408M/1.18G [00:06&lt;00:11, 65.9MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  35%|███▌      | 415M/1.18G [00:06&lt;00:11, 65.9MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  36%|███▌      | 423M/1.18G [00:06&lt;00:11, 65.6MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  37%|███▋      | 431M/1.18G [00:07&lt;00:11, 65.4MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  37%|███▋      | 438M/1.18G [00:07&lt;00:11, 65.6MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  38%|███▊      | 446M/1.18G [00:07&lt;00:11, 65.6MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  38%|███▊      | 454M/1.18G [00:07&lt;00:11, 65.7MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  39%|███▉      | 462M/1.18G [00:07&lt;00:10, 65.8MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  40%|███▉      | 469M/1.18G [00:07&lt;00:10, 65.9MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  40%|████      | 477M/1.18G [00:07&lt;00:10, 66.1MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  41%|████      | 485M/1.18G [00:07&lt;00:10, 66.0MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  42%|████▏     | 492M/1.18G [00:07&lt;00:10, 66.0MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  42%|████▏     | 500M/1.18G [00:08&lt;00:10, 65.9MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  43%|████▎     | 508M/1.18G [00:08&lt;00:10, 66.0MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  44%|████▎     | 516M/1.18G [00:08&lt;00:10, 66.0MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  44%|████▍     | 523M/1.18G [00:08&lt;00:09, 66.1MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  45%|████▌     | 531M/1.18G [00:08&lt;00:09, 66.1MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  46%|████▌     | 539M/1.18G [00:08&lt;00:09, 68.8MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  46%|████▋     | 546M/1.18G [00:08&lt;00:09, 67.5MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  47%|████▋     | 552M/1.18G [00:08&lt;00:09, 65.7MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  47%|████▋     | 559M/1.18G [00:08&lt;00:09, 65.1MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  48%|████▊     | 566M/1.18G [00:09&lt;00:09, 64.8MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  49%|████▊     | 574M/1.18G [00:09&lt;00:09, 64.8MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  49%|████▉     | 581M/1.18G [00:09&lt;00:09, 65.0MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  50%|████▉     | 589M/1.18G [00:09&lt;00:09, 65.2MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  51%|█████     | 597M/1.18G [00:09&lt;00:08, 65.4MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  51%|█████     | 604M/1.18G [00:09&lt;00:08, 65.6MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  52%|█████▏    | 611M/1.18G [00:09&lt;00:09, 59.3MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  52%|█████▏    | 617M/1.18G [00:09&lt;00:11, 50.2MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  53%|█████▎    | 625M/1.18G [00:10&lt;00:10, 54.2MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  54%|█████▎    | 632M/1.18G [00:10&lt;00:09, 57.4MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  54%|█████▍    | 638M/1.18G [00:10&lt;00:09, 55.4MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  55%|█████▍    | 644M/1.18G [00:10&lt;00:09, 54.5MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  55%|█████▌    | 650M/1.18G [00:10&lt;00:09, 53.7MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  56%|█████▌    | 658M/1.18G [00:10&lt;00:09, 57.1MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  56%|█████▋    | 666M/1.18G [00:10&lt;00:08, 59.5MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  57%|█████▋    | 674M/1.18G [00:10&lt;00:08, 61.5MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  58%|█████▊    | 681M/1.18G [00:11&lt;00:07, 62.8MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  58%|█████▊    | 689M/1.18G [00:11&lt;00:07, 63.6MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  59%|█████▉    | 697M/1.18G [00:11&lt;00:07, 64.3MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  60%|█████▉    | 705M/1.18G [00:11&lt;00:07, 64.8MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  60%|██████    | 712M/1.18G [00:11&lt;00:07, 65.1MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  61%|██████    | 720M/1.18G [00:11&lt;00:07, 64.9MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  62%|██████▏   | 727M/1.18G [00:11&lt;00:07, 64.0MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  62%|██████▏   | 735M/1.18G [00:11&lt;00:06, 64.6MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  63%|██████▎   | 743M/1.18G [00:11&lt;00:06, 64.8MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  64%|██████▎   | 751M/1.18G [00:12&lt;00:06, 65.2MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  64%|██████▍   | 758M/1.18G [00:12&lt;00:06, 65.4MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  65%|██████▍   | 766M/1.18G [00:12&lt;00:06, 65.5MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  66%|██████▌   | 774M/1.18G [00:12&lt;00:06, 65.7MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  66%|██████▌   | 781M/1.18G [00:12&lt;00:06, 65.3MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  67%|██████▋   | 789M/1.18G [00:12&lt;00:05, 66.0MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  68%|██████▊   | 797M/1.18G [00:12&lt;00:05, 65.8MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  68%|██████▊   | 804M/1.18G [00:12&lt;00:05, 65.8MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  69%|██████▉   | 812M/1.18G [00:13&lt;00:05, 65.9MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  70%|██████▉   | 820M/1.18G [00:13&lt;00:05, 65.5MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  70%|███████   | 828M/1.18G [00:13&lt;00:05, 65.7MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  71%|███████   | 835M/1.18G [00:13&lt;00:05, 65.8MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  71%|███████▏  | 843M/1.18G [00:13&lt;00:05, 65.9MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  72%|███████▏  | 851M/1.18G [00:13&lt;00:04, 66.0MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  73%|███████▎  | 858M/1.18G [00:13&lt;00:04, 66.0MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  73%|███████▎  | 866M/1.18G [00:13&lt;00:04, 68.9MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  74%|███████▍  | 873M/1.18G [00:13&lt;00:04, 67.1MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  75%|███████▍  | 880M/1.18G [00:14&lt;00:04, 66.1MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  75%|███████▌  | 886M/1.18G [00:14&lt;00:04, 64.5MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  76%|███████▌  | 893M/1.18G [00:14&lt;00:04, 65.3MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  76%|███████▋  | 901M/1.18G [00:14&lt;00:04, 65.4MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  77%|███████▋  | 908M/1.18G [00:14&lt;00:04, 67.6MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  78%|███████▊  | 915M/1.18G [00:14&lt;00:04, 65.8MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  78%|███████▊  | 921M/1.18G [00:14&lt;00:03, 64.6MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  79%|███████▊  | 928M/1.18G [00:14&lt;00:03, 63.4MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  79%|███████▉  | 935M/1.18G [00:14&lt;00:03, 66.0MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  80%|███████▉  | 943M/1.18G [00:15&lt;00:03, 68.1MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  81%|████████  | 950M/1.18G [00:15&lt;00:03, 65.4MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  81%|████████  | 956M/1.18G [00:15&lt;00:03, 64.8MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  82%|████████▏ | 963M/1.18G [00:15&lt;00:03, 63.9MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  82%|████████▏ | 970M/1.18G [00:15&lt;00:03, 66.2MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  83%|████████▎ | 977M/1.18G [00:15&lt;00:03, 66.4MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  83%|████████▎ | 984M/1.18G [00:15&lt;00:02, 65.9MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  84%|████████▍ | 990M/1.18G [00:15&lt;00:02, 63.9MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  85%|████████▍ | 997M/1.18G [00:15&lt;00:02, 65.0MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  85%|████████▌ | 1.00G/1.18G [00:15&lt;00:02, 66.9MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  86%|████████▌ | 1.01G/1.18G [00:16&lt;00:02, 66.6MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  86%|████████▋ | 1.02G/1.18G [00:16&lt;00:02, 66.2MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  87%|████████▋ | 1.02G/1.18G [00:16&lt;00:02, 65.7MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  87%|████████▋ | 1.03G/1.18G [00:16&lt;00:02, 66.2MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  88%|████████▊ | 1.04G/1.18G [00:16&lt;00:02, 66.4MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  89%|████████▊ | 1.05G/1.18G [00:16&lt;00:02, 63.3MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  89%|████████▉ | 1.05G/1.18G [00:16&lt;00:02, 60.5MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  90%|████████▉ | 1.06G/1.18G [00:16&lt;00:02, 59.4MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  90%|█████████ | 1.06G/1.18G [00:16&lt;00:01, 58.2MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  91%|█████████ | 1.07G/1.18G [00:17&lt;00:01, 60.4MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  91%|█████████▏| 1.08G/1.18G [00:17&lt;00:01, 62.1MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  92%|█████████▏| 1.09G/1.18G [00:17&lt;00:01, 63.1MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  93%|█████████▎| 1.09G/1.18G [00:17&lt;00:01, 63.8MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  93%|█████████▎| 1.10G/1.18G [00:17&lt;00:01, 64.3MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  94%|█████████▍| 1.11G/1.18G [00:17&lt;00:01, 64.8MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  95%|█████████▍| 1.12G/1.18G [00:17&lt;00:00, 65.0MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  95%|█████████▌| 1.13G/1.18G [00:17&lt;00:00, 65.1MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  96%|█████████▌| 1.13G/1.18G [00:17&lt;00:00, 65.4MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  97%|█████████▋| 1.14G/1.18G [00:18&lt;00:00, 65.6MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  97%|█████████▋| 1.15G/1.18G [00:18&lt;00:00, 65.8MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  98%|█████████▊| 1.16G/1.18G [00:18&lt;00:00, 65.9MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  99%|█████████▊| 1.16G/1.18G [00:18&lt;00:00, 65.9MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data:  99%|█████████▉| 1.17G/1.18G [00:18&lt;00:00, 66.0MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data: 100%|█████████▉| 1.18G/1.18G [00:18&lt;00:00, 65.7MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data: 100%|██████████| 1.18G/1.18G [00:21&lt;00:00, 56.1MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating train split:   0%|          | 0/74004228 [00:00&lt;?, ? examples/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating train split:   0%|          | 9957/74004228 [00:00&lt;13:58, 88258.68 examples/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating train split:   0%|          | 21330/74004228 [00:00&lt;12:01, 102485.89 examples/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating train split:   0%|          | 33993/74004228 [00:00&lt;11:20, 108707.56 examples/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating train split:   0%|          | 49851/74004228 [00:00&lt;11:29, 107275.90 examples/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating train split:   0%|          | 65813/74004228 [00:00&lt;11:35, 106281.04 examples/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating train split:   0%|          | 77198/74004228 [00:00&lt;11:22, 108379.85 examples/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating train split:   0%|          | 93615/74004228 [00:00&lt;11:28, 107416.39 examples/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating train split:   0%|          | 105087/74004228 [00:00&lt;11:16, 109307.27 examples/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating train split:   0%|          | 116717/74004228 [00:01&lt;11:04, 111190.69 examples/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating train split:   0%|          | 132483/74004228 [00:01&lt;11:17, 108956.08 examples/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating train split:   0%|          | 149222/74004228 [00:01&lt;11:25, 107777.29 examples/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating train split:   0%|          | 160489/74004228 [00:01&lt;11:17, 108965.22 examples/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating train split:   0%|          | 177952/74004228 [00:01&lt;11:20, 108462.84 examples/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating train split:   0%|          | 189397/74004228 [00:01&lt;11:12, 109812.68 examples/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating train split:   0%|          | 206956/74004228 [00:01&lt;11:14, 109336.67 examples/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating train split:   0%|          | 218448/74004228 [00:02&lt;11:06, 110674.96 examples/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating train split:   0%|          | 235446/74004228 [00:02&lt;11:14, 109316.18 examples/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating train split:   0%|          | 253763/74004228 [00:02&lt;11:14, 109401.24 examples/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating train split:   0%|          | 265194/74004228 [00:02&lt;11:07, 110519.90 examples/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating train split:   0%|          | 282931/74004228 [00:02&lt;11:13, 109535.36 examples/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating train split:   0%|          | 298748/74004228 [00:02&lt;11:22, 108000.03 examples/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating train split:   0%|          | 316517/74004228 [00:02&lt;11:21, 108191.79 examples/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating train split:   0%|          | 328000/74004228 [00:03&lt;11:12, 109620.44 examples/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating train split:   0%|          | 346026/74004228 [00:03&lt;11:13, 109400.35 examples/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating train split:   0%|          | 361525/74004228 [00:03&lt;11:49, 103750.09 examples/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating train split:   1%|          | 372884/74004228 [00:03&lt;11:35, 105920.56 examples/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating train split:   1%|          | 384001/74004228 [00:03&lt;11:26, 107176.47 examples/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating train split:   1%|          | 399413/74004228 [00:03&lt;12:03, 101707.72 examples/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating train split:   1%|          | 410871/74004228 [00:03&lt;11:42, 104760.97 examples/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating train split:   1%|          | 422304/74004228 [00:03&lt;11:28, 106939.58 examples/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating train split:   1%|          | 439692/74004228 [00:04&lt;11:25, 107312.19 examples/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating train split:   1%|          | 457625/74004228 [00:04&lt;11:23, 107596.66 examples/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating train split:   1%|          | 474730/74004228 [00:04&lt;11:25, 107196.15 examples/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating train split:   1%|          | 486080/74004228 [00:04&lt;11:16, 108614.00 examples/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating train split:   1%|          | 497697/74004228 [00:04&lt;11:05, 110449.52 examples/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating train split:   1%|          | 516018/74004228 [00:04&lt;11:08, 109897.16 examples/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating train split:   1%|          | 527499/74004228 [00:04&lt;11:01, 111065.75 examples/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating train split:   1%|          | 544587/74004228 [00:05&lt;11:09, 109641.35 examples/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating train split:   1%|          | 555799/74004228 [00:05&lt;11:06, 110234.61 examples/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating train split:   1%|          | 566934/74004228 [00:05&lt;11:38, 105135.54 examples/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating train split:   1%|          | 578428/74004228 [00:05&lt;11:23, 107480.41 examples/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating train split:   1%|          | 589954/74004228 [00:05&lt;11:10, 109568.31 examples/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating train split:   1%|          | 601194/74004228 [00:05&lt;11:05, 110344.23 examples/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating train split:   1%|          | 612526/74004228 [00:05&lt;11:00, 111187.00 examples/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating train split:   1%|          | 625415/74004228 [00:05&lt;10:51, 112716.93 examples/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating train split:   1%|          | 642073/74004228 [00:05&lt;11:08, 109692.19 examples/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating train split:   1%|          | 659000/74004228 [00:06&lt;11:17, 108279.12 examples/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating train split:   1%|          | 675047/74004228 [00:06&lt;11:25, 106900.30 examples/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating train split:   1%|          | 686583/74004228 [00:06&lt;11:13, 108881.96 examples/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating train split:   1%|          | 697928/74004228 [00:06&lt;11:06, 110025.41 examples/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating train split:   1%|          | 709341/74004228 [00:06&lt;11:00, 110995.05 examples/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating train split:   1%|          | 726489/74004228 [00:06&lt;11:08, 109679.49 examples/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating train split:   1%|          | 737915/74004228 [00:06&lt;11:01, 110828.86 examples/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating train split:   1%|          | 755044/74004228 [00:06&lt;11:11, 109154.32 examples/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating train split:   1%|          | 766554/74004228 [00:07&lt;11:02, 110611.70 examples/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating train split:   1%|          | 767342/74004228 [00:07&lt;11:16, 108209.50 examples/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">KeyboardInterrupt</span><span class="g g-Whitespace">                         </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">line</span> <span class="mi">1</span>
<span class="ne">----&gt; </span><span class="mi">1</span> <span class="n">ds</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s1">&#39;bookcorpus&#39;</span><span class="p">,</span><span class="n">split</span><span class="o">=</span><span class="s1">&#39;all&#39;</span><span class="p">,</span> <span class="n">trust_remote_code</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span> <span class="nb">print</span><span class="p">(</span><span class="n">ds</span><span class="p">)</span>
<span class="g g-Whitespace">      </span><span class="mi">3</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span><span class="n">sample</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">ds</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">6</span><span class="p">][</span><span class="s1">&#39;text&#39;</span><span class="p">]):</span>

<span class="nn">File /opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/site-packages/datasets/load.py:2151,</span> in <span class="ni">load_dataset</span><span class="nt">(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">2148</span>     <span class="k">return</span> <span class="n">builder_instance</span><span class="o">.</span><span class="n">as_streaming_dataset</span><span class="p">(</span><span class="n">split</span><span class="o">=</span><span class="n">split</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">2150</span> <span class="c1"># Download and prepare data</span>
<span class="ne">-&gt; </span><span class="mi">2151</span> <span class="n">builder_instance</span><span class="o">.</span><span class="n">download_and_prepare</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">2152</span>     <span class="n">download_config</span><span class="o">=</span><span class="n">download_config</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">2153</span>     <span class="n">download_mode</span><span class="o">=</span><span class="n">download_mode</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">2154</span>     <span class="n">verification_mode</span><span class="o">=</span><span class="n">verification_mode</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">2155</span>     <span class="n">num_proc</span><span class="o">=</span><span class="n">num_proc</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">2156</span>     <span class="n">storage_options</span><span class="o">=</span><span class="n">storage_options</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">2157</span> <span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">2159</span> <span class="c1"># Build dataset for splits</span>
<span class="g g-Whitespace">   </span><span class="mi">2160</span> <span class="n">keep_in_memory</span> <span class="o">=</span> <span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">2161</span>     <span class="n">keep_in_memory</span> <span class="k">if</span> <span class="n">keep_in_memory</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">is_small_dataset</span><span class="p">(</span><span class="n">builder_instance</span><span class="o">.</span><span class="n">info</span><span class="o">.</span><span class="n">dataset_size</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">2162</span> <span class="p">)</span>

<span class="nn">File /opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/site-packages/datasets/builder.py:924,</span> in <span class="ni">DatasetBuilder.download_and_prepare</span><span class="nt">(self, output_dir, download_config, download_mode, verification_mode, dl_manager, base_path, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)</span>
<span class="g g-Whitespace">    </span><span class="mi">922</span> <span class="k">if</span> <span class="n">num_proc</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">923</span>     <span class="n">prepare_split_kwargs</span><span class="p">[</span><span class="s2">&quot;num_proc&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">num_proc</span>
<span class="ne">--&gt; </span><span class="mi">924</span> <span class="bp">self</span><span class="o">.</span><span class="n">_download_and_prepare</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">925</span>     <span class="n">dl_manager</span><span class="o">=</span><span class="n">dl_manager</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">926</span>     <span class="n">verification_mode</span><span class="o">=</span><span class="n">verification_mode</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">927</span>     <span class="o">**</span><span class="n">prepare_split_kwargs</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">928</span>     <span class="o">**</span><span class="n">download_and_prepare_kwargs</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">929</span> <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">930</span> <span class="c1"># Sync info</span>
<span class="nn">    931 self.info.dataset_size = sum(split.num_bytes for split</span> in <span class="ni">self.info.splits.values</span><span class="nt">())</span>

<span class="nn">File /opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/site-packages/datasets/builder.py:1648,</span> in <span class="ni">GeneratorBasedBuilder._download_and_prepare</span><span class="nt">(self, dl_manager, verification_mode, **prepare_splits_kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1647</span> <span class="k">def</span><span class="w"> </span><span class="nf">_download_and_prepare</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dl_manager</span><span class="p">,</span> <span class="n">verification_mode</span><span class="p">,</span> <span class="o">**</span><span class="n">prepare_splits_kwargs</span><span class="p">):</span>
<span class="ne">-&gt; </span><span class="mi">1648</span>     <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_download_and_prepare</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">1649</span>         <span class="n">dl_manager</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1650</span>         <span class="n">verification_mode</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1651</span>         <span class="n">check_duplicate_keys</span><span class="o">=</span><span class="n">verification_mode</span> <span class="o">==</span> <span class="n">VerificationMode</span><span class="o">.</span><span class="n">BASIC_CHECKS</span>
<span class="g g-Whitespace">   </span><span class="mi">1652</span>         <span class="ow">or</span> <span class="n">verification_mode</span> <span class="o">==</span> <span class="n">VerificationMode</span><span class="o">.</span><span class="n">ALL_CHECKS</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1653</span>         <span class="o">**</span><span class="n">prepare_splits_kwargs</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1654</span>     <span class="p">)</span>

<span class="nn">File /opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/site-packages/datasets/builder.py:1000,</span> in <span class="ni">DatasetBuilder._download_and_prepare</span><span class="nt">(self, dl_manager, verification_mode, **prepare_split_kwargs)</span>
<span class="g g-Whitespace">    </span><span class="mi">996</span> <span class="n">split_dict</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">split_generator</span><span class="o">.</span><span class="n">split_info</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">998</span> <span class="k">try</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">999</span>     <span class="c1"># Prepare split will record examples associated to the split</span>
<span class="ne">-&gt; </span><span class="mi">1000</span>     <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_split</span><span class="p">(</span><span class="n">split_generator</span><span class="p">,</span> <span class="o">**</span><span class="n">prepare_split_kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1001</span> <span class="k">except</span> <span class="ne">OSError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
<span class="g g-Whitespace">   </span><span class="mi">1002</span>     <span class="k">raise</span> <span class="ne">OSError</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">1003</span>         <span class="s2">&quot;Cannot find data file. &quot;</span>
<span class="g g-Whitespace">   </span><span class="mi">1004</span>         <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">manual_download_instructions</span> <span class="ow">or</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1005</span>         <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Original error:</span><span class="se">\n</span><span class="s2">&quot;</span>
<span class="g g-Whitespace">   </span><span class="mi">1006</span>         <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1007</span>     <span class="p">)</span> <span class="kn">from</span><span class="w"> </span><span class="kc">None</span>

<span class="nn">File /opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/site-packages/datasets/builder.py:1486,</span> in <span class="ni">GeneratorBasedBuilder._prepare_split</span><span class="nt">(self, split_generator, check_duplicate_keys, file_format, num_proc, max_shard_size)</span>
<span class="g g-Whitespace">   </span><span class="mi">1484</span> <span class="n">job_id</span> <span class="o">=</span> <span class="mi">0</span>
<span class="g g-Whitespace">   </span><span class="mi">1485</span> <span class="k">with</span> <span class="n">pbar</span><span class="p">:</span>
<span class="ne">-&gt; </span><span class="mi">1486</span>     <span class="k">for</span> <span class="n">job_id</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">content</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_split_single</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">1487</span>         <span class="n">gen_kwargs</span><span class="o">=</span><span class="n">gen_kwargs</span><span class="p">,</span> <span class="n">job_id</span><span class="o">=</span><span class="n">job_id</span><span class="p">,</span> <span class="o">**</span><span class="n">_prepare_split_args</span>
<span class="g g-Whitespace">   </span><span class="mi">1488</span>     <span class="p">):</span>
<span class="g g-Whitespace">   </span><span class="mi">1489</span>         <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
<span class="g g-Whitespace">   </span><span class="mi">1490</span>             <span class="n">result</span> <span class="o">=</span> <span class="n">content</span>

<span class="nn">File /opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/site-packages/datasets/builder.py:1607,</span> in <span class="ni">GeneratorBasedBuilder._prepare_split_single</span><span class="nt">(self, gen_kwargs, fpath, file_format, max_shard_size, split_info, check_duplicate_keys, job_id)</span>
<span class="g g-Whitespace">   </span><span class="mi">1605</span> <span class="k">try</span><span class="p">:</span>
<span class="g g-Whitespace">   </span><span class="mi">1606</span>     <span class="n">_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="ne">-&gt; </span><span class="mi">1607</span>     <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">record</span> <span class="ow">in</span> <span class="n">generator</span><span class="p">:</span>
<span class="g g-Whitespace">   </span><span class="mi">1608</span>         <span class="k">if</span> <span class="n">max_shard_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">writer</span><span class="o">.</span><span class="n">_num_bytes</span> <span class="o">&gt;</span> <span class="n">max_shard_size</span><span class="p">:</span>
<span class="g g-Whitespace">   </span><span class="mi">1609</span>             <span class="n">num_examples</span><span class="p">,</span> <span class="n">num_bytes</span> <span class="o">=</span> <span class="n">writer</span><span class="o">.</span><span class="n">finalize</span><span class="p">()</span>

<span class="nn">File ~/.cache/huggingface/modules/datasets_modules/datasets/bookcorpus/eddee3cae1cc263a431aa98207d4d27fd8a73b0a9742f692af0e6c65afa4d75f/bookcorpus.py:93,</span> in <span class="ni">Bookcorpus._generate_examples</span><span class="nt">(self, files)</span>
<span class="g g-Whitespace">     </span><span class="mi">91</span> <span class="n">_id</span> <span class="o">=</span> <span class="mi">0</span>
<span class="g g-Whitespace">     </span><span class="mi">92</span> <span class="k">for</span> <span class="n">path</span><span class="p">,</span> <span class="n">file</span> <span class="ow">in</span> <span class="n">files</span><span class="p">:</span>
<span class="ne">---&gt; </span><span class="mi">93</span>     <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">file</span><span class="p">:</span>
<span class="g g-Whitespace">     </span><span class="mi">94</span>         <span class="k">yield</span> <span class="n">_id</span><span class="p">,</span> <span class="p">{</span><span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="n">line</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">strip</span><span class="p">()}</span>
<span class="g g-Whitespace">     </span><span class="mi">95</span>         <span class="n">_id</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="nn">File /opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/tarfile.py:705,</span> in <span class="ni">_FileInFile.readinto</span><span class="nt">(self, b)</span>
<span class="g g-Whitespace">    </span><span class="mi">704</span> <span class="k">def</span><span class="w"> </span><span class="nf">readinto</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
<span class="ne">--&gt; </span><span class="mi">705</span>     <span class="n">buf</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">read</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">b</span><span class="p">))</span>
<span class="g g-Whitespace">    </span><span class="mi">706</span>     <span class="n">b</span><span class="p">[:</span><span class="nb">len</span><span class="p">(</span><span class="n">buf</span><span class="p">)]</span> <span class="o">=</span> <span class="n">buf</span>
<span class="g g-Whitespace">    </span><span class="mi">707</span>     <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="n">buf</span><span class="p">)</span>

<span class="nn">File /opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/tarfile.py:694,</span> in <span class="ni">_FileInFile.read</span><span class="nt">(self, size)</span>
<span class="g g-Whitespace">    </span><span class="mi">692</span> <span class="k">if</span> <span class="n">data</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">693</span>     <span class="bp">self</span><span class="o">.</span><span class="n">fileobj</span><span class="o">.</span><span class="n">seek</span><span class="p">(</span><span class="n">offset</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">position</span> <span class="o">-</span> <span class="n">start</span><span class="p">))</span>
<span class="ne">--&gt; </span><span class="mi">694</span>     <span class="n">b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fileobj</span><span class="o">.</span><span class="n">read</span><span class="p">(</span><span class="n">length</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">695</span>     <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">b</span><span class="p">)</span> <span class="o">!=</span> <span class="n">length</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">696</span>         <span class="k">raise</span> <span class="n">ReadError</span><span class="p">(</span><span class="s2">&quot;unexpected end of data&quot;</span><span class="p">)</span>

<span class="nn">File /opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/tarfile.py:527,</span> in <span class="ni">_Stream.read</span><span class="nt">(self, size)</span>
<span class="g g-Whitespace">    </span><span class="mi">525</span><span class="w"> </span><span class="sd">&quot;&quot;&quot;Return the next size number of bytes from the stream.&quot;&quot;&quot;</span>
<span class="g g-Whitespace">    </span><span class="mi">526</span> <span class="k">assert</span> <span class="n">size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
<span class="ne">--&gt; </span><span class="mi">527</span> <span class="n">buf</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_read</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">528</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="n">buf</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">529</span> <span class="k">return</span> <span class="n">buf</span>

<span class="nn">File /opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/tarfile.py:549,</span> in <span class="ni">_Stream._read</span><span class="nt">(self, size)</span>
<span class="g g-Whitespace">    </span><span class="mi">547</span>         <span class="k">break</span>
<span class="g g-Whitespace">    </span><span class="mi">548</span> <span class="k">try</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">549</span>     <span class="n">buf</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cmp</span><span class="o">.</span><span class="n">decompress</span><span class="p">(</span><span class="n">buf</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">550</span> <span class="k">except</span> <span class="bp">self</span><span class="o">.</span><span class="n">exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">551</span>     <span class="k">raise</span> <span class="n">ReadError</span><span class="p">(</span><span class="s2">&quot;invalid compressed data&quot;</span><span class="p">)</span> <span class="kn">from</span><span class="w"> </span><span class="nn">e</span>

<span class="ne">KeyboardInterrupt</span>: 
</pre></div>
</div>
</div>
</div>
</section>
<section id="tokenize">
<h3>Tokenize<a class="headerlink" href="#tokenize" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tokenizers</span><span class="w"> </span><span class="kn">import</span> <span class="n">Tokenizer</span>
</pre></div>
</div>
</div>
</div>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-center"><p><strong>Component</strong></p></th>
<th class="head text-center"><p><strong>Choice</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p>normalizer</p></td>
<td class="text-center"><p>Lowercase</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>pre-tokenizer</p></td>
<td class="text-center"><p>Whitespace</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>model</p></td>
<td class="text-center"><p>BPE</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>postprocessor</p></td>
<td class="text-center"><p>None</p></td>
</tr>
</tbody>
</table>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tokenizers.normalizers</span><span class="w"> </span><span class="kn">import</span> <span class="n">Lowercase</span> 
<span class="kn">from</span><span class="w"> </span><span class="nn">tokenizers.pre_tokenizers</span><span class="w"> </span><span class="kn">import</span> <span class="n">Whitespace</span> 
<span class="kn">from</span><span class="w"> </span><span class="nn">tokenizers.models</span><span class="w"> </span><span class="kn">import</span> <span class="n">BPE</span>

<span class="k">try</span><span class="p">:</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">BPE</span><span class="p">(</span><span class="n">unk_token</span><span class="o">=</span><span class="s2">&quot;[UNK]&quot;</span><span class="p">)</span>
    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="n">tokenizer</span><span class="o">.</span><span class="n">normalizer</span> <span class="o">=</span> <span class="n">Lowercase</span><span class="p">()</span>
    <span class="n">tokenizer</span><span class="o">.</span><span class="n">pre_tokenizer</span> <span class="o">=</span> <span class="n">Whitespace</span><span class="p">()</span>
<span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ERROR: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>    
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tokenizers.trainers</span><span class="w"> </span><span class="kn">import</span> <span class="n">BpeTrainer</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">BpeTrainer</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="mi">32000</span><span class="p">,</span><span class="n">special_tokens</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;[PAD]&quot;</span><span class="p">,</span><span class="s2">&quot;[UNK]&quot;</span><span class="p">],</span><span class="n">continuing_subword_prefix</span><span class="o">=</span><span class="s1">&#39;##&#39;</span><span class="p">)</span>
<span class="c1"># pipeline is done</span>

<span class="k">def</span><span class="w"> </span><span class="nf">get_examples</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">ds</span><span class="p">),</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="k">yield</span> <span class="n">ds</span><span class="p">[</span><span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="n">batch_size</span><span class="p">][</span><span class="s1">&#39;text&#39;</span><span class="p">]</span>    

<span class="kn">from</span><span class="w"> </span><span class="nn">multiprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">cpu_count</span>
<span class="n">cpus</span> <span class="o">=</span> <span class="n">cpu_count</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cpus</span><span class="p">)</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">tqdm</span><span class="w"> </span><span class="kn">import</span> <span class="n">tqdm</span>
<span class="n">example_iterator</span> <span class="o">=</span> <span class="n">get_examples</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>
<span class="n">example_iterator_with_progress</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">example_iterator</span><span class="p">,</span> <span class="n">total</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">ds</span><span class="p">),</span> <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Training tokenizer&quot;</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">train_from_iterator</span><span class="p">(</span><span class="n">example_iterator_with_progress</span><span class="p">,</span> <span class="n">trainer</span><span class="o">=</span><span class="n">trainer</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">ds</span><span class="p">))</span>

<span class="c1"># tokenizer.train_from_iterator(get_examples(batch_size=10000),trainer=trainer,length=len(ds))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>12
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="k">try</span><span class="p">:</span>
    <span class="n">save_dir</span> <span class="o">=</span> <span class="s1">&#39;model&#39;</span>
    <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">save_dir</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">tokenizer</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">save_dir</span><span class="p">,</span><span class="n">prefix</span><span class="o">=</span><span class="s1">&#39;hopper&#39;</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ERROR: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="vocabulary">
<h3>Vocabulary<a class="headerlink" href="#vocabulary" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;model/hopper-merges.txt&#39;</span><span class="p">,</span><span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>    
    <span class="n">lines</span> <span class="o">=</span> <span class="n">file</span><span class="o">.</span><span class="n">readlines</span><span class="p">()</span> 
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Number of merges:</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">lines</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span> 
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;vocab size:</span><span class="si">{</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">get_vocab_size</span><span class="p">()</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span> 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of merges:31871
vocab size:32000
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">vocab</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">get_vocab</span><span class="p">()</span>
<span class="n">vocab_sorted</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">vocab</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">item</span><span class="p">:</span> <span class="n">item</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">vocab_sorted</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(&#39;[PAD]&#39;, 0),
 (&#39;[UNK]&#39;, 1),
 (&#39;\x13&#39;, 2),
 (&#39;\x14&#39;, 3),
 (&#39;\x18&#39;, 4),
 (&#39;\x19&#39;, 5),
 (&#39;\x1c&#39;, 6),
 (&#39;\x1d&#39;, 7),
 (&#39;\x1f&#39;, 8),
 (&#39;!&#39;, 9),
 (&#39;#&#39;, 10),
 (&#39;$&#39;, 11),
 (&#39;%&#39;, 12),
 (&#39;&amp;&#39;, 13),
 (&quot;&#39;&quot;, 14),
 (&#39;(&#39;, 15),
 (&#39;)&#39;, 16),
 (&#39;*&#39;, 17),
 (&#39;+&#39;, 18),
 (&#39;,&#39;, 19),
 (&#39;-&#39;, 20),
 (&#39;.&#39;, 21),
 (&#39;/&#39;, 22),
 (&#39;0&#39;, 23),
 (&#39;1&#39;, 24),
 (&#39;2&#39;, 25),
 (&#39;3&#39;, 26),
 (&#39;4&#39;, 27),
 (&#39;5&#39;, 28),
 (&#39;6&#39;, 29),
 (&#39;7&#39;, 30),
 (&#39;8&#39;, 31),
 (&#39;9&#39;, 32),
 (&#39;:&#39;, 33),
 (&#39;;&#39;, 34),
 (&#39;&lt;&#39;, 35),
 (&#39;=&#39;, 36),
 (&#39;&gt;&#39;, 37),
 (&#39;?&#39;, 38),
 (&#39;@&#39;, 39),
 (&#39;[&#39;, 40),
 (&#39;\\&#39;, 41),
 (&#39;]&#39;, 42),
 (&#39;^&#39;, 43),
 (&#39;_&#39;, 44),
 (&#39;`&#39;, 45),
 (&#39;a&#39;, 46),
 (&#39;b&#39;, 47),
 (&#39;c&#39;, 48),
 (&#39;d&#39;, 49),
 (&#39;e&#39;, 50),
 (&#39;f&#39;, 51),
 (&#39;g&#39;, 52),
 (&#39;h&#39;, 53),
 (&#39;i&#39;, 54),
 (&#39;j&#39;, 55),
 (&#39;k&#39;, 56),
 (&#39;l&#39;, 57),
 (&#39;m&#39;, 58),
 (&#39;n&#39;, 59),
 (&#39;o&#39;, 60),
 (&#39;p&#39;, 61),
 (&#39;q&#39;, 62),
 (&#39;r&#39;, 63),
 (&#39;s&#39;, 64),
 (&#39;t&#39;, 65),
 (&#39;u&#39;, 66),
 (&#39;v&#39;, 67),
 (&#39;w&#39;, 68),
 (&#39;x&#39;, 69),
 (&#39;y&#39;, 70),
 (&#39;z&#39;, 71),
 (&#39;{&#39;, 72),
 (&#39;|&#39;, 73),
 (&#39;}&#39;, 74),
 (&#39;~&#39;, 75),
 (&#39;\x7f&#39;, 76),
 (&#39;##g&#39;, 77),
 (&#39;##i&#39;, 78),
 (&#39;##t&#39;, 79),
 (&#39;##a&#39;, 80),
 (&#39;##d&#39;, 81),
 (&#39;##o&#39;, 82),
 (&#39;##s&#39;, 83),
 (&#39;##u&#39;, 84),
 (&#39;##l&#39;, 85),
 (&#39;##n&#39;, 86),
 (&#39;##c&#39;, 87),
 (&#39;##p&#39;, 88),
 (&#39;##y&#39;, 89),
 (&#39;##h&#39;, 90),
 (&#39;##r&#39;, 91),
 (&#39;##b&#39;, 92),
 (&#39;##m&#39;, 93),
 (&#39;##e&#39;, 94),
 (&#39;##z&#39;, 95),
 (&#39;##0&#39;, 96),
 (&#39;##k&#39;, 97),
 (&#39;##7&#39;, 98),
 (&#39;##2&#39;, 99),
 (&#39;##4&#39;, 100),
 (&#39;##f&#39;, 101),
 (&#39;##q&#39;, 102),
 (&#39;##v&#39;, 103),
 (&#39;##w&#39;, 104),
 (&#39;##j&#39;, 105),
 (&#39;##x&#39;, 106),
 (&#39;##1&#39;, 107),
 (&#39;##3&#39;, 108),
 (&#39;##9&#39;, 109),
 (&#39;##8&#39;, 110),
 (&#39;##5&#39;, 111),
 (&#39;##*&#39;, 112),
 (&#39;##~&#39;, 113),
 (&#39;##6&#39;, 114),
 (&#39;##_&#39;, 115),
 (&#39;##=&#39;, 116),
 (&#39;##.&#39;, 117),
 (&#39;##^&#39;, 118),
 (&quot;##&#39;&quot;, 119),
 (&#39;##\\&#39;, 120),
 (&#39;##,&#39;, 121),
 (&#39;##`&#39;, 122),
 (&#39;##/&#39;, 123),
 (&#39;##-&#39;, 124),
 (&#39;##\x19&#39;, 125),
 (&#39;##+&#39;, 126),
 (&#39;##|&#39;, 127),
 (&#39;##\x1d&#39;, 128),
 (&#39;##:&#39;, 129),
 (&#39;##he&#39;, 130),
 (&#39;the&#39;, 131),
 (&#39;##in&#39;, 132),
 (&#39;##er&#39;, 133),
 (&#39;##ed&#39;, 134),
 (&#39;##ou&#39;, 135),
 (&#39;##nd&#39;, 136),
 (&#39;##ing&#39;, 137),
 (&#39;to&#39;, 138),
 (&#39;##at&#39;, 139),
 (&#39;##re&#39;, 140),
 (&#39;th&#39;, 141),
 (&#39;##is&#39;, 142),
 (&#39;and&#39;, 143),
 (&#39;##as&#39;, 144),
 (&#39;##en&#39;, 145),
 (&#39;##an&#39;, 146),
 (&#39;##on&#39;, 147),
 (&#39;##ll&#39;, 148),
 (&#39;he&#39;, 149),
 (&#39;##ar&#39;, 150),
 (&#39;##or&#39;, 151),
 (&#39;##es&#39;, 152),
 (&#39;of&#39;, 153),
 (&#39;in&#39;, 154),
 (&#39;##it&#39;, 155),
 (&#39;``&#39;, 156),
 (&quot;&#39;&#39;&quot;, 157),
 (&#39;ha&#39;, 158),
 (&#39;##om&#39;, 159),
 (&#39;you&#39;, 160),
 (&#39;##ow&#39;, 161),
 (&#39;be&#39;, 162),
 (&#39;her&#39;, 163),
 (&#39;##id&#39;, 164),
 (&#39;##ut&#39;, 165),
 (&#39;was&#39;, 166),
 (&#39;it&#39;, 167),
 (&#39;##ot&#39;, 168),
 (&#39;##le&#39;, 169),
 (&#39;##ld&#39;, 170),
 (&#39;##ac&#39;, 171),
 (&#39;##gh&#39;, 172),
 (&#39;on&#39;, 173),
 (&#39;she&#39;, 174),
 (&#39;##et&#39;, 175),
 (&#39;st&#39;, 176),
 (&#39;his&#39;, 177),
 (&#39;that&#39;, 178),
 (&#39;##ly&#39;, 179),
 (&#39;##im&#39;, 180),
 (&#39;##ay&#39;, 181),
 (&#39;##st&#39;, 182),
 (&#39;##ve&#39;, 183),
 (&#39;##ic&#39;, 184),
 (&#39;wh&#39;, 185),
 (&#39;as&#39;, 186),
 (&#39;re&#39;, 187),
 (&#39;my&#39;, 188),
 (&#39;me&#39;, 189),
 (&#39;##ad&#39;, 190),
 (&#39;##al&#39;, 191),
 (&#39;##se&#39;, 192),
 (&#39;##oo&#39;, 193),
 (&#39;##ver&#39;, 194),
 (&#39;##ght&#39;, 195),
 (&#39;##ould&#39;, 196),
 (&#39;##ur&#39;, 197),
 (&#39;##ith&#39;, 198),
 (&#39;##ir&#39;, 199),
 (&#39;for&#39;, 200),
 (&#39;with&#39;, 201),
 (&#39;##ent&#39;, 202),
 (&#39;do&#39;, 203),
 (&#39;an&#39;, 204),
 (&#39;##ke&#39;, 205),
 (&#39;had&#39;, 206),
 (&#39;li&#39;, 207),
 (&#39;at&#39;, 208),
 (&#39;sh&#39;, 209),
 (&#39;##am&#39;, 210),
 (&#39;we&#39;, 211),
 (&#39;but&#39;, 212),
 (&#39;##ion&#39;, 213),
 (&#39;him&#39;, 214),
 (&#39;##all&#39;, 215),
 (&#39;fr&#39;, 216),
 (&#39;##her&#39;, 217),
 (&#39;se&#39;, 218),
 (&#39;sa&#39;, 219),
 (&#39;not&#39;, 220),
 (&#39;##ter&#39;, 221),
 (&#39;##ight&#39;, 222),
 (&#39;##ked&#39;, 223),
 (&#39;##pp&#39;, 224),
 (&#39;ne&#39;, 225),
 (&#39;##ro&#39;, 226),
 (&#39;##ack&#39;, 227),
 (&#39;so&#39;, 228),
 (&#39;##ain&#39;, 229),
 (&#39;##ch&#39;, 230),
 (&#39;##ill&#39;, 231),
 (&#39;##ant&#39;, 232),
 (&#39;##ce&#39;, 233),
 (&#39;##il&#39;, 234),
 (&#39;kn&#39;, 235),
 (&#39;out&#39;, 236),
 (&#39;what&#39;, 237),
 (&#39;##ore&#39;, 238),
 (&#39;is&#39;, 239),
 (&#39;##el&#39;, 240),
 (&#39;up&#39;, 241),
 (&#39;##out&#39;, 242),
 (&#39;##ust&#39;, 243),
 (&#39;##rou&#39;, 244),
 (&#39;##ri&#39;, 245),
 (&#39;whe&#39;, 246),
 (&#39;have&#39;, 247),
 (&#39;##ome&#39;, 248),
 (&#39;said&#39;, 249),
 (&#39;they&#39;, 250),
 (&#39;no&#39;, 251),
 (&#39;##hing&#39;, 252),
 (&#39;##ard&#39;, 253),
 (&#39;this&#39;, 254),
 (&#39;ch&#39;, 255),
 (&#39;from&#39;, 256),
 (&#39;go&#39;, 257),
 (&#39;su&#39;, 258),
 (&#39;did&#39;, 259),
 (&#39;ab&#39;, 260),
 (&#39;##ra&#39;, 261),
 (&#39;##ct&#39;, 262),
 (&#39;le&#39;, 263),
 (&#39;##nt&#39;, 264),
 (&#39;##king&#39;, 265),
 (&#39;##ere&#39;, 266),
 (&#39;all&#39;, 267),
 (&#39;##ie&#39;, 268),
 (&#39;de&#39;, 269),
 (&#39;al&#39;, 270),
 (&#39;##us&#39;, 271),
 (&#39;##one&#39;, 272),
 (&#39;could&#39;, 273),
 (&#39;##ind&#39;, 274),
 (&#39;loo&#39;, 275),
 (&#39;back&#39;, 276),
 (&#39;would&#39;, 277),
 (&#39;were&#39;, 278),
 (&#39;##to&#39;, 279),
 (&#39;##hed&#39;, 280),
 (&#39;like&#39;, 281),
 (&#39;fe&#39;, 282),
 (&#39;if&#39;, 283),
 (&#39;##ess&#39;, 284),
 (&#39;##art&#39;, 285),
 (&#39;##ge&#39;, 286),
 (&#39;##own&#39;, 287),
 (&#39;##est&#39;, 288),
 (&#39;one&#39;, 289),
 (&#39;cl&#39;, 290),
 (&#39;there&#39;, 291),
 (&#39;##un&#39;, 292),
 (&#39;##ood&#39;, 293),
 (&#39;sp&#39;, 294),
 (&#39;con&#39;, 295),
 (&#39;##ss&#39;, 296),
 (&#39;just&#39;, 297),
 (&#39;ag&#39;, 298),
 (&#39;into&#39;, 299),
 (&#39;##ab&#39;, 300),
 (&#39;##ast&#39;, 301),
 (&#39;##au&#39;, 302),
 (&#39;when&#39;, 303),
 (&#39;wor&#39;, 304),
 (&#39;ex&#39;, 305),
 (&#39;know&#39;, 306),
 (&#39;##ers&#39;, 307),
 (&#39;##ry&#39;, 308),
 (&#39;ro&#39;, 309),
 (&#39;##ide&#39;, 310),
 (&#39;about&#39;, 311),
 (&#39;##ep&#39;, 312),
 (&#39;##ck&#39;, 313),
 (&#39;##ace&#39;, 314),
 (&#39;##and&#39;, 315),
 (&#39;bl&#39;, 316),
 (&#39;or&#39;, 317),
 (&#39;hand&#39;, 318),
 (&#39;##ally&#39;, 319),
 (&#39;##ake&#39;, 320),
 (&#39;then&#39;, 321),
 (&#39;##um&#39;, 322),
 (&#39;see&#39;, 323),
 (&#39;them&#39;, 324),
 (&#39;your&#39;, 325),
 (&#39;want&#39;, 326),
 (&#39;##way&#39;, 327),
 (&#39;over&#39;, 328),
 (&#39;##ul&#39;, 329),
 (&#39;te&#39;, 330),
 (&#39;any&#39;, 331),
 (&#39;##op&#39;, 332),
 (&#39;are&#39;, 333),
 (&#39;##ong&#39;, 334),
 (&#39;##lf&#39;, 335),
 (&#39;ey&#39;, 336),
 (&#39;been&#39;, 337),
 (&#39;##ist&#39;, 338),
 (&#39;##ate&#39;, 339),
 (&#39;sm&#39;, 340),
 (&#39;get&#39;, 341),
 (&#39;##ven&#39;, 342),
 (&#39;tim&#39;, 343),
 (&#39;##ap&#39;, 344),
 (&#39;pl&#39;, 345),
 (&#39;down&#39;, 346),
 (&#39;lo&#39;, 347),
 (&#39;co&#39;, 348),
 (&#39;ever&#39;, 349),
 (&#39;some&#39;, 350),
 (&#39;mo&#39;, 351),
 (&#39;too&#39;, 352),
 (&#39;##ig&#39;, 353),
 (&#39;##ame&#39;, 354),
 (&#39;bo&#39;, 355),
 (&#39;again&#39;, 356),
 (&#39;##urn&#39;, 357),
 (&#39;##ft&#39;, 358),
 (&#39;thou&#39;, 359),
 (&#39;com&#39;, 360),
 (&#39;##are&#39;, 361),
 (&#39;who&#39;, 362),
 (&#39;##round&#39;, 363),
 (&#39;man&#39;, 364),
 (&#39;off&#39;, 365),
 (&#39;##ation&#39;, 366),
 (&#39;##ice&#39;, 367),
 (&#39;ar&#39;, 368),
 (&#39;br&#39;, 369),
 (&#39;time&#39;, 370),
 (&#39;now&#39;, 371),
 (&#39;how&#39;, 372),
 (&#39;##ink&#39;, 373),
 (&#39;fl&#39;, 374),
 (&#39;by&#39;, 375),
 (&#39;##ff&#39;, 376),
 (&#39;po&#39;, 377),
 (&#39;##ous&#39;, 378),
 (&#39;eyes&#39;, 379),
 (&#39;##ag&#39;, 380),
 (&#39;head&#39;, 381),
 (&#39;us&#39;, 382),
 (&#39;qu&#39;, 383),
 (&#39;more&#39;, 384),
 (&#39;##ick&#39;, 385),
 (&#39;##ive&#39;, 386),
 (&#39;##ort&#39;, 387),
 (&#39;##ine&#39;, 388),
 (&#39;##ull&#39;, 389),
 (&#39;can&#39;, 390),
 (&#39;sl&#39;, 391),
 (&#39;en&#39;, 392),
 (&#39;##ak&#39;, 393),
 (&#39;sc&#39;, 394),
 (&#39;tr&#39;, 395),
 (&#39;##pped&#39;, 396),
 (&#39;##ect&#39;, 397),
 (&#39;##ther&#39;, 398),
 (&#39;##other&#39;, 399),
 (&#39;tw&#39;, 400),
 (&#39;##ound&#39;, 401),
 (&#39;##os&#39;, 402),
 (&#39;than&#39;, 403),
 (&#39;un&#39;, 404),
 (&#39;##our&#39;, 405),
 (&#39;##nder&#39;, 406),
 (&#39;##itt&#39;, 407),
 (&#39;##self&#39;, 408),
 (&#39;even&#39;, 409),
 (&#39;their&#39;, 410),
 (&#39;##ved&#39;, 411),
 (&#39;way&#39;, 412),
 (&#39;im&#39;, 413),
 (&#39;##fore&#39;, 414),
 (&#39;##led&#39;, 415),
 (&#39;##ile&#39;, 416),
 (&#39;##dd&#39;, 417),
 (&#39;before&#39;, 418),
 (&#39;gr&#39;, 419),
 (&#39;##ire&#39;, 420),
 (&#39;every&#39;, 421),
 (&#39;around&#39;, 422),
 (&#39;will&#39;, 423),
 (&#39;..&#39;, 424),
 (&#39;##reat&#39;, 425),
 (&#39;say&#39;, 426),
 (&#39;going&#39;, 427),
 (&#39;other&#39;, 428),
 (&#39;##ass&#39;, 429),
 (&#39;...&#39;, 430),
 (&#39;##ure&#39;, 431),
 (&#39;pre&#39;, 432),
 (&#39;##ving&#39;, 433),
 (&#39;here&#39;, 434),
 (&#39;pro&#39;, 435),
 (&#39;af&#39;, 436),
 (&#39;som&#39;, 437),
 (&#39;right&#39;, 438),
 (&#39;##th&#39;, 439),
 (&#39;##ade&#39;, 440),
 (&#39;somet&#39;, 441),
 (&#39;only&#39;, 442),
 (&#39;think&#39;, 443),
 (&#39;##rough&#39;, 444),
 (&#39;bec&#39;, 445),
 (&#39;turn&#39;, 446),
 (&#39;through&#39;, 447),
 (&#39;need&#39;, 448),
 (&#39;gl&#39;, 449),
 (&#39;##ue&#39;, 450),
 (&#39;##ip&#39;, 451),
 (&#39;looked&#39;, 452),
 (&#39;##ear&#39;, 453),
 (&#39;##ered&#39;, 454),
 (&#39;##ose&#39;, 455),
 (&#39;look&#39;, 456),
 (&#39;pr&#39;, 457),
 (&#39;##fe&#39;, 458),
 (&#39;let&#39;, 459),
 (&#39;thought&#39;, 460),
 (&#39;##ite&#39;, 461),
 (&#39;should&#39;, 462),
 (&#39;sw&#39;, 463),
 (&#39;thing&#39;, 464),
 (&#39;##ward&#39;, 465),
 (&#39;##act&#39;, 466),
 (&#39;##iss&#39;, 467),
 (&#39;still&#39;, 468),
 (&#39;after&#39;, 469),
 (&#39;away&#39;, 470),
 (&#39;##ol&#39;, 471),
 (&#39;gu&#39;, 472),
 (&#39;kne&#39;, 473),
 (&#39;something&#39;, 474),
 (&#39;face&#39;, 475),
 (&#39;##thing&#39;, 476),
 (&#39;am&#39;, 477),
 (&#39;door&#39;, 478),
 (&#39;##pt&#39;, 479),
 (&#39;mu&#39;, 480),
 (&#39;##ont&#39;, 481),
 (&#39;exp&#39;, 482),
 (&#39;##ity&#39;, 483),
 (&#39;good&#39;, 484),
 (&#39;##ies&#39;, 485),
 (&#39;##ause&#39;, 486),
 (&#39;long&#39;, 487),
 (&#39;##ang&#39;, 488),
 (&#39;never&#39;, 489),
 (&#39;##ment&#39;, 490),
 (&#39;wat&#39;, 491),
 (&#39;ac&#39;, 492),
 (&#39;##ened&#39;, 493),
 (&#39;pe&#39;, 494),
 (&#39;wa&#39;, 495),
 (&#39;##ach&#39;, 496),
 (&#39;##dy&#39;, 497),
 (&#39;##iz&#39;, 498),
 (&#39;bet&#39;, 499),
 (&#39;feel&#39;, 500),
 (&#39;ll&#39;, 501),
 (&#39;##ble&#39;, 502),
 (&#39;got&#39;, 503),
 (&#39;well&#39;, 504),
 (&#39;asked&#39;, 505),
 (&#39;where&#39;, 506),
 (&#39;che&#39;, 507),
 (&#39;##able&#39;, 508),
 (&#39;car&#39;, 509),
 (&#39;##ark&#39;, 510),
 (&#39;two&#39;, 511),
 (&#39;##ree&#39;, 512),
 (&#39;op&#39;, 513),
 (&#39;##ated&#39;, 514),
 (&#39;fo&#39;, 515),
 (&#39;##wn&#39;, 516),
 (&#39;##ance&#39;, 517),
 (&#39;##ell&#39;, 518),
 (&#39;fir&#39;, 519),
 (&#39;arm&#39;, 520),
 (&#39;made&#39;, 521),
 (&#39;##ched&#39;, 522),
 (&#39;why&#39;, 523),
 (&#39;##ich&#39;, 524),
 (&#39;much&#39;, 525),
 (&#39;ve&#39;, 526),
 (&#39;##med&#39;, 527),
 (&#39;##ave&#39;, 528),
 (&#39;##ather&#39;, 529),
 (&#39;##ious&#39;, 530),
 (&#39;our&#39;, 531),
 (&#39;because&#39;, 532),
 (&#39;tell&#39;, 533),
 (&#39;##ting&#39;, 534),
 (&#39;rem&#39;, 535),
 (&#39;room&#39;, 536),
 (&#39;knew&#39;, 537),
 (&#39;dis&#39;, 538),
 (&#39;##ittle&#39;, 539),
 (&#39;##age&#39;, 540),
 (&#39;##aking&#39;, 541),
 (&#39;##ath&#39;, 542),
 (&#39;under&#39;, 543),
 (&#39;ho&#39;, 544),
 (&#39;little&#39;, 545),
 (&#39;mom&#39;, 546),
 (&#39;##te&#39;, 547),
 (&#39;##ning&#39;, 548),
 (&#39;##ady&#39;, 549),
 (&#39;##ough&#39;, 550),
 (&#39;day&#39;, 551),
 (&#39;cr&#39;, 552),
 (&#39;ke&#39;, 553),
 (&#39;make&#39;, 554),
 (&#39;##ery&#39;, 555),
 (&#39;come&#39;, 556),
 (&#39;##co&#39;, 557),
 (&#39;take&#39;, 558),
 (&#39;##elt&#39;, 559),
 (&#39;##gg&#39;, 560),
 (&#39;call&#39;, 561),
 (&#39;happ&#39;, 562),
 (&#39;wal&#39;, 563),
 (&#39;em&#39;, 564),
 (&#39;may&#39;, 565),
 (&#39;##ried&#39;, 566),
 (&#39;##ase&#39;, 567),
 (&#39;##ful&#39;, 568),
 (&#39;##ks&#39;, 569),
 (&#39;first&#39;, 570),
 (&#39;##ress&#39;, 571),
 (&#39;cont&#39;, 572),
 (&#39;##side&#39;, 573),
 (&#39;##be&#39;, 574),
 (&#39;##ru&#39;, 575),
 (&#39;##air&#39;, 576),
 (&#39;vo&#39;, 577),
 (&#39;##ber&#39;, 578),
 (&#39;its&#39;, 579),
 (&#39;felt&#39;, 580),
 (&#39;jo&#39;, 581),
 (&#39;##ence&#39;, 582),
 (&#39;sure&#39;, 583),
 (&#39;wo&#39;, 584),
 (&#39;ca&#39;, 585),
 (&#39;##ves&#39;, 586),
 (&#39;start&#39;, 587),
 (&#39;hands&#39;, 588),
 (&#39;##ied&#39;, 589),
 (&#39;tal&#39;, 590),
 (&#39;##ans&#39;, 591),
 (&#39;##ared&#39;, 592),
 (&#39;hel&#39;, 593),
 (&#39;pull&#39;, 594),
 (&#39;wanted&#39;, 595),
 (&#39;took&#39;, 596),
 (&#39;##ah&#39;, 597),
 (&#39;comp&#39;, 598),
 (&#39;##int&#39;, 599),
 (&#39;turned&#39;, 600),
 (&#39;##em&#39;, 601),
 (&#39;night&#39;, 602),
 (&#39;el&#39;, 603),
 (&#39;app&#39;, 604),
 (&#39;sk&#39;, 605),
 (&#39;really&#39;, 606),
 (&#39;##ling&#39;, 607),
 (&#39;##ps&#39;, 608),
 (&#39;per&#39;, 609),
 (&#39;des&#39;, 610),
 (&#39;mar&#39;, 611),
 (&#39;##oth&#39;, 612),
 (&#39;try&#39;, 613),
 (&#39;##ild&#39;, 614),
 (&#39;##ily&#39;, 615),
 (&#39;against&#39;, 616),
 (&#39;voice&#39;, 617),
 (&#39;sur&#39;, 618),
 (&#39;has&#39;, 619),
 (&#39;ad&#39;, 620),
 (&#39;##uck&#39;, 621),
 (&#39;##ia&#39;, 622),
 (&#39;part&#39;, 623),
 (&#39;res&#39;, 624),
 (&#39;bel&#39;, 625),
 (&#39;##ud&#39;, 626),
 (&#39;fin&#39;, 627),
 (&#39;hard&#39;, 628),
 (&#39;##qu&#39;, 629),
 (&#39;peop&#39;, 630),
 (&#39;real&#39;, 631),
 (&#39;##ced&#39;, 632),
 (&#39;left&#39;, 633),
 (&#39;which&#39;, 634),
 (&#39;##ens&#39;, 635),
 (&#39;##bb&#39;, 636),
 (&#39;very&#39;, 637),
 (&#39;beh&#39;, 638),
 (&#39;help&#39;, 639),
 (&#39;cou&#39;, 640),
 (&#39;people&#39;, 641),
 (&#39;came&#39;, 642),
 (&#39;told&#39;, 643),
 (&#39;another&#39;, 644),
 (&#39;##ds&#39;, 645),
 (&#39;last&#39;, 646),
 (&#39;put&#39;, 647),
 (&#39;##owed&#39;, 648),
 (&#39;##ise&#39;, 649),
 (&#39;side&#39;, 650),
 (&#39;life&#39;, 651),
 (&#39;##een&#39;, 652),
 (&#39;##ook&#39;, 653),
 (&#39;##ish&#39;, 654),
 (&#39;while&#39;, 655),
 (&#39;##ross&#39;, 656),
 (&#39;body&#39;, 657),
 (&#39;wom&#39;, 658),
 (&#39;wait&#39;, 659),
 (&#39;##ty&#39;, 660),
 (&#39;find&#39;, 661),
 (&#39;few&#39;, 662),
 (&#39;sn&#39;, 663),
 (&#39;##av&#39;, 664),
 (&#39;spe&#39;, 665),
 (&#39;gra&#39;, 666),
 (&#39;anything&#39;, 667),
 (&#39;rep&#39;, 668),
 (&#39;moment&#39;, 669),
 (&#39;yes&#39;, 670),
 (&#39;##az&#39;, 671),
 (&#39;being&#39;, 672),
 (&#39;##les&#39;, 673),
 (&#39;new&#39;, 674),
 (&#39;##hes&#39;, 675),
 (&#39;war&#39;, 676),
 (&#39;breat&#39;, 677),
 (&#39;nothing&#39;, 678),
 (&#39;##ned&#39;, 679),
 (&#39;##get&#39;, 680),
 (&#39;ph&#39;, 681),
 (&#39;##xt&#39;, 682),
 (&#39;str&#39;, 683),
 (&#39;own&#39;, 684),
 (&#39;di&#39;, 685),
 (&#39;behind&#39;, 686),
 (&#39;ste&#39;, 687),
 (&#39;keep&#39;, 688),
 (&#39;unt&#39;, 689),
 (&#39;##ions&#39;, 690),
 (&#39;year&#39;, 691),
 (&#39;enough&#39;, 692),
 (&#39;toward&#39;, 693),
 (&#39;went&#39;, 694),
 (&#39;light&#39;, 695),
 (&#39;lau&#39;, 696),
 (&#39;does&#39;, 697),
 (&#39;mat&#39;, 698),
 (&#39;might&#39;, 699),
 (&#39;##ouse&#39;, 700),
 (&#39;saw&#39;, 701),
 (&#39;wr&#39;, 702),
 (&#39;hair&#39;, 703),
 (&#39;bed&#39;, 704),
 (&#39;dr&#39;, 705),
 (&#39;##ci&#39;, 706),
 (&#39;##igh&#39;, 707),
 (&#39;##per&#39;, 708),
 (&#39;beg&#39;, 709),
 (&#39;dark&#39;, 710),
 (&#39;clos&#39;, 711),
 (&#39;sound&#39;, 712),
 (&#39;gir&#39;, 713),
 (&#39;old&#39;, 714),
 (&#39;hu&#39;, 715),
 (&#39;until&#39;, 716),
 (&#39;things&#39;, 717),
 (&#39;love&#39;, 718),
 (&#39;mind&#39;, 719),
 (&#39;cle&#39;, 720),
 (&#39;##outh&#39;, 721),
 (&#39;##ways&#39;, 722),
 (&#39;though&#39;, 723),
 (&#39;id&#39;, 724),
 (&#39;##ars&#39;, 725),
 (&#39;maybe&#39;, 726),
 (&#39;bu&#39;, 727),
 (&#39;those&#39;, 728),
 (&#39;set&#39;, 729),
 (&#39;##amp&#39;, 730),
 (&#39;##ching&#39;, 731),
 (&#39;##ail&#39;, 732),
 (&#39;open&#39;, 733),
 (&#39;end&#39;, 734),
 (&#39;##ys&#39;, 735),
 (&#39;act&#39;, 736),
 (&#39;##aring&#39;, 737),
 (&#39;looking&#39;, 738),
 (&#39;fing&#39;, 739),
 (&#39;##ness&#39;, 740),
 (&#39;once&#39;, 741),
 (&#39;##ng&#39;, 742),
 (&#39;##ord&#39;, 743),
 (&#39;##ost&#39;, 744),
 (&#39;##orm&#39;, 745),
 (&#39;most&#39;, 746),
 (&#39;##ream&#39;, 747),
 (&#39;##ince&#39;, 748),
 (&#39;care&#39;, 749),
 (&#39;##ne&#39;, 750),
 (&#39;##iend&#39;, 751),
 (&#39;both&#39;, 752),
 (&#39;gi&#39;, 753),
 (&#39;girl&#39;, 754),
 (&#39;friend&#39;, 755),
 (&#39;##ool&#39;, 756),
 (&#39;always&#39;, 757),
 (&#39;ass&#39;, 758),
 (&#39;##dded&#39;, 759),
 (&#39;next&#39;, 760),
 (&#39;##ons&#39;, 761),
 (&#39;##cked&#39;, 762),
 (&#39;##ner&#39;, 763),
 (&#39;place&#39;, 764),
 (&#39;sil&#39;, 765),
 (&#39;att&#39;, 766),
 (&#39;cur&#39;, 767),
 (&#39;min&#39;, 768),
 (&#39;hold&#39;, 769),
 (&#39;##ower&#39;, 770),
 (&#39;imp&#39;, 771),
 (&#39;oh&#39;, 772),
 (&#39;kill&#39;, 773),
 (&#39;smil&#39;, 774),
 (&#39;house&#39;, 775),
 (&#39;mouth&#39;, 776),
 (&#39;inside&#39;, 777),
 (&#39;sat&#39;, 778),
 (&#39;run&#39;, 779),
 (&#39;found&#39;, 780),
 (&#39;front&#39;, 781),
 (&#39;##row&#39;, 782),
 (&#39;ple&#39;, 783),
 (&#39;##ted&#39;, 784),
 (&#39;heart&#39;, 785),
 (&#39;inst&#39;, 786),
 (&#39;without&#39;, 787),
 (&#39;dec&#39;, 788),
 (&#39;ser&#39;, 789),
 (&#39;gre&#39;, 790),
 (&#39;##less&#39;, 791),
 (&#39;##ating&#39;, 792),
 (&#39;home&#39;, 793),
 (&#39;##ian&#39;, 794),
 (&#39;rel&#39;, 795),
 (&#39;blood&#39;, 796),
 (&#39;same&#39;, 797),
 (&#39;##ory&#39;, 798),
 (&#39;work&#39;, 799),
 (&#39;##aw&#39;, 800),
 (&#39;everything&#39;, 801),
 (&#39;rest&#39;, 802),
 (&#39;##ually&#39;, 803),
 (&#39;someone&#39;, 804),
 (&#39;himself&#39;, 805),
 (&#39;sho&#39;, 806),
 (&#39;stand&#39;, 807),
 (&#39;inter&#39;, 808),
 (&#39;small&#39;, 809),
 (&#39;##ary&#39;, 810),
 (&#39;woman&#39;, 811),
 (&#39;hop&#39;, 812),
 (&#39;bit&#39;, 813),
 (&#39;bre&#39;, 814),
 (&#39;##sw&#39;, 815),
 (&#39;trying&#39;, 816),
 (&#39;heard&#39;, 817),
 (&#39;better&#39;, 818),
 (&#39;ok&#39;, 819),
 (&#39;arms&#39;, 820),
 (&#39;pulled&#39;, 821),
 (&#39;flo&#39;, 822),
 (&#39;cor&#39;, 823),
 (&#39;dra&#39;, 824),
 (&#39;ret&#39;, 825),
 (&#39;cour&#39;, 826),
 (&#39;betw&#39;, 827),
 (&#39;each&#39;, 828),
 (&#39;between&#39;, 829),
 (&#39;father&#39;, 830),
 (&#39;stre&#39;, 831),
 (&#39;give&#39;, 832),
 (&#39;##ised&#39;, 833),
 (&#39;kiss&#39;, 834),
 (&#39;mean&#39;, 835),
 (&#39;wind&#39;, 836),
 (&#39;##cond&#39;, 837),
 (&#39;mother&#39;, 838),
 (&#39;seemed&#39;, 839),
 (&#39;black&#39;, 840),
 (&#39;##ier&#39;, 841),
 (&#39;deep&#39;, 842),
 (&#39;##ung&#39;, 843),
 (&#39;smile&#39;, 844),
 (&#39;##red&#39;, 845),
 (&#39;second&#39;, 846),
 (&#39;answ&#39;, 847),
 (&#39;##pping&#39;, 848),
 (&#39;##if&#39;, 849),
 (&#39;##ange&#39;, 850),
 (&#39;##tered&#39;, 851),
 (&#39;far&#39;, 852),
 (&#39;ang&#39;, 853),
 (&#39;alm&#39;, 854),
 (&#39;##tain&#39;, 855),
 (&#39;across&#39;, 856),
 (&#39;##ably&#39;, 857),
 (&#39;##sp&#39;, 858),
 (&#39;fam&#39;, 859),
 (&#39;pain&#39;, 860),
 (&#39;stop&#39;, 861),
 (&#39;myself&#39;, 862),
 (&#39;hur&#39;, 863),
 (&#39;sto&#39;, 864),
 (&#39;three&#39;, 865),
 (&#39;##ready&#39;, 866),
 (&#39;##sh&#39;, 867),
 (&#39;##used&#39;, 868),
 (&#39;since&#39;, 869),
 (&#39;dri&#39;, 870),
 (&#39;already&#39;, 871),
 (&#39;pass&#39;, 872),
 (&#39;##ever&#39;, 873),
 (&#39;sle&#39;, 874),
 (&#39;##aught&#39;, 875),
 (&#39;started&#39;, 876),
 (&#39;point&#39;, 877),
 (&#39;##ump&#39;, 878),
 (&#39;quick&#39;, 879),
 (&#39;almost&#39;, 880),
 (&#39;gave&#39;, 881),
 (&#39;stu&#39;, 882),
 (&#39;breath&#39;, 883),
 (&#39;##ict&#39;, 884),
 (&#39;dont&#39;, 885),
 (&#39;years&#39;, 886),
 (&#39;must&#39;, 887),
 (&#39;belie&#39;, 888),
 (&#39;stay&#39;, 889),
 (&#39;##ached&#39;, 890),
 (&#39;stood&#39;, 891),
 (&#39;men&#39;, 892),
 (&#39;kind&#39;, 893),
 (&#39;##ix&#39;, 894),
 (&#39;##ank&#39;, 895),
 (&#39;remem&#39;, 896),
 (&#39;ear&#39;, 897),
 (&#39;air&#39;, 898),
 (&#39;else&#39;, 899),
 (&#39;done&#39;, 900),
 (&#39;mor&#39;, 901),
 (&#39;wall&#39;, 902),
 (&#39;dro&#39;, 903),
 (&#39;okay&#39;, 904),
 (&#39;foll&#39;, 905),
 (&#39;##ors&#39;, 906),
 (&#39;doing&#39;, 907),
 (&#39;hell&#39;, 908),
 (&#39;##gether&#39;, 909),
 (&#39;tried&#39;, 910),
 (&#39;tra&#39;, 911),
 (&#39;##umb&#39;, 912),
 (&#39;world&#39;, 913),
 (&#39;ob&#39;, 914),
 (&#39;##ible&#39;, 915),
 (&#39;##ings&#39;, 916),
 (&#39;lips&#39;, 917),
 (&#39;walked&#39;, 918),
 (&#39;close&#39;, 919),
 (&#39;##ane&#39;, 920),
 (&#39;words&#39;, 921),
 (&#39;big&#39;, 922),
 (&#39;ev&#39;, 923),
 (&#39;hum&#39;, 924),
 (&#39;tou&#39;, 925),
 (&#39;mon&#39;, 926),
 (&#39;slow&#39;, 927),
 (&#39;##ater&#39;, 928),
 (&#39;minut&#39;, 929),
 (&#39;together&#39;, 930),
 (&#39;##rew&#39;, 931),
 (&#39;##ized&#39;, 932),
 (&#39;feet&#39;, 933),
 (&#39;nodded&#39;, 934),
 (&#39;ye&#39;, 935),
 (&#39;sig&#39;, 936),
 (&#39;##ott&#39;, 937),
 (&#39;##so&#39;, 938),
 (&#39;##ple&#39;, 939),
 (&#39;##ense&#39;, 940),
 (&#39;didnt&#39;, 941),
 (&#39;##ushed&#39;, 942),
 (&#39;wonder&#39;, 943),
 (&#39;held&#39;, 944),
 (&#39;near&#39;, 945),
 (&#39;seen&#39;, 946),
 (&#39;##ph&#39;, 947),
 (&#39;met&#39;, 948),
 (&#39;many&#39;, 949),
 (&#39;miss&#39;, 950),
 (&#39;##ured&#39;, 951),
 (&#39;##ren&#39;, 952),
 (&#39;these&#39;, 953),
 (&#39;lot&#39;, 954),
 (&#39;ra&#39;, 955),
 (&#39;quest&#39;, 956),
 (&#39;underst&#39;, 957),
 (&#39;##ully&#39;, 958),
 (&#39;##anc&#39;, 959),
 (&#39;##ash&#39;, 960),
 (&#39;bar&#39;, 961),
 (&#39;diff&#39;, 962),
 (&#39;##ently&#39;, 963),
 (&#39;ma&#39;, 964),
 (&#39;supp&#39;, 965),
 (&#39;int&#39;, 966),
 (&#39;##gs&#39;, 967),
 (&#39;##irt&#39;, 968),
 (&#39;##ason&#39;, 969),
 (&#39;tre&#39;, 970),
 (&#39;sim&#39;, 971),
 (&#39;whis&#39;, 972),
 (&#39;pers&#39;, 973),
 (&#39;ent&#39;, 974),
 (&#39;##ond&#39;, 975),
 (&#39;##icked&#39;, 976),
 (&#39;##inking&#39;, 977),
 (&#39;hear&#39;, 978),
 (&#39;##pr&#39;, 979),
 (&#39;play&#39;, 980),
 (&#39;yet&#39;, 981),
 (&#39;talk&#39;, 982),
 (&#39;**&#39;, 983),
 (&#39;says&#39;, 984),
 (&#39;move&#39;, 985),
 (&#39;##ither&#39;, 986),
 (&#39;conf&#39;, 987),
 (&#39;bad&#39;, 988),
 (&#39;##ows&#39;, 989),
 (&#39;##akes&#39;, 990),
 (&#39;floor&#39;, 991),
 (&#39;water&#39;, 992),
 (&#39;expl&#39;, 993),
 (&#39;leave&#39;, 994),
 (&#39;ed&#39;, 995),
 (&#39;smiled&#39;, 996),
 (&#39;mr&#39;, 997),
 (&#39;##ib&#39;, 998),
 (&#39;##day&#39;, 999),
 ...]
</pre></div>
</div>
</div>
</div>
</section>
<section id="encoding">
<h3>Encoding<a class="headerlink" href="#encoding" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sample</span> <span class="o">=</span> <span class="n">ds</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;text&#39;</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;sample: </span><span class="si">{</span><span class="n">sample</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">encoding</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">encoding</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>sample: usually , he would be tearing around the living room , playing with his toys .
Encoding(num_tokens=16, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">token_ids</span> <span class="o">=</span> <span class="n">encoding</span><span class="o">.</span><span class="n">ids</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="n">encoding</span><span class="o">.</span><span class="n">tokens</span>
<span class="n">type_ids</span> <span class="o">=</span> <span class="n">encoding</span><span class="o">.</span><span class="n">type_ids</span>
<span class="n">attention_mask</span> <span class="o">=</span> <span class="n">encoding</span><span class="o">.</span><span class="n">attention_mask</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">tokenizers.tools</span><span class="w"> </span><span class="kn">import</span> <span class="n">EncodingVisualizer</span>
<span class="n">visualizer</span> <span class="o">=</span> <span class="n">EncodingVisualizer</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>
<span class="n">visualizer</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">sample</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">
    <html>
        <head>
            <style>
                .tokenized-text {
    width:100%;
    padding:2rem;
    max-height: 400px;
    overflow-y: auto;
    box-sizing:border-box;
    line-height:4rem; /* Lots of space between lines */
    font-family: "Roboto Light", "Ubuntu Light", "Ubuntu", monospace;
    box-shadow: 2px 2px 2px rgba(0,0,0,0.2);
    background-color: rgba(0,0,0,0.01);
    letter-spacing:2px; /* Give some extra separation between chars */
}
.non-token{
    /* White space and other things the tokenizer ignores*/
    white-space: pre;
    letter-spacing:4px;
    border-top:1px solid #A0A0A0; /* A gentle border on top and bottom makes tabs more ovious*/
    border-bottom:1px solid #A0A0A0;
    line-height: 1rem;
    height: calc(100% - 2px);
}

.token {
    white-space: pre;
    position:relative;
    color:black;
    letter-spacing:2px;
}

.annotation{
    white-space:nowrap; /* Important - ensures that annotations appears even if the annotated text wraps a line */
    border-radius:4px;
    position:relative;
    width:fit-content;
}
.annotation:before {
    /*The before holds the text and the after holds the background*/
    z-index:1000; /* Make sure this is above the background */
    content:attr(data-label); /* The annotations label is on a data attribute */
    color:white;
    position:absolute;
    font-size:1rem;
    text-align:center;
    font-weight:bold;

    top:1.75rem;
    line-height:0;
    left:0;
    width:100%;
    padding:0.5rem 0;
    /* These make it so an annotation doesn't stretch beyond the annotated text if the label is longer*/
    overflow: hidden;
    white-space: nowrap;
    text-overflow:ellipsis;
}

.annotation:after {
    content:attr(data-label); /* The content defines the width of the annotation*/
    position:absolute;
    font-size:0.75rem;
    text-align:center;
    font-weight:bold;
    text-overflow:ellipsis;
    top:1.75rem;
    line-height:0;
    overflow: hidden;
    white-space: nowrap;

    left:0;
    width:100%; /* 100% of the parent, which is the annotation whose width is the tokens inside it*/

    padding:0.5rem 0;
    /* Nast hack below:
    We set the annotations color in code because we don't know the colors at css time.
    But you can't pass a color as a data attribute to get it into the pseudo element (this thing)
    So to get around that, annotations have the color set on them with a style attribute and then we
    can get the color with currentColor.
    Annotations wrap tokens and tokens set the color back to black
     */
    background-color: currentColor;
}
.annotation:hover::after, .annotation:hover::before{
    /* When the user hovers over an annotation expand the label to display in full
     */
    min-width: fit-content;
}

.annotation:hover{
    /* Emphasize the annotation start end with a border on hover*/
    border-color: currentColor;
    border: 2px solid;
}
.special-token:not(:empty){
    /*
    A none empty special token is like UNK (as opposed to CLS which has no representation in the text )
     */
    position:relative;
}
.special-token:empty::before{
    /* Special tokens that don't have text are displayed as pseudo elements so we dont select them with the mouse*/
    content:attr(data-stok);
    background:#202020;
    font-size:0.75rem;
    color:white;
    margin: 0 0.25rem;
    padding: 0.25rem;
    border-radius:4px
}

.special-token:not(:empty):before {
    /* Special tokens that have text (UNK) are displayed above the actual text*/
    content:attr(data-stok);
    position:absolute;
    bottom:1.75rem;
    min-width:100%;
    width:100%;
    height:1rem;
    line-height:1rem;
    font-size:1rem;
    text-align:center;
    color:white;
    font-weight:bold;
    background:#202020;
    border-radius:10%;
}
/*
We want to alternate the color of tokens, but we can't use nth child because tokens might be broken up by annotations
instead we apply even and odd class at generation time and color them that way
 */
.even-token{
    background:#DCDCDC	;
    border: 1px solid #DCDCDC;
}
.odd-token{
    background:#A0A0A0;
    border: 1px solid #A0A0A0;
}
.even-token.multi-token,.odd-token.multi-token{
    background:  repeating-linear-gradient(
    45deg,
    transparent,
    transparent 1px,
    #ccc 1px,
    #ccc 1px
    ),
    /* on "bottom" */
    linear-gradient(
    to bottom,
    #FFB6C1,
    #999
    );
}

.multi-token:hover::after {
    content:"This char has more than 1 token"; /* The content defines the width of the annotation*/
    color:white;
    background-color: black;
    position:absolute;
    font-size:0.75rem;
    text-align:center;
    font-weight:bold;
    text-overflow:ellipsis;
    top:1.75rem;
    line-height:0;
    overflow: hidden;
    white-space: nowrap;
    left:0;
    width:fit-content; /* 100% of the parent, which is the annotation whose width is the tokens inside it*/
    padding:0.5rem 0;
}

            </style>
        </head>
        <body>
            <div class="tokenized-text" dir=auto>
            <span class="token even-token"  >usually</span><span class="non-token"  > </span><span class="token odd-token"  >,</span><span class="non-token"  > </span><span class="token even-token"  >he</span><span class="non-token"  > </span><span class="token odd-token"  >would</span><span class="non-token"  > </span><span class="token even-token"  >be</span><span class="non-token"  > </span><span class="token odd-token"  >tearing</span><span class="non-token"  > </span><span class="token even-token"  >around</span><span class="non-token"  > </span><span class="token odd-token"  >the</span><span class="non-token"  > </span><span class="token even-token"  >living</span><span class="non-token"  > </span><span class="token odd-token"  >room</span><span class="non-token"  > </span><span class="token even-token"  >,</span><span class="non-token"  > </span><span class="token odd-token"  >playing</span><span class="non-token"  > </span><span class="token even-token"  >with</span><span class="non-token"  > </span><span class="token odd-token"  >his</span><span class="non-token"  > </span><span class="token even-token"  >toys</span><span class="non-token"  > </span><span class="token odd-token"  >.</span>
            </div>
        </body>
    </html>
    </div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="n">out_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;tokens&#39;</span><span class="p">:</span><span class="n">tokens</span><span class="p">,</span><span class="s1">&#39;ids&#39;</span><span class="p">:</span><span class="n">token_ids</span><span class="p">,</span><span class="s1">&#39;type_ids&#39;</span><span class="p">:</span><span class="n">type_ids</span><span class="p">,</span><span class="s1">&#39;attention_mask&#39;</span><span class="p">:</span><span class="n">attention_mask</span><span class="p">}</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="n">out_dict</span><span class="p">)</span>
<span class="n">df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>tokens</th>
      <th>ids</th>
      <th>type_ids</th>
      <th>attention_mask</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>usually</td>
      <td>2462</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>,</td>
      <td>19</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>he</td>
      <td>149</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>would</td>
      <td>277</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>be</td>
      <td>162</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>5</th>
      <td>tearing</td>
      <td>6456</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>6</th>
      <td>around</td>
      <td>422</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>7</th>
      <td>the</td>
      <td>131</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>8</th>
      <td>living</td>
      <td>1559</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>9</th>
      <td>room</td>
      <td>536</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>10</th>
      <td>,</td>
      <td>19</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>11</th>
      <td>playing</td>
      <td>2301</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>12</th>
      <td>with</td>
      <td>201</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>13</th>
      <td>his</td>
      <td>177</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>14</th>
      <td>toys</td>
      <td>9774</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>15</th>
      <td>.</td>
      <td>21</td>
      <td>0</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
<section id="batch-encoding">
<h3>Batch Encoding<a class="headerlink" href="#batch-encoding" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">pprint</span><span class="w"> </span><span class="kn">import</span> <span class="n">pprint</span> 
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">samples</span> <span class="o">=</span> <span class="n">ds</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">4</span><span class="p">][</span><span class="s1">&#39;text&#39;</span><span class="p">]</span>
<span class="n">batch_encoding</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode_batch</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">batch_encoding</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[Encoding(num_tokens=16, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),
 Encoding(num_tokens=14, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),
 Encoding(num_tokens=14, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),
 Encoding(num_tokens=42, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># all default args</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">enable_padding</span><span class="p">(</span><span class="n">direction</span> <span class="o">=</span> <span class="s1">&#39;right&#39;</span><span class="p">,</span>
                         <span class="n">pad_id</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
                         <span class="n">pad_type_id</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
                         <span class="n">pad_token</span> <span class="o">=</span> <span class="s1">&#39;[PAD]&#39;</span><span class="p">,</span>
                         <span class="n">length</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="c1"># None default to max_len in the batch</span>
                         <span class="n">pad_to_multiple_of</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> 

<span class="n">tokenizer</span><span class="o">.</span><span class="n">enable_truncation</span><span class="p">(</span><span class="n">max_length</span><span class="o">=</span><span class="mi">128</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">batch_encoding</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode_batch</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">batch_encoding</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[Encoding(num_tokens=42, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),
 Encoding(num_tokens=42, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),
 Encoding(num_tokens=42, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),
 Encoding(num_tokens=42, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])]
</pre></div>
</div>
</div>
</div>
</section>
<section id="testing">
<h3>Testing<a class="headerlink" href="#testing" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;All this is so simple to do in HF இ😊.&quot;</span>
<span class="n">encoded</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">text</span><span class="p">)</span><span class="o">.</span><span class="n">tokens</span>
<span class="nb">print</span><span class="p">(</span><span class="n">encoded</span><span class="p">)</span>
<span class="n">visualizer</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;all&#39;,
 &#39;this&#39;,
 &#39;is&#39;,
 &#39;so&#39;,
 &#39;simple&#39;,
 &#39;to&#39;,
 &#39;do&#39;,
 &#39;in&#39;,
 &#39;h&#39;,
 &#39;##f&#39;,
 &#39;[UNK]&#39;,
 &#39;[UNK]&#39;,
 &#39;##.&#39;]
</pre></div>
</div>
<div class="output text_html">
    <html>
        <head>
            <style>
                .tokenized-text {
    width:100%;
    padding:2rem;
    max-height: 400px;
    overflow-y: auto;
    box-sizing:border-box;
    line-height:4rem; /* Lots of space between lines */
    font-family: "Roboto Light", "Ubuntu Light", "Ubuntu", monospace;
    box-shadow: 2px 2px 2px rgba(0,0,0,0.2);
    background-color: rgba(0,0,0,0.01);
    letter-spacing:2px; /* Give some extra separation between chars */
}
.non-token{
    /* White space and other things the tokenizer ignores*/
    white-space: pre;
    letter-spacing:4px;
    border-top:1px solid #A0A0A0; /* A gentle border on top and bottom makes tabs more ovious*/
    border-bottom:1px solid #A0A0A0;
    line-height: 1rem;
    height: calc(100% - 2px);
}

.token {
    white-space: pre;
    position:relative;
    color:black;
    letter-spacing:2px;
}

.annotation{
    white-space:nowrap; /* Important - ensures that annotations appears even if the annotated text wraps a line */
    border-radius:4px;
    position:relative;
    width:fit-content;
}
.annotation:before {
    /*The before holds the text and the after holds the background*/
    z-index:1000; /* Make sure this is above the background */
    content:attr(data-label); /* The annotations label is on a data attribute */
    color:white;
    position:absolute;
    font-size:1rem;
    text-align:center;
    font-weight:bold;

    top:1.75rem;
    line-height:0;
    left:0;
    width:100%;
    padding:0.5rem 0;
    /* These make it so an annotation doesn't stretch beyond the annotated text if the label is longer*/
    overflow: hidden;
    white-space: nowrap;
    text-overflow:ellipsis;
}

.annotation:after {
    content:attr(data-label); /* The content defines the width of the annotation*/
    position:absolute;
    font-size:0.75rem;
    text-align:center;
    font-weight:bold;
    text-overflow:ellipsis;
    top:1.75rem;
    line-height:0;
    overflow: hidden;
    white-space: nowrap;

    left:0;
    width:100%; /* 100% of the parent, which is the annotation whose width is the tokens inside it*/

    padding:0.5rem 0;
    /* Nast hack below:
    We set the annotations color in code because we don't know the colors at css time.
    But you can't pass a color as a data attribute to get it into the pseudo element (this thing)
    So to get around that, annotations have the color set on them with a style attribute and then we
    can get the color with currentColor.
    Annotations wrap tokens and tokens set the color back to black
     */
    background-color: currentColor;
}
.annotation:hover::after, .annotation:hover::before{
    /* When the user hovers over an annotation expand the label to display in full
     */
    min-width: fit-content;
}

.annotation:hover{
    /* Emphasize the annotation start end with a border on hover*/
    border-color: currentColor;
    border: 2px solid;
}
.special-token:not(:empty){
    /*
    A none empty special token is like UNK (as opposed to CLS which has no representation in the text )
     */
    position:relative;
}
.special-token:empty::before{
    /* Special tokens that don't have text are displayed as pseudo elements so we dont select them with the mouse*/
    content:attr(data-stok);
    background:#202020;
    font-size:0.75rem;
    color:white;
    margin: 0 0.25rem;
    padding: 0.25rem;
    border-radius:4px
}

.special-token:not(:empty):before {
    /* Special tokens that have text (UNK) are displayed above the actual text*/
    content:attr(data-stok);
    position:absolute;
    bottom:1.75rem;
    min-width:100%;
    width:100%;
    height:1rem;
    line-height:1rem;
    font-size:1rem;
    text-align:center;
    color:white;
    font-weight:bold;
    background:#202020;
    border-radius:10%;
}
/*
We want to alternate the color of tokens, but we can't use nth child because tokens might be broken up by annotations
instead we apply even and odd class at generation time and color them that way
 */
.even-token{
    background:#DCDCDC	;
    border: 1px solid #DCDCDC;
}
.odd-token{
    background:#A0A0A0;
    border: 1px solid #A0A0A0;
}
.even-token.multi-token,.odd-token.multi-token{
    background:  repeating-linear-gradient(
    45deg,
    transparent,
    transparent 1px,
    #ccc 1px,
    #ccc 1px
    ),
    /* on "bottom" */
    linear-gradient(
    to bottom,
    #FFB6C1,
    #999
    );
}

.multi-token:hover::after {
    content:"This char has more than 1 token"; /* The content defines the width of the annotation*/
    color:white;
    background-color: black;
    position:absolute;
    font-size:0.75rem;
    text-align:center;
    font-weight:bold;
    text-overflow:ellipsis;
    top:1.75rem;
    line-height:0;
    overflow: hidden;
    white-space: nowrap;
    left:0;
    width:fit-content; /* 100% of the parent, which is the annotation whose width is the tokens inside it*/
    padding:0.5rem 0;
}

            </style>
        </head>
        <body>
            <div class="tokenized-text" dir=auto>
            <span class="token even-token"  >All</span><span class="non-token"  > </span><span class="token odd-token"  >this</span><span class="non-token"  > </span><span class="token even-token"  >is</span><span class="non-token"  > </span><span class="token odd-token"  >so</span><span class="non-token"  > </span><span class="token even-token"  >simple</span><span class="non-token"  > </span><span class="token odd-token"  >to</span><span class="non-token"  > </span><span class="token even-token"  >do</span><span class="non-token"  > </span><span class="token odd-token"  >in</span><span class="non-token"  > </span><span class="token even-token"  >H</span><span class="token odd-token"  >F</span><span class="non-token"  > </span><span class="token even-token special-token"  data-stok="[UNK]" >இ</span><span class="token odd-token special-token"  data-stok="[UNK]" >😊</span><span class="token even-token"  >.</span>
            </div>
        </body>
    </html>
    </div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">try</span><span class="p">:</span>
    <span class="n">tokenizer</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">save_dir</span><span class="p">,</span> <span class="s1">&#39;hopper.json&#39;</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ERROR: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&quot;ERROR: argument &#39;pretty&#39;: &#39;str&#39; object cannot be converted to &#39;PyBool&#39;&quot;
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="assignment">
<h2>Assignment<a class="headerlink" href="#assignment" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">pprint</span><span class="w"> </span><span class="kn">import</span> <span class="n">pprint</span>
</pre></div>
</div>
</div>
</div>
<p>Download the BookCorpus dataset. Take every 7-th sample (the indices are multiple of 7:[0,7,14,21,…]) from the entire dataset. This will result in a dataset with 10 million samples (exactly, 10,572,033). Use these samples to build a tokenizer with the BPE tokenization algorithm by varying the vocabulary size.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_dataset</span>
<span class="k">try</span><span class="p">:</span>
    <span class="n">all_ds</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s1">&#39;bookcorpus&#39;</span><span class="p">,</span><span class="n">split</span><span class="o">=</span><span class="s1">&#39;all&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">all_ds</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ERROR: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>c:\Users\dell\AppData\Local\Programs\Python\Python311\Lib\site-packages\tqdm\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
  from .autonotebook import tqdm as notebook_tqdm
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Dataset({
    features: [&#39;text&#39;],
    num_rows: 74004228
})
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ds</span> <span class="o">=</span> <span class="n">all_ds</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">all_ds</span><span class="o">.</span><span class="n">num_rows</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">ds</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Dataset({
    features: [&#39;text&#39;],
    num_rows: 10572033
})
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Normalizer: LowerCase</p></li>
<li><p>PreTokenizer: WhiteSpace</p></li>
<li><p>Model: BPE</p></li>
<li><p>Special tokens: [GO],[UNK],[PAD],[EOS]</p></li>
<li><p>PostProcessing: None</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tokenizers</span><span class="w"> </span><span class="kn">import</span> <span class="n">Tokenizer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tokenizers.normalizers</span><span class="w"> </span><span class="kn">import</span> <span class="n">Lowercase</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tokenizers.pre_tokenizers</span><span class="w"> </span><span class="kn">import</span> <span class="n">Whitespace</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tokenizers.models</span><span class="w"> </span><span class="kn">import</span>  <span class="n">BPE</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tokenizers.trainers</span><span class="w"> </span><span class="kn">import</span> <span class="n">BpeTrainer</span>
</pre></div>
</div>
</div>
</div>
<p>Tokenize the input text: “SEBI study finds 93% of individual F&amp;O traders made losses between FY22 and FY24.” using the following configurations.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;SEBI study finds 93</span><span class="si">% o</span><span class="s2">f individual F&amp;O traders made losses between FY22 and FY24.&quot;</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">),</span><span class="n">tokens</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>14 [&#39;SEBI&#39;, &#39;study&#39;, &#39;finds&#39;, &#39;93%&#39;, &#39;of&#39;, &#39;individual&#39;, &#39;F&amp;O&#39;, &#39;traders&#39;, &#39;made&#39;, &#39;losses&#39;, &#39;between&#39;, &#39;FY22&#39;, &#39;and&#39;, &#39;FY24.&#39;]
</pre></div>
</div>
</div>
</div>
<ol class="arabic simple">
<li><p>Keep the vocabulary size at 5000 and tokenize the input text using the learned vocabulary. Choose the number of tokens returned by the tokenizer.</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">BPE</span><span class="p">(</span><span class="n">unk_token</span><span class="o">=</span> <span class="s2">&quot;[UNK]&quot;</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">normalizer</span> <span class="o">=</span> <span class="n">Lowercase</span><span class="p">()</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">pre_tokenizer</span> <span class="o">=</span> <span class="n">Whitespace</span><span class="p">()</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">BpeTrainer</span><span class="p">(</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="mi">5000</span><span class="p">,</span>
    <span class="n">special_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;[GO]&quot;</span><span class="p">,</span><span class="s2">&quot;[UNK]&quot;</span><span class="p">,</span><span class="s2">&quot;[PAD]&quot;</span><span class="p">,</span><span class="s2">&quot;[EOS]&quot;</span><span class="p">],</span>
    <span class="n">continuing_subword_prefix</span><span class="o">=</span><span class="s1">&#39;##&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">samples</span><span class="p">(</span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">ds</span><span class="p">),</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="k">yield</span> <span class="n">ds</span><span class="p">[</span><span class="n">i</span> <span class="p">:</span> <span class="n">i</span><span class="o">+</span> <span class="n">batch_size</span><span class="p">][</span><span class="s1">&#39;text&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tqdm</span><span class="w"> </span><span class="kn">import</span> <span class="n">tqdm</span>
<span class="n">bsize</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">samples_iterator</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">samples</span><span class="p">(</span><span class="n">bsize</span><span class="p">),</span> <span class="n">total</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">ds</span><span class="p">)</span> <span class="o">//</span> <span class="n">bsize</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Tokenizer Training&quot;</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">train_from_iterator</span><span class="p">(</span><span class="n">samples_iterator</span><span class="p">,</span> <span class="n">trainer</span><span class="o">=</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">ds</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Tokenizer Training: 10573it [01:57, 89.79it/s]                           
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">text</span><span class="p">)</span><span class="o">.</span><span class="n">tokens</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">),</span> <span class="n">tokens</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>32 [&#39;seb&#39;, &#39;##i&#39;, &#39;study&#39;, &#39;find&#39;, &#39;##s&#39;, &#39;9&#39;, &#39;##3&#39;, &#39;%&#39;, &#39;of&#39;, &#39;ind&#39;, &#39;##ivid&#39;, &#39;##ual&#39;, &#39;f&#39;, &#39;&amp;&#39;, &#39;o&#39;, &#39;tr&#39;, &#39;##ad&#39;, &#39;##ers&#39;, &#39;made&#39;, &#39;loss&#39;, &#39;##es&#39;, &#39;between&#39;, &#39;f&#39;, &#39;##y&#39;, &#39;##2&#39;, &#39;##2&#39;, &#39;and&#39;, &#39;f&#39;, &#39;##y&#39;, &#39;##2&#39;, &#39;##4&#39;, &#39;.&#39;]
</pre></div>
</div>
</div>
</div>
<p><strong>Q2:</strong> Increase the vocabulary size to 10K, 15K and 32K. For each case, tokenize the same input with the newly learned vocabulary. Choose all the correct statements</p>
<p>Do change the <code class="docutils literal notranslate"><span class="pre">vocab_size</span></code> and retrain the model</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">vsizes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">10_000</span><span class="p">,</span> <span class="mi">15_000</span><span class="p">,</span> <span class="mi">32_000</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">vsizes</span><span class="p">:</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">BPE</span><span class="p">(</span><span class="n">unk_token</span><span class="o">=</span> <span class="s2">&quot;[UNK]&quot;</span><span class="p">)</span>
    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="n">tokenizer</span><span class="o">.</span><span class="n">normalizer</span> <span class="o">=</span> <span class="n">Lowercase</span><span class="p">()</span>
    <span class="n">tokenizer</span><span class="o">.</span><span class="n">pre_tokenizer</span> <span class="o">=</span> <span class="n">Whitespace</span><span class="p">()</span>

    <span class="n">trainer</span> <span class="o">=</span> <span class="n">BpeTrainer</span><span class="p">(</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">i</span><span class="p">,</span>
    <span class="n">special_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;[GO]&quot;</span><span class="p">,</span><span class="s2">&quot;[UNK]&quot;</span><span class="p">,</span><span class="s2">&quot;[PAD]&quot;</span><span class="p">,</span><span class="s2">&quot;[EOS]&quot;</span><span class="p">],</span>
    <span class="n">continuing_subword_prefix</span><span class="o">=</span><span class="s1">&#39;##&#39;</span><span class="p">)</span>
    
    <span class="n">samples_iterator</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">samples</span><span class="p">(</span><span class="n">bsize</span><span class="p">),</span> <span class="n">total</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">ds</span><span class="p">)</span> <span class="o">//</span> <span class="n">bsize</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Batches Trained for Vocab size </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">tokenizer</span><span class="o">.</span><span class="n">train_from_iterator</span><span class="p">(</span><span class="n">samples_iterator</span><span class="p">,</span> <span class="n">trainer</span><span class="o">=</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">ds</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;token size =&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">text</span><span class="p">)</span><span class="o">.</span><span class="n">tokens</span><span class="p">))</span>
    <span class="n">visualizer</span> <span class="o">=</span> <span class="n">EncodingVisualizer</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>
    <span class="n">visualizer</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Batches Trained for Vocab size 10000: 10573it [01:55, 91.61it/s]                            
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>token size = 28
</pre></div>
</div>
<div class="output text_html">
    <html>
        <head>
            <style>
                .tokenized-text {
    width:100%;
    padding:2rem;
    max-height: 400px;
    overflow-y: auto;
    box-sizing:border-box;
    line-height:4rem; /* Lots of space between lines */
    font-family: "Roboto Light", "Ubuntu Light", "Ubuntu", monospace;
    box-shadow: 2px 2px 2px rgba(0,0,0,0.2);
    background-color: rgba(0,0,0,0.01);
    letter-spacing:2px; /* Give some extra separation between chars */
}
.non-token{
    /* White space and other things the tokenizer ignores*/
    white-space: pre;
    letter-spacing:4px;
    border-top:1px solid #A0A0A0; /* A gentle border on top and bottom makes tabs more ovious*/
    border-bottom:1px solid #A0A0A0;
    line-height: 1rem;
    height: calc(100% - 2px);
}

.token {
    white-space: pre;
    position:relative;
    color:black;
    letter-spacing:2px;
}

.annotation{
    white-space:nowrap; /* Important - ensures that annotations appears even if the annotated text wraps a line */
    border-radius:4px;
    position:relative;
    width:fit-content;
}
.annotation:before {
    /*The before holds the text and the after holds the background*/
    z-index:1000; /* Make sure this is above the background */
    content:attr(data-label); /* The annotations label is on a data attribute */
    color:white;
    position:absolute;
    font-size:1rem;
    text-align:center;
    font-weight:bold;

    top:1.75rem;
    line-height:0;
    left:0;
    width:100%;
    padding:0.5rem 0;
    /* These make it so an annotation doesn't stretch beyond the annotated text if the label is longer*/
    overflow: hidden;
    white-space: nowrap;
    text-overflow:ellipsis;
}

.annotation:after {
    content:attr(data-label); /* The content defines the width of the annotation*/
    position:absolute;
    font-size:0.75rem;
    text-align:center;
    font-weight:bold;
    text-overflow:ellipsis;
    top:1.75rem;
    line-height:0;
    overflow: hidden;
    white-space: nowrap;

    left:0;
    width:100%; /* 100% of the parent, which is the annotation whose width is the tokens inside it*/

    padding:0.5rem 0;
    /* Nast hack below:
    We set the annotations color in code because we don't know the colors at css time.
    But you can't pass a color as a data attribute to get it into the pseudo element (this thing)
    So to get around that, annotations have the color set on them with a style attribute and then we
    can get the color with currentColor.
    Annotations wrap tokens and tokens set the color back to black
     */
    background-color: currentColor;
}
.annotation:hover::after, .annotation:hover::before{
    /* When the user hovers over an annotation expand the label to display in full
     */
    min-width: fit-content;
}

.annotation:hover{
    /* Emphasize the annotation start end with a border on hover*/
    border-color: currentColor;
    border: 2px solid;
}
.special-token:not(:empty){
    /*
    A none empty special token is like UNK (as opposed to CLS which has no representation in the text )
     */
    position:relative;
}
.special-token:empty::before{
    /* Special tokens that don't have text are displayed as pseudo elements so we dont select them with the mouse*/
    content:attr(data-stok);
    background:#202020;
    font-size:0.75rem;
    color:white;
    margin: 0 0.25rem;
    padding: 0.25rem;
    border-radius:4px
}

.special-token:not(:empty):before {
    /* Special tokens that have text (UNK) are displayed above the actual text*/
    content:attr(data-stok);
    position:absolute;
    bottom:1.75rem;
    min-width:100%;
    width:100%;
    height:1rem;
    line-height:1rem;
    font-size:1rem;
    text-align:center;
    color:white;
    font-weight:bold;
    background:#202020;
    border-radius:10%;
}
/*
We want to alternate the color of tokens, but we can't use nth child because tokens might be broken up by annotations
instead we apply even and odd class at generation time and color them that way
 */
.even-token{
    background:#DCDCDC	;
    border: 1px solid #DCDCDC;
}
.odd-token{
    background:#A0A0A0;
    border: 1px solid #A0A0A0;
}
.even-token.multi-token,.odd-token.multi-token{
    background:  repeating-linear-gradient(
    45deg,
    transparent,
    transparent 1px,
    #ccc 1px,
    #ccc 1px
    ),
    /* on "bottom" */
    linear-gradient(
    to bottom,
    #FFB6C1,
    #999
    );
}

.multi-token:hover::after {
    content:"This char has more than 1 token"; /* The content defines the width of the annotation*/
    color:white;
    background-color: black;
    position:absolute;
    font-size:0.75rem;
    text-align:center;
    font-weight:bold;
    text-overflow:ellipsis;
    top:1.75rem;
    line-height:0;
    overflow: hidden;
    white-space: nowrap;
    left:0;
    width:fit-content; /* 100% of the parent, which is the annotation whose width is the tokens inside it*/
    padding:0.5rem 0;
}

            </style>
        </head>
        <body>
            <div class="tokenized-text" dir=auto>
            <span class="token even-token"  >SEB</span><span class="token odd-token"  >I</span><span class="non-token"  > </span><span class="token even-token"  >study</span><span class="non-token"  > </span><span class="token odd-token"  >find</span><span class="token even-token"  >s</span><span class="non-token"  > </span><span class="token odd-token"  >9</span><span class="token even-token"  >3</span><span class="token odd-token"  >%</span><span class="non-token"  > </span><span class="token even-token"  >of</span><span class="non-token"  > </span><span class="token odd-token"  >ind</span><span class="token even-token"  >ivid</span><span class="token odd-token"  >ual</span><span class="non-token"  > </span><span class="token even-token"  >F</span><span class="token odd-token"  >&</span><span class="token even-token"  >O</span><span class="non-token"  > </span><span class="token odd-token"  >tr</span><span class="token even-token"  >ad</span><span class="token odd-token"  >ers</span><span class="non-token"  > </span><span class="token even-token"  >made</span><span class="non-token"  > </span><span class="token odd-token"  >loss</span><span class="token even-token"  >es</span><span class="non-token"  > </span><span class="token odd-token"  >between</span><span class="non-token"  > </span><span class="token even-token"  >F</span><span class="token odd-token"  >Y</span><span class="token even-token"  >2</span><span class="token odd-token"  >2</span><span class="non-token"  > </span><span class="token even-token"  >and</span><span class="non-token"  > </span><span class="token odd-token"  >F</span><span class="token even-token"  >Y</span><span class="token odd-token"  >2</span><span class="token even-token"  >4</span><span class="token odd-token"  >.</span>
            </div>
        </body>
    </html>
    </div><div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Batches Trained for Vocab size 15000: 10573it [03:27, 50.90it/s]                           
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>token size = 28
</pre></div>
</div>
<div class="output text_html">
    <html>
        <head>
            <style>
                .tokenized-text {
    width:100%;
    padding:2rem;
    max-height: 400px;
    overflow-y: auto;
    box-sizing:border-box;
    line-height:4rem; /* Lots of space between lines */
    font-family: "Roboto Light", "Ubuntu Light", "Ubuntu", monospace;
    box-shadow: 2px 2px 2px rgba(0,0,0,0.2);
    background-color: rgba(0,0,0,0.01);
    letter-spacing:2px; /* Give some extra separation between chars */
}
.non-token{
    /* White space and other things the tokenizer ignores*/
    white-space: pre;
    letter-spacing:4px;
    border-top:1px solid #A0A0A0; /* A gentle border on top and bottom makes tabs more ovious*/
    border-bottom:1px solid #A0A0A0;
    line-height: 1rem;
    height: calc(100% - 2px);
}

.token {
    white-space: pre;
    position:relative;
    color:black;
    letter-spacing:2px;
}

.annotation{
    white-space:nowrap; /* Important - ensures that annotations appears even if the annotated text wraps a line */
    border-radius:4px;
    position:relative;
    width:fit-content;
}
.annotation:before {
    /*The before holds the text and the after holds the background*/
    z-index:1000; /* Make sure this is above the background */
    content:attr(data-label); /* The annotations label is on a data attribute */
    color:white;
    position:absolute;
    font-size:1rem;
    text-align:center;
    font-weight:bold;

    top:1.75rem;
    line-height:0;
    left:0;
    width:100%;
    padding:0.5rem 0;
    /* These make it so an annotation doesn't stretch beyond the annotated text if the label is longer*/
    overflow: hidden;
    white-space: nowrap;
    text-overflow:ellipsis;
}

.annotation:after {
    content:attr(data-label); /* The content defines the width of the annotation*/
    position:absolute;
    font-size:0.75rem;
    text-align:center;
    font-weight:bold;
    text-overflow:ellipsis;
    top:1.75rem;
    line-height:0;
    overflow: hidden;
    white-space: nowrap;

    left:0;
    width:100%; /* 100% of the parent, which is the annotation whose width is the tokens inside it*/

    padding:0.5rem 0;
    /* Nast hack below:
    We set the annotations color in code because we don't know the colors at css time.
    But you can't pass a color as a data attribute to get it into the pseudo element (this thing)
    So to get around that, annotations have the color set on them with a style attribute and then we
    can get the color with currentColor.
    Annotations wrap tokens and tokens set the color back to black
     */
    background-color: currentColor;
}
.annotation:hover::after, .annotation:hover::before{
    /* When the user hovers over an annotation expand the label to display in full
     */
    min-width: fit-content;
}

.annotation:hover{
    /* Emphasize the annotation start end with a border on hover*/
    border-color: currentColor;
    border: 2px solid;
}
.special-token:not(:empty){
    /*
    A none empty special token is like UNK (as opposed to CLS which has no representation in the text )
     */
    position:relative;
}
.special-token:empty::before{
    /* Special tokens that don't have text are displayed as pseudo elements so we dont select them with the mouse*/
    content:attr(data-stok);
    background:#202020;
    font-size:0.75rem;
    color:white;
    margin: 0 0.25rem;
    padding: 0.25rem;
    border-radius:4px
}

.special-token:not(:empty):before {
    /* Special tokens that have text (UNK) are displayed above the actual text*/
    content:attr(data-stok);
    position:absolute;
    bottom:1.75rem;
    min-width:100%;
    width:100%;
    height:1rem;
    line-height:1rem;
    font-size:1rem;
    text-align:center;
    color:white;
    font-weight:bold;
    background:#202020;
    border-radius:10%;
}
/*
We want to alternate the color of tokens, but we can't use nth child because tokens might be broken up by annotations
instead we apply even and odd class at generation time and color them that way
 */
.even-token{
    background:#DCDCDC	;
    border: 1px solid #DCDCDC;
}
.odd-token{
    background:#A0A0A0;
    border: 1px solid #A0A0A0;
}
.even-token.multi-token,.odd-token.multi-token{
    background:  repeating-linear-gradient(
    45deg,
    transparent,
    transparent 1px,
    #ccc 1px,
    #ccc 1px
    ),
    /* on "bottom" */
    linear-gradient(
    to bottom,
    #FFB6C1,
    #999
    );
}

.multi-token:hover::after {
    content:"This char has more than 1 token"; /* The content defines the width of the annotation*/
    color:white;
    background-color: black;
    position:absolute;
    font-size:0.75rem;
    text-align:center;
    font-weight:bold;
    text-overflow:ellipsis;
    top:1.75rem;
    line-height:0;
    overflow: hidden;
    white-space: nowrap;
    left:0;
    width:fit-content; /* 100% of the parent, which is the annotation whose width is the tokens inside it*/
    padding:0.5rem 0;
}

            </style>
        </head>
        <body>
            <div class="tokenized-text" dir=auto>
            <span class="token even-token"  >SEB</span><span class="token odd-token"  >I</span><span class="non-token"  > </span><span class="token even-token"  >study</span><span class="non-token"  > </span><span class="token odd-token"  >find</span><span class="token even-token"  >s</span><span class="non-token"  > </span><span class="token odd-token"  >9</span><span class="token even-token"  >3</span><span class="token odd-token"  >%</span><span class="non-token"  > </span><span class="token even-token"  >of</span><span class="non-token"  > </span><span class="token odd-token"  >ind</span><span class="token even-token"  >ivid</span><span class="token odd-token"  >ual</span><span class="non-token"  > </span><span class="token even-token"  >F</span><span class="token odd-token"  >&</span><span class="token even-token"  >O</span><span class="non-token"  > </span><span class="token odd-token"  >tr</span><span class="token even-token"  >ad</span><span class="token odd-token"  >ers</span><span class="non-token"  > </span><span class="token even-token"  >made</span><span class="non-token"  > </span><span class="token odd-token"  >loss</span><span class="token even-token"  >es</span><span class="non-token"  > </span><span class="token odd-token"  >between</span><span class="non-token"  > </span><span class="token even-token"  >F</span><span class="token odd-token"  >Y</span><span class="token even-token"  >2</span><span class="token odd-token"  >2</span><span class="non-token"  > </span><span class="token even-token"  >and</span><span class="non-token"  > </span><span class="token odd-token"  >F</span><span class="token even-token"  >Y</span><span class="token odd-token"  >2</span><span class="token even-token"  >4</span><span class="token odd-token"  >.</span>
            </div>
        </body>
    </html>
    </div><div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Batches Trained for Vocab size 32000: 10573it [04:17, 41.12it/s]                           
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>token size = 25
</pre></div>
</div>
<div class="output text_html">
    <html>
        <head>
            <style>
                .tokenized-text {
    width:100%;
    padding:2rem;
    max-height: 400px;
    overflow-y: auto;
    box-sizing:border-box;
    line-height:4rem; /* Lots of space between lines */
    font-family: "Roboto Light", "Ubuntu Light", "Ubuntu", monospace;
    box-shadow: 2px 2px 2px rgba(0,0,0,0.2);
    background-color: rgba(0,0,0,0.01);
    letter-spacing:2px; /* Give some extra separation between chars */
}
.non-token{
    /* White space and other things the tokenizer ignores*/
    white-space: pre;
    letter-spacing:4px;
    border-top:1px solid #A0A0A0; /* A gentle border on top and bottom makes tabs more ovious*/
    border-bottom:1px solid #A0A0A0;
    line-height: 1rem;
    height: calc(100% - 2px);
}

.token {
    white-space: pre;
    position:relative;
    color:black;
    letter-spacing:2px;
}

.annotation{
    white-space:nowrap; /* Important - ensures that annotations appears even if the annotated text wraps a line */
    border-radius:4px;
    position:relative;
    width:fit-content;
}
.annotation:before {
    /*The before holds the text and the after holds the background*/
    z-index:1000; /* Make sure this is above the background */
    content:attr(data-label); /* The annotations label is on a data attribute */
    color:white;
    position:absolute;
    font-size:1rem;
    text-align:center;
    font-weight:bold;

    top:1.75rem;
    line-height:0;
    left:0;
    width:100%;
    padding:0.5rem 0;
    /* These make it so an annotation doesn't stretch beyond the annotated text if the label is longer*/
    overflow: hidden;
    white-space: nowrap;
    text-overflow:ellipsis;
}

.annotation:after {
    content:attr(data-label); /* The content defines the width of the annotation*/
    position:absolute;
    font-size:0.75rem;
    text-align:center;
    font-weight:bold;
    text-overflow:ellipsis;
    top:1.75rem;
    line-height:0;
    overflow: hidden;
    white-space: nowrap;

    left:0;
    width:100%; /* 100% of the parent, which is the annotation whose width is the tokens inside it*/

    padding:0.5rem 0;
    /* Nast hack below:
    We set the annotations color in code because we don't know the colors at css time.
    But you can't pass a color as a data attribute to get it into the pseudo element (this thing)
    So to get around that, annotations have the color set on them with a style attribute and then we
    can get the color with currentColor.
    Annotations wrap tokens and tokens set the color back to black
     */
    background-color: currentColor;
}
.annotation:hover::after, .annotation:hover::before{
    /* When the user hovers over an annotation expand the label to display in full
     */
    min-width: fit-content;
}

.annotation:hover{
    /* Emphasize the annotation start end with a border on hover*/
    border-color: currentColor;
    border: 2px solid;
}
.special-token:not(:empty){
    /*
    A none empty special token is like UNK (as opposed to CLS which has no representation in the text )
     */
    position:relative;
}
.special-token:empty::before{
    /* Special tokens that don't have text are displayed as pseudo elements so we dont select them with the mouse*/
    content:attr(data-stok);
    background:#202020;
    font-size:0.75rem;
    color:white;
    margin: 0 0.25rem;
    padding: 0.25rem;
    border-radius:4px
}

.special-token:not(:empty):before {
    /* Special tokens that have text (UNK) are displayed above the actual text*/
    content:attr(data-stok);
    position:absolute;
    bottom:1.75rem;
    min-width:100%;
    width:100%;
    height:1rem;
    line-height:1rem;
    font-size:1rem;
    text-align:center;
    color:white;
    font-weight:bold;
    background:#202020;
    border-radius:10%;
}
/*
We want to alternate the color of tokens, but we can't use nth child because tokens might be broken up by annotations
instead we apply even and odd class at generation time and color them that way
 */
.even-token{
    background:#DCDCDC	;
    border: 1px solid #DCDCDC;
}
.odd-token{
    background:#A0A0A0;
    border: 1px solid #A0A0A0;
}
.even-token.multi-token,.odd-token.multi-token{
    background:  repeating-linear-gradient(
    45deg,
    transparent,
    transparent 1px,
    #ccc 1px,
    #ccc 1px
    ),
    /* on "bottom" */
    linear-gradient(
    to bottom,
    #FFB6C1,
    #999
    );
}

.multi-token:hover::after {
    content:"This char has more than 1 token"; /* The content defines the width of the annotation*/
    color:white;
    background-color: black;
    position:absolute;
    font-size:0.75rem;
    text-align:center;
    font-weight:bold;
    text-overflow:ellipsis;
    top:1.75rem;
    line-height:0;
    overflow: hidden;
    white-space: nowrap;
    left:0;
    width:fit-content; /* 100% of the parent, which is the annotation whose width is the tokens inside it*/
    padding:0.5rem 0;
}

            </style>
        </head>
        <body>
            <div class="tokenized-text" dir=auto>
            <span class="token even-token"  >SEB</span><span class="token odd-token"  >I</span><span class="non-token"  > </span><span class="token even-token"  >study</span><span class="non-token"  > </span><span class="token odd-token"  >find</span><span class="token even-token"  >s</span><span class="non-token"  > </span><span class="token odd-token"  >9</span><span class="token even-token"  >3</span><span class="token odd-token"  >%</span><span class="non-token"  > </span><span class="token even-token"  >of</span><span class="non-token"  > </span><span class="token odd-token"  >ind</span><span class="token even-token"  >ivid</span><span class="token odd-token"  >ual</span><span class="non-token"  > </span><span class="token even-token"  >F</span><span class="token odd-token"  >&</span><span class="token even-token"  >O</span><span class="non-token"  > </span><span class="token odd-token"  >tr</span><span class="token even-token"  >ad</span><span class="token odd-token"  >ers</span><span class="non-token"  > </span><span class="token even-token"  >made</span><span class="non-token"  > </span><span class="token odd-token"  >loss</span><span class="token even-token"  >es</span><span class="non-token"  > </span><span class="token odd-token"  >between</span><span class="non-token"  > </span><span class="token even-token"  >F</span><span class="token odd-token"  >Y</span><span class="token even-token"  >2</span><span class="token odd-token"  >2</span><span class="non-token"  > </span><span class="token even-token"  >and</span><span class="non-token"  > </span><span class="token odd-token"  >F</span><span class="token even-token"  >Y</span><span class="token odd-token"  >2</span><span class="token even-token"  >4</span><span class="token odd-token"  >.</span>
            </div>
        </body>
    </html>
    </div></div>
</div>
<p><strong>Q3</strong>: Download the pre-trained tokenizer file “hopper.json” used in the lecture, from <a class="reference external" href="https://drive.google.com/file/d/1QNnyh8iMN-IqW_h1w8gAMtw09Em7-e1e/view?usp=sharing">here</a>. The tokenizer was trained on all 70 million samples in the BookCorpus dataset. Tokenize the same input text using this “hopper” tokenizer. How many tokens are there?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">trained_tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">BPE</span><span class="p">())</span>
<span class="n">trained_tokenizer</span> <span class="o">=</span> <span class="n">trained_tokenizer</span><span class="o">.</span><span class="n">from_file</span><span class="p">(</span><span class="s1">&#39;hopper.json&#39;</span><span class="p">)</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="n">trained_tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">text</span><span class="p">)</span><span class="o">.</span><span class="n">tokens</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">))</span>
<span class="n">EncodingVisualizer</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">=</span><span class="n">trained_tokenizer</span><span class="p">)(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>25
</pre></div>
</div>
<div class="output text_html">
    <html>
        <head>
            <style>
                .tokenized-text {
    width:100%;
    padding:2rem;
    max-height: 400px;
    overflow-y: auto;
    box-sizing:border-box;
    line-height:4rem; /* Lots of space between lines */
    font-family: "Roboto Light", "Ubuntu Light", "Ubuntu", monospace;
    box-shadow: 2px 2px 2px rgba(0,0,0,0.2);
    background-color: rgba(0,0,0,0.01);
    letter-spacing:2px; /* Give some extra separation between chars */
}
.non-token{
    /* White space and other things the tokenizer ignores*/
    white-space: pre;
    letter-spacing:4px;
    border-top:1px solid #A0A0A0; /* A gentle border on top and bottom makes tabs more ovious*/
    border-bottom:1px solid #A0A0A0;
    line-height: 1rem;
    height: calc(100% - 2px);
}

.token {
    white-space: pre;
    position:relative;
    color:black;
    letter-spacing:2px;
}

.annotation{
    white-space:nowrap; /* Important - ensures that annotations appears even if the annotated text wraps a line */
    border-radius:4px;
    position:relative;
    width:fit-content;
}
.annotation:before {
    /*The before holds the text and the after holds the background*/
    z-index:1000; /* Make sure this is above the background */
    content:attr(data-label); /* The annotations label is on a data attribute */
    color:white;
    position:absolute;
    font-size:1rem;
    text-align:center;
    font-weight:bold;

    top:1.75rem;
    line-height:0;
    left:0;
    width:100%;
    padding:0.5rem 0;
    /* These make it so an annotation doesn't stretch beyond the annotated text if the label is longer*/
    overflow: hidden;
    white-space: nowrap;
    text-overflow:ellipsis;
}

.annotation:after {
    content:attr(data-label); /* The content defines the width of the annotation*/
    position:absolute;
    font-size:0.75rem;
    text-align:center;
    font-weight:bold;
    text-overflow:ellipsis;
    top:1.75rem;
    line-height:0;
    overflow: hidden;
    white-space: nowrap;

    left:0;
    width:100%; /* 100% of the parent, which is the annotation whose width is the tokens inside it*/

    padding:0.5rem 0;
    /* Nast hack below:
    We set the annotations color in code because we don't know the colors at css time.
    But you can't pass a color as a data attribute to get it into the pseudo element (this thing)
    So to get around that, annotations have the color set on them with a style attribute and then we
    can get the color with currentColor.
    Annotations wrap tokens and tokens set the color back to black
     */
    background-color: currentColor;
}
.annotation:hover::after, .annotation:hover::before{
    /* When the user hovers over an annotation expand the label to display in full
     */
    min-width: fit-content;
}

.annotation:hover{
    /* Emphasize the annotation start end with a border on hover*/
    border-color: currentColor;
    border: 2px solid;
}
.special-token:not(:empty){
    /*
    A none empty special token is like UNK (as opposed to CLS which has no representation in the text )
     */
    position:relative;
}
.special-token:empty::before{
    /* Special tokens that don't have text are displayed as pseudo elements so we dont select them with the mouse*/
    content:attr(data-stok);
    background:#202020;
    font-size:0.75rem;
    color:white;
    margin: 0 0.25rem;
    padding: 0.25rem;
    border-radius:4px
}

.special-token:not(:empty):before {
    /* Special tokens that have text (UNK) are displayed above the actual text*/
    content:attr(data-stok);
    position:absolute;
    bottom:1.75rem;
    min-width:100%;
    width:100%;
    height:1rem;
    line-height:1rem;
    font-size:1rem;
    text-align:center;
    color:white;
    font-weight:bold;
    background:#202020;
    border-radius:10%;
}
/*
We want to alternate the color of tokens, but we can't use nth child because tokens might be broken up by annotations
instead we apply even and odd class at generation time and color them that way
 */
.even-token{
    background:#DCDCDC	;
    border: 1px solid #DCDCDC;
}
.odd-token{
    background:#A0A0A0;
    border: 1px solid #A0A0A0;
}
.even-token.multi-token,.odd-token.multi-token{
    background:  repeating-linear-gradient(
    45deg,
    transparent,
    transparent 1px,
    #ccc 1px,
    #ccc 1px
    ),
    /* on "bottom" */
    linear-gradient(
    to bottom,
    #FFB6C1,
    #999
    );
}

.multi-token:hover::after {
    content:"This char has more than 1 token"; /* The content defines the width of the annotation*/
    color:white;
    background-color: black;
    position:absolute;
    font-size:0.75rem;
    text-align:center;
    font-weight:bold;
    text-overflow:ellipsis;
    top:1.75rem;
    line-height:0;
    overflow: hidden;
    white-space: nowrap;
    left:0;
    width:fit-content; /* 100% of the parent, which is the annotation whose width is the tokens inside it*/
    padding:0.5rem 0;
}

            </style>
        </head>
        <body>
            <div class="tokenized-text" dir=auto>
            <span class="token even-token"  >SEB</span><span class="token odd-token"  >I</span><span class="non-token"  > </span><span class="token even-token"  >study</span><span class="non-token"  > </span><span class="token odd-token"  >finds</span><span class="non-token"  > </span><span class="token even-token"  >9</span><span class="token odd-token"  >3</span><span class="token even-token"  >%</span><span class="non-token"  > </span><span class="token odd-token"  >of</span><span class="non-token"  > </span><span class="token even-token"  >individual</span><span class="non-token"  > </span><span class="token odd-token"  >F</span><span class="token even-token"  >&</span><span class="token odd-token"  >O</span><span class="non-token"  > </span><span class="token even-token"  >traders</span><span class="non-token"  > </span><span class="token odd-token"  >made</span><span class="non-token"  > </span><span class="token even-token"  >losses</span><span class="non-token"  > </span><span class="token odd-token"  >between</span><span class="non-token"  > </span><span class="token even-token"  >F</span><span class="token odd-token"  >Y</span><span class="token even-token"  >22</span><span class="non-token"  > </span><span class="token odd-token"  >and</span><span class="non-token"  > </span><span class="token even-token"  >F</span><span class="token odd-token"  >Y</span><span class="token even-token"  >2</span><span class="token odd-token"  >4</span><span class="token even-token"  >.</span>
            </div>
        </body>
    </html>
    </div></div>
</div>
<p><strong>Q4</strong>: Suppose we know that the acronym “FY” will likely appear very frequently in most of the input text (assume the text comes from the financial domain). Therefore, we hope that adding it manually to the vocabulary might help. Add the token “FY” to the vocabulary and tokenize the input text. Enter the number of tokens produced.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">trained_tokenizer</span><span class="o">.</span><span class="n">get_vocab_size</span><span class="p">())</span>
<span class="n">trained_tokenizer</span><span class="o">.</span><span class="n">add_tokens</span><span class="p">([</span><span class="s1">&#39;FY&#39;</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">trained_tokenizer</span><span class="o">.</span><span class="n">get_vocab_size</span><span class="p">())</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="n">trained_tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">text</span><span class="p">)</span><span class="o">.</span><span class="n">tokens</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">))</span>
<span class="n">EncodingVisualizer</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">=</span><span class="n">trained_tokenizer</span><span class="p">)(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>32000
32001
22
</pre></div>
</div>
<div class="output text_html">
    <html>
        <head>
            <style>
                .tokenized-text {
    width:100%;
    padding:2rem;
    max-height: 400px;
    overflow-y: auto;
    box-sizing:border-box;
    line-height:4rem; /* Lots of space between lines */
    font-family: "Roboto Light", "Ubuntu Light", "Ubuntu", monospace;
    box-shadow: 2px 2px 2px rgba(0,0,0,0.2);
    background-color: rgba(0,0,0,0.01);
    letter-spacing:2px; /* Give some extra separation between chars */
}
.non-token{
    /* White space and other things the tokenizer ignores*/
    white-space: pre;
    letter-spacing:4px;
    border-top:1px solid #A0A0A0; /* A gentle border on top and bottom makes tabs more ovious*/
    border-bottom:1px solid #A0A0A0;
    line-height: 1rem;
    height: calc(100% - 2px);
}

.token {
    white-space: pre;
    position:relative;
    color:black;
    letter-spacing:2px;
}

.annotation{
    white-space:nowrap; /* Important - ensures that annotations appears even if the annotated text wraps a line */
    border-radius:4px;
    position:relative;
    width:fit-content;
}
.annotation:before {
    /*The before holds the text and the after holds the background*/
    z-index:1000; /* Make sure this is above the background */
    content:attr(data-label); /* The annotations label is on a data attribute */
    color:white;
    position:absolute;
    font-size:1rem;
    text-align:center;
    font-weight:bold;

    top:1.75rem;
    line-height:0;
    left:0;
    width:100%;
    padding:0.5rem 0;
    /* These make it so an annotation doesn't stretch beyond the annotated text if the label is longer*/
    overflow: hidden;
    white-space: nowrap;
    text-overflow:ellipsis;
}

.annotation:after {
    content:attr(data-label); /* The content defines the width of the annotation*/
    position:absolute;
    font-size:0.75rem;
    text-align:center;
    font-weight:bold;
    text-overflow:ellipsis;
    top:1.75rem;
    line-height:0;
    overflow: hidden;
    white-space: nowrap;

    left:0;
    width:100%; /* 100% of the parent, which is the annotation whose width is the tokens inside it*/

    padding:0.5rem 0;
    /* Nast hack below:
    We set the annotations color in code because we don't know the colors at css time.
    But you can't pass a color as a data attribute to get it into the pseudo element (this thing)
    So to get around that, annotations have the color set on them with a style attribute and then we
    can get the color with currentColor.
    Annotations wrap tokens and tokens set the color back to black
     */
    background-color: currentColor;
}
.annotation:hover::after, .annotation:hover::before{
    /* When the user hovers over an annotation expand the label to display in full
     */
    min-width: fit-content;
}

.annotation:hover{
    /* Emphasize the annotation start end with a border on hover*/
    border-color: currentColor;
    border: 2px solid;
}
.special-token:not(:empty){
    /*
    A none empty special token is like UNK (as opposed to CLS which has no representation in the text )
     */
    position:relative;
}
.special-token:empty::before{
    /* Special tokens that don't have text are displayed as pseudo elements so we dont select them with the mouse*/
    content:attr(data-stok);
    background:#202020;
    font-size:0.75rem;
    color:white;
    margin: 0 0.25rem;
    padding: 0.25rem;
    border-radius:4px
}

.special-token:not(:empty):before {
    /* Special tokens that have text (UNK) are displayed above the actual text*/
    content:attr(data-stok);
    position:absolute;
    bottom:1.75rem;
    min-width:100%;
    width:100%;
    height:1rem;
    line-height:1rem;
    font-size:1rem;
    text-align:center;
    color:white;
    font-weight:bold;
    background:#202020;
    border-radius:10%;
}
/*
We want to alternate the color of tokens, but we can't use nth child because tokens might be broken up by annotations
instead we apply even and odd class at generation time and color them that way
 */
.even-token{
    background:#DCDCDC	;
    border: 1px solid #DCDCDC;
}
.odd-token{
    background:#A0A0A0;
    border: 1px solid #A0A0A0;
}
.even-token.multi-token,.odd-token.multi-token{
    background:  repeating-linear-gradient(
    45deg,
    transparent,
    transparent 1px,
    #ccc 1px,
    #ccc 1px
    ),
    /* on "bottom" */
    linear-gradient(
    to bottom,
    #FFB6C1,
    #999
    );
}

.multi-token:hover::after {
    content:"This char has more than 1 token"; /* The content defines the width of the annotation*/
    color:white;
    background-color: black;
    position:absolute;
    font-size:0.75rem;
    text-align:center;
    font-weight:bold;
    text-overflow:ellipsis;
    top:1.75rem;
    line-height:0;
    overflow: hidden;
    white-space: nowrap;
    left:0;
    width:fit-content; /* 100% of the parent, which is the annotation whose width is the tokens inside it*/
    padding:0.5rem 0;
}

            </style>
        </head>
        <body>
            <div class="tokenized-text" dir=auto>
            <span class="token even-token"  >SEB</span><span class="token odd-token"  >I</span><span class="non-token"  > </span><span class="token even-token"  >study</span><span class="non-token"  > </span><span class="token odd-token"  >finds</span><span class="non-token"  > </span><span class="token even-token"  >9</span><span class="token odd-token"  >3</span><span class="token even-token"  >%</span><span class="non-token"  > </span><span class="token odd-token"  >of</span><span class="non-token"  > </span><span class="token even-token"  >individual</span><span class="non-token"  > </span><span class="token odd-token"  >F</span><span class="token even-token"  >&</span><span class="token odd-token"  >O</span><span class="non-token"  > </span><span class="token even-token"  >traders</span><span class="non-token"  > </span><span class="token odd-token"  >made</span><span class="non-token"  > </span><span class="token even-token"  >losses</span><span class="non-token"  > </span><span class="token odd-token"  >between</span><span class="non-token"  > </span><span class="token even-token"  >FY</span><span class="token odd-token"  >22</span><span class="non-token"  > </span><span class="token even-token"  >and</span><span class="non-token"  > </span><span class="token odd-token"  >FY</span><span class="token even-token"  >24</span><span class="token odd-token"  >.</span>
            </div>
        </body>
    </html>
    </div></div>
</div>
<p><strong>Q5</strong> Load the “bert-base-uncased” and “gpt2” tokenizers (use AutoTokenizer function from transformers). Which of the following special tokens are used in these tokenizers?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span>
<span class="n">bert_tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>
<span class="n">gpt2_tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;gpt2&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;bbu special tokens - </span><span class="si">{</span><span class="n">bert_tokenizer</span><span class="o">.</span><span class="n">all_special_tokens</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;gpt2 special tokens - </span><span class="si">{</span><span class="n">gpt2_tokenizer</span><span class="o">.</span><span class="n">all_special_tokens</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>bbu special tokens - [&#39;[UNK]&#39;, &#39;[SEP]&#39;, &#39;[PAD]&#39;, &#39;[CLS]&#39;, &#39;[MASK]&#39;]
gpt2 special tokens - [&#39;&lt;|endoftext|&gt;&#39;]
</pre></div>
</div>
</div>
</div>
<p><strong>Q6</strong> By now, we have four tokenizers. <br></p>
<ol class="arabic simple">
<li><p>Custom tokenizer (vocab size 32K, trained on 10 million samples) <br></p></li>
<li><p>bert-base-uncased <br></p></li>
<li><p>gpt2 <br></p></li>
<li><p>hopper <br></p></li>
</ol>
<p>Use these four tokenizers to count the number of tokens for the entire “imdb” dataset (drop the “unsupervised” part of the dataset). Enter the tokenizers in order such that the size of the dataset (measured in tokens) as returned by the tokenizers is in decreasing order. For example, if the first tokenizer yields the smallest number of tokens and the fourth tokenizer yields the largest, you would enter 1234 (without any spaces).”</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">hopper_tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">BPE</span><span class="p">())</span>
<span class="n">hopper_tokenizer</span> <span class="o">=</span> <span class="n">trained_tokenizer</span><span class="o">.</span><span class="n">from_file</span><span class="p">(</span><span class="s1">&#39;hopper.json&#39;</span><span class="p">)</span>

<span class="n">imdb</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;stanfordnlp/imdb&quot;</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s1">&#39;train+test&#39;</span><span class="p">)</span>
<span class="n">imdb</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>c:\Users\dell\AppData\Local\Programs\Python\Python311\Lib\site-packages\huggingface_hub\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\Users\dell\.cache\huggingface\hub\datasets--stanfordnlp--imdb. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.
To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development
  warnings.warn(message)
Generating train split: 100%|██████████| 25000/25000 [00:00&lt;00:00, 149950.81 examples/s]
Generating test split: 100%|██████████| 25000/25000 [00:00&lt;00:00, 363102.96 examples/s]
Generating unsupervised split: 100%|██████████| 50000/50000 [00:00&lt;00:00, 364136.93 examples/s]
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Dataset({
    features: [&#39;text&#39;, &#39;label&#39;],
    num_rows: 50000
})
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="p">[</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">hopper_tokenizer</span><span class="p">]:</span>
    <span class="n">num_tokens</span> <span class="o">=</span><span class="mi">0</span>
    <span class="k">for</span> <span class="n">sample</span> <span class="ow">in</span> <span class="n">imdb</span><span class="p">:</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">tokens</span>
        <span class="n">num_tokens</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">num_tokens</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>15352840
13526933
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="p">[</span><span class="n">bert_tokenizer</span><span class="p">,</span> <span class="n">gpt2_tokenizer</span><span class="p">]:</span>
    <span class="n">num_tokens</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">sample</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">imdb</span><span class="p">,</span> <span class="n">total</span><span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">imdb</span><span class="p">)):</span>
        <span class="n">token_ids</span> <span class="o">=</span> <span class="n">t</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">])[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">]</span>
        <span class="n">num_tokens</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="n">token_ids</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">num_tokens</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  0%|          | 0/50000 [00:00&lt;?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (720 &gt; 512). Running this sequence through the model will result in indexing errors
100%|██████████| 50000/50000 [00:48&lt;00:00, 1025.08it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>15516058
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  0%|          | 0/50000 [00:00&lt;?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1168 &gt; 1024). Running this sequence through the model will result in indexing errors
100%|██████████| 50000/50000 [00:50&lt;00:00, 998.79it/s] 
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>14812432
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./notebooks"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="DLP_Week_1.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Week 1</p>
      </div>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tokenization">Tokenization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#byte-pair-encoding">Byte Pair Encoding</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#word-piece-tokenizer">Word-Piece Tokenizer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#practice">Practice</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#load-dataset">Load Dataset</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tokenize">Tokenize</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vocabulary">Vocabulary</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#encoding">Encoding</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#batch-encoding">Batch Encoding</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#testing">Testing</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#assignment">Assignment</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Yash Mishra
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>