{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "1. Tokenizer → 2. Token to Id → 3. embeddings → 4. Language Model → 5. Id to Token → 6. Token to words\n",
    "\n",
    "Encoder [1-2]\n",
    "Decoder [5-6]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Byte Pair Encoding\n",
    "Small Vocabulary → Large Sequence Length\n",
    "Large Vocabulary → Problem in Computing Softmax\n",
    "Encodes language without spaces\n",
    "→ Based on Frequency\n",
    "→ Fertility → No. of subwords broken out from a word\n",
    "→ 1 Merge = 1 Addition to Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word-Piece Tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 1.18G/1.18G [03:46<00:00, 5.21MB/s]  \n",
      "Generating train split: 100%|██████████| 74004228/74004228 [14:07<00:00, 87321.80 examples/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 74004228\n",
      "})\n",
      "0 : usually , he would be tearing around the living room , playing with his toys .\n",
      "1 : but just one look at a minion sent him practically catatonic .\n",
      "2 : that had been megan 's plan when she got him dressed earlier .\n",
      "3 : he 'd seen the movie almost by mistake , considering he was a little young for the pg cartoon , but with older cousins , along with her brothers , mason was often exposed to things that were older .\n",
      "4 : she liked to think being surrounded by adults and older kids was one reason why he was a such a good talker for his age .\n",
      "5 : `` are n't you being a good boy ? ''\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset('bookcorpus',split='all', trust_remote_code=True)\n",
    "print(ds)\n",
    "for idx,sample in enumerate(ds[0:6]['text']):\n",
    "    print(f'{idx} : {sample}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|**Component** |**Choice**  |\n",
    "|:------------:|:----------:|\n",
    "|normalizer    |Lowercase   |\n",
    "|pre-tokenizer |Whitespace  |\n",
    "|model         | BPE        |\n",
    "|postprocessor | None       |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.normalizers import Lowercase \n",
    "from tokenizers.pre_tokenizers import Whitespace \n",
    "from tokenizers.models import BPE\n",
    "\n",
    "try:\n",
    "    model = BPE(unk_token=\"[UNK]\")\n",
    "    tokenizer = Tokenizer(model)\n",
    "    tokenizer.normalizer = Lowercase()\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: {e}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "from tokenizers.trainers import BpeTrainer\n",
    "trainer = BpeTrainer(vocab_size=32000,special_tokens=[\"[PAD]\",\"[UNK]\"],continuing_subword_prefix='##')\n",
    "# pipeline is done\n",
    "\n",
    "def get_examples(batch_size=1000):\n",
    "    for i in range(0, len(ds), batch_size):\n",
    "        yield ds[i : i + batch_size]['text']    \n",
    "\n",
    "from multiprocessing import cpu_count\n",
    "cpus = cpu_count()\n",
    "print(cpus)\n",
    "\n",
    "from tqdm import tqdm\n",
    "example_iterator = get_examples(batch_size=10000)\n",
    "example_iterator_with_progress = tqdm(example_iterator, total=len(ds), desc=\"Training tokenizer\")\n",
    "tokenizer.train_from_iterator(example_iterator_with_progress, trainer=trainer, length=len(ds))\n",
    "\n",
    "# tokenizer.train_from_iterator(get_examples(batch_size=10000),trainer=trainer,length=len(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "try:\n",
    "    save_dir = 'model'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    tokenizer.model.save(save_dir,prefix='hopper')\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of merges:31871\n",
      "vocab size:32000\n"
     ]
    }
   ],
   "source": [
    "with open('model/hopper-merges.txt','r') as file:    \n",
    "    lines = file.readlines() \n",
    "print(f'Number of merges:{len(lines)}') \n",
    "print(f'vocab size:{tokenizer.get_vocab_size()}') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('[PAD]', 0),\n",
       " ('[UNK]', 1),\n",
       " ('\\x13', 2),\n",
       " ('\\x14', 3),\n",
       " ('\\x18', 4),\n",
       " ('\\x19', 5),\n",
       " ('\\x1c', 6),\n",
       " ('\\x1d', 7),\n",
       " ('\\x1f', 8),\n",
       " ('!', 9),\n",
       " ('#', 10),\n",
       " ('$', 11),\n",
       " ('%', 12),\n",
       " ('&', 13),\n",
       " (\"'\", 14),\n",
       " ('(', 15),\n",
       " (')', 16),\n",
       " ('*', 17),\n",
       " ('+', 18),\n",
       " (',', 19),\n",
       " ('-', 20),\n",
       " ('.', 21),\n",
       " ('/', 22),\n",
       " ('0', 23),\n",
       " ('1', 24),\n",
       " ('2', 25),\n",
       " ('3', 26),\n",
       " ('4', 27),\n",
       " ('5', 28),\n",
       " ('6', 29),\n",
       " ('7', 30),\n",
       " ('8', 31),\n",
       " ('9', 32),\n",
       " (':', 33),\n",
       " (';', 34),\n",
       " ('<', 35),\n",
       " ('=', 36),\n",
       " ('>', 37),\n",
       " ('?', 38),\n",
       " ('@', 39),\n",
       " ('[', 40),\n",
       " ('\\\\', 41),\n",
       " (']', 42),\n",
       " ('^', 43),\n",
       " ('_', 44),\n",
       " ('`', 45),\n",
       " ('a', 46),\n",
       " ('b', 47),\n",
       " ('c', 48),\n",
       " ('d', 49),\n",
       " ('e', 50),\n",
       " ('f', 51),\n",
       " ('g', 52),\n",
       " ('h', 53),\n",
       " ('i', 54),\n",
       " ('j', 55),\n",
       " ('k', 56),\n",
       " ('l', 57),\n",
       " ('m', 58),\n",
       " ('n', 59),\n",
       " ('o', 60),\n",
       " ('p', 61),\n",
       " ('q', 62),\n",
       " ('r', 63),\n",
       " ('s', 64),\n",
       " ('t', 65),\n",
       " ('u', 66),\n",
       " ('v', 67),\n",
       " ('w', 68),\n",
       " ('x', 69),\n",
       " ('y', 70),\n",
       " ('z', 71),\n",
       " ('{', 72),\n",
       " ('|', 73),\n",
       " ('}', 74),\n",
       " ('~', 75),\n",
       " ('\\x7f', 76),\n",
       " ('##g', 77),\n",
       " ('##i', 78),\n",
       " ('##t', 79),\n",
       " ('##a', 80),\n",
       " ('##d', 81),\n",
       " ('##o', 82),\n",
       " ('##s', 83),\n",
       " ('##u', 84),\n",
       " ('##l', 85),\n",
       " ('##n', 86),\n",
       " ('##c', 87),\n",
       " ('##p', 88),\n",
       " ('##y', 89),\n",
       " ('##h', 90),\n",
       " ('##r', 91),\n",
       " ('##b', 92),\n",
       " ('##m', 93),\n",
       " ('##e', 94),\n",
       " ('##z', 95),\n",
       " ('##0', 96),\n",
       " ('##k', 97),\n",
       " ('##7', 98),\n",
       " ('##2', 99),\n",
       " ('##4', 100),\n",
       " ('##f', 101),\n",
       " ('##q', 102),\n",
       " ('##v', 103),\n",
       " ('##w', 104),\n",
       " ('##j', 105),\n",
       " ('##x', 106),\n",
       " ('##1', 107),\n",
       " ('##3', 108),\n",
       " ('##9', 109),\n",
       " ('##8', 110),\n",
       " ('##5', 111),\n",
       " ('##*', 112),\n",
       " ('##~', 113),\n",
       " ('##6', 114),\n",
       " ('##_', 115),\n",
       " ('##=', 116),\n",
       " ('##.', 117),\n",
       " ('##^', 118),\n",
       " (\"##'\", 119),\n",
       " ('##\\\\', 120),\n",
       " ('##,', 121),\n",
       " ('##`', 122),\n",
       " ('##/', 123),\n",
       " ('##-', 124),\n",
       " ('##\\x19', 125),\n",
       " ('##+', 126),\n",
       " ('##|', 127),\n",
       " ('##\\x1d', 128),\n",
       " ('##:', 129),\n",
       " ('##he', 130),\n",
       " ('the', 131),\n",
       " ('##in', 132),\n",
       " ('##er', 133),\n",
       " ('##ed', 134),\n",
       " ('##ou', 135),\n",
       " ('##nd', 136),\n",
       " ('##ing', 137),\n",
       " ('to', 138),\n",
       " ('##at', 139),\n",
       " ('##re', 140),\n",
       " ('th', 141),\n",
       " ('##is', 142),\n",
       " ('and', 143),\n",
       " ('##as', 144),\n",
       " ('##en', 145),\n",
       " ('##an', 146),\n",
       " ('##on', 147),\n",
       " ('##ll', 148),\n",
       " ('he', 149),\n",
       " ('##ar', 150),\n",
       " ('##or', 151),\n",
       " ('##es', 152),\n",
       " ('of', 153),\n",
       " ('in', 154),\n",
       " ('##it', 155),\n",
       " ('``', 156),\n",
       " (\"''\", 157),\n",
       " ('ha', 158),\n",
       " ('##om', 159),\n",
       " ('you', 160),\n",
       " ('##ow', 161),\n",
       " ('be', 162),\n",
       " ('her', 163),\n",
       " ('##id', 164),\n",
       " ('##ut', 165),\n",
       " ('was', 166),\n",
       " ('it', 167),\n",
       " ('##ot', 168),\n",
       " ('##le', 169),\n",
       " ('##ld', 170),\n",
       " ('##ac', 171),\n",
       " ('##gh', 172),\n",
       " ('on', 173),\n",
       " ('she', 174),\n",
       " ('##et', 175),\n",
       " ('st', 176),\n",
       " ('his', 177),\n",
       " ('that', 178),\n",
       " ('##ly', 179),\n",
       " ('##im', 180),\n",
       " ('##ay', 181),\n",
       " ('##st', 182),\n",
       " ('##ve', 183),\n",
       " ('##ic', 184),\n",
       " ('wh', 185),\n",
       " ('as', 186),\n",
       " ('re', 187),\n",
       " ('my', 188),\n",
       " ('me', 189),\n",
       " ('##ad', 190),\n",
       " ('##al', 191),\n",
       " ('##se', 192),\n",
       " ('##oo', 193),\n",
       " ('##ver', 194),\n",
       " ('##ght', 195),\n",
       " ('##ould', 196),\n",
       " ('##ur', 197),\n",
       " ('##ith', 198),\n",
       " ('##ir', 199),\n",
       " ('for', 200),\n",
       " ('with', 201),\n",
       " ('##ent', 202),\n",
       " ('do', 203),\n",
       " ('an', 204),\n",
       " ('##ke', 205),\n",
       " ('had', 206),\n",
       " ('li', 207),\n",
       " ('at', 208),\n",
       " ('sh', 209),\n",
       " ('##am', 210),\n",
       " ('we', 211),\n",
       " ('but', 212),\n",
       " ('##ion', 213),\n",
       " ('him', 214),\n",
       " ('##all', 215),\n",
       " ('fr', 216),\n",
       " ('##her', 217),\n",
       " ('se', 218),\n",
       " ('sa', 219),\n",
       " ('not', 220),\n",
       " ('##ter', 221),\n",
       " ('##ight', 222),\n",
       " ('##ked', 223),\n",
       " ('##pp', 224),\n",
       " ('ne', 225),\n",
       " ('##ro', 226),\n",
       " ('##ack', 227),\n",
       " ('so', 228),\n",
       " ('##ain', 229),\n",
       " ('##ch', 230),\n",
       " ('##ill', 231),\n",
       " ('##ant', 232),\n",
       " ('##ce', 233),\n",
       " ('##il', 234),\n",
       " ('kn', 235),\n",
       " ('out', 236),\n",
       " ('what', 237),\n",
       " ('##ore', 238),\n",
       " ('is', 239),\n",
       " ('##el', 240),\n",
       " ('up', 241),\n",
       " ('##out', 242),\n",
       " ('##ust', 243),\n",
       " ('##rou', 244),\n",
       " ('##ri', 245),\n",
       " ('whe', 246),\n",
       " ('have', 247),\n",
       " ('##ome', 248),\n",
       " ('said', 249),\n",
       " ('they', 250),\n",
       " ('no', 251),\n",
       " ('##hing', 252),\n",
       " ('##ard', 253),\n",
       " ('this', 254),\n",
       " ('ch', 255),\n",
       " ('from', 256),\n",
       " ('go', 257),\n",
       " ('su', 258),\n",
       " ('did', 259),\n",
       " ('ab', 260),\n",
       " ('##ra', 261),\n",
       " ('##ct', 262),\n",
       " ('le', 263),\n",
       " ('##nt', 264),\n",
       " ('##king', 265),\n",
       " ('##ere', 266),\n",
       " ('all', 267),\n",
       " ('##ie', 268),\n",
       " ('de', 269),\n",
       " ('al', 270),\n",
       " ('##us', 271),\n",
       " ('##one', 272),\n",
       " ('could', 273),\n",
       " ('##ind', 274),\n",
       " ('loo', 275),\n",
       " ('back', 276),\n",
       " ('would', 277),\n",
       " ('were', 278),\n",
       " ('##to', 279),\n",
       " ('##hed', 280),\n",
       " ('like', 281),\n",
       " ('fe', 282),\n",
       " ('if', 283),\n",
       " ('##ess', 284),\n",
       " ('##art', 285),\n",
       " ('##ge', 286),\n",
       " ('##own', 287),\n",
       " ('##est', 288),\n",
       " ('one', 289),\n",
       " ('cl', 290),\n",
       " ('there', 291),\n",
       " ('##un', 292),\n",
       " ('##ood', 293),\n",
       " ('sp', 294),\n",
       " ('con', 295),\n",
       " ('##ss', 296),\n",
       " ('just', 297),\n",
       " ('ag', 298),\n",
       " ('into', 299),\n",
       " ('##ab', 300),\n",
       " ('##ast', 301),\n",
       " ('##au', 302),\n",
       " ('when', 303),\n",
       " ('wor', 304),\n",
       " ('ex', 305),\n",
       " ('know', 306),\n",
       " ('##ers', 307),\n",
       " ('##ry', 308),\n",
       " ('ro', 309),\n",
       " ('##ide', 310),\n",
       " ('about', 311),\n",
       " ('##ep', 312),\n",
       " ('##ck', 313),\n",
       " ('##ace', 314),\n",
       " ('##and', 315),\n",
       " ('bl', 316),\n",
       " ('or', 317),\n",
       " ('hand', 318),\n",
       " ('##ally', 319),\n",
       " ('##ake', 320),\n",
       " ('then', 321),\n",
       " ('##um', 322),\n",
       " ('see', 323),\n",
       " ('them', 324),\n",
       " ('your', 325),\n",
       " ('want', 326),\n",
       " ('##way', 327),\n",
       " ('over', 328),\n",
       " ('##ul', 329),\n",
       " ('te', 330),\n",
       " ('any', 331),\n",
       " ('##op', 332),\n",
       " ('are', 333),\n",
       " ('##ong', 334),\n",
       " ('##lf', 335),\n",
       " ('ey', 336),\n",
       " ('been', 337),\n",
       " ('##ist', 338),\n",
       " ('##ate', 339),\n",
       " ('sm', 340),\n",
       " ('get', 341),\n",
       " ('##ven', 342),\n",
       " ('tim', 343),\n",
       " ('##ap', 344),\n",
       " ('pl', 345),\n",
       " ('down', 346),\n",
       " ('lo', 347),\n",
       " ('co', 348),\n",
       " ('ever', 349),\n",
       " ('some', 350),\n",
       " ('mo', 351),\n",
       " ('too', 352),\n",
       " ('##ig', 353),\n",
       " ('##ame', 354),\n",
       " ('bo', 355),\n",
       " ('again', 356),\n",
       " ('##urn', 357),\n",
       " ('##ft', 358),\n",
       " ('thou', 359),\n",
       " ('com', 360),\n",
       " ('##are', 361),\n",
       " ('who', 362),\n",
       " ('##round', 363),\n",
       " ('man', 364),\n",
       " ('off', 365),\n",
       " ('##ation', 366),\n",
       " ('##ice', 367),\n",
       " ('ar', 368),\n",
       " ('br', 369),\n",
       " ('time', 370),\n",
       " ('now', 371),\n",
       " ('how', 372),\n",
       " ('##ink', 373),\n",
       " ('fl', 374),\n",
       " ('by', 375),\n",
       " ('##ff', 376),\n",
       " ('po', 377),\n",
       " ('##ous', 378),\n",
       " ('eyes', 379),\n",
       " ('##ag', 380),\n",
       " ('head', 381),\n",
       " ('us', 382),\n",
       " ('qu', 383),\n",
       " ('more', 384),\n",
       " ('##ick', 385),\n",
       " ('##ive', 386),\n",
       " ('##ort', 387),\n",
       " ('##ine', 388),\n",
       " ('##ull', 389),\n",
       " ('can', 390),\n",
       " ('sl', 391),\n",
       " ('en', 392),\n",
       " ('##ak', 393),\n",
       " ('sc', 394),\n",
       " ('tr', 395),\n",
       " ('##pped', 396),\n",
       " ('##ect', 397),\n",
       " ('##ther', 398),\n",
       " ('##other', 399),\n",
       " ('tw', 400),\n",
       " ('##ound', 401),\n",
       " ('##os', 402),\n",
       " ('than', 403),\n",
       " ('un', 404),\n",
       " ('##our', 405),\n",
       " ('##nder', 406),\n",
       " ('##itt', 407),\n",
       " ('##self', 408),\n",
       " ('even', 409),\n",
       " ('their', 410),\n",
       " ('##ved', 411),\n",
       " ('way', 412),\n",
       " ('im', 413),\n",
       " ('##fore', 414),\n",
       " ('##led', 415),\n",
       " ('##ile', 416),\n",
       " ('##dd', 417),\n",
       " ('before', 418),\n",
       " ('gr', 419),\n",
       " ('##ire', 420),\n",
       " ('every', 421),\n",
       " ('around', 422),\n",
       " ('will', 423),\n",
       " ('..', 424),\n",
       " ('##reat', 425),\n",
       " ('say', 426),\n",
       " ('going', 427),\n",
       " ('other', 428),\n",
       " ('##ass', 429),\n",
       " ('...', 430),\n",
       " ('##ure', 431),\n",
       " ('pre', 432),\n",
       " ('##ving', 433),\n",
       " ('here', 434),\n",
       " ('pro', 435),\n",
       " ('af', 436),\n",
       " ('som', 437),\n",
       " ('right', 438),\n",
       " ('##th', 439),\n",
       " ('##ade', 440),\n",
       " ('somet', 441),\n",
       " ('only', 442),\n",
       " ('think', 443),\n",
       " ('##rough', 444),\n",
       " ('bec', 445),\n",
       " ('turn', 446),\n",
       " ('through', 447),\n",
       " ('need', 448),\n",
       " ('gl', 449),\n",
       " ('##ue', 450),\n",
       " ('##ip', 451),\n",
       " ('looked', 452),\n",
       " ('##ear', 453),\n",
       " ('##ered', 454),\n",
       " ('##ose', 455),\n",
       " ('look', 456),\n",
       " ('pr', 457),\n",
       " ('##fe', 458),\n",
       " ('let', 459),\n",
       " ('thought', 460),\n",
       " ('##ite', 461),\n",
       " ('should', 462),\n",
       " ('sw', 463),\n",
       " ('thing', 464),\n",
       " ('##ward', 465),\n",
       " ('##act', 466),\n",
       " ('##iss', 467),\n",
       " ('still', 468),\n",
       " ('after', 469),\n",
       " ('away', 470),\n",
       " ('##ol', 471),\n",
       " ('gu', 472),\n",
       " ('kne', 473),\n",
       " ('something', 474),\n",
       " ('face', 475),\n",
       " ('##thing', 476),\n",
       " ('am', 477),\n",
       " ('door', 478),\n",
       " ('##pt', 479),\n",
       " ('mu', 480),\n",
       " ('##ont', 481),\n",
       " ('exp', 482),\n",
       " ('##ity', 483),\n",
       " ('good', 484),\n",
       " ('##ies', 485),\n",
       " ('##ause', 486),\n",
       " ('long', 487),\n",
       " ('##ang', 488),\n",
       " ('never', 489),\n",
       " ('##ment', 490),\n",
       " ('wat', 491),\n",
       " ('ac', 492),\n",
       " ('##ened', 493),\n",
       " ('pe', 494),\n",
       " ('wa', 495),\n",
       " ('##ach', 496),\n",
       " ('##dy', 497),\n",
       " ('##iz', 498),\n",
       " ('bet', 499),\n",
       " ('feel', 500),\n",
       " ('ll', 501),\n",
       " ('##ble', 502),\n",
       " ('got', 503),\n",
       " ('well', 504),\n",
       " ('asked', 505),\n",
       " ('where', 506),\n",
       " ('che', 507),\n",
       " ('##able', 508),\n",
       " ('car', 509),\n",
       " ('##ark', 510),\n",
       " ('two', 511),\n",
       " ('##ree', 512),\n",
       " ('op', 513),\n",
       " ('##ated', 514),\n",
       " ('fo', 515),\n",
       " ('##wn', 516),\n",
       " ('##ance', 517),\n",
       " ('##ell', 518),\n",
       " ('fir', 519),\n",
       " ('arm', 520),\n",
       " ('made', 521),\n",
       " ('##ched', 522),\n",
       " ('why', 523),\n",
       " ('##ich', 524),\n",
       " ('much', 525),\n",
       " ('ve', 526),\n",
       " ('##med', 527),\n",
       " ('##ave', 528),\n",
       " ('##ather', 529),\n",
       " ('##ious', 530),\n",
       " ('our', 531),\n",
       " ('because', 532),\n",
       " ('tell', 533),\n",
       " ('##ting', 534),\n",
       " ('rem', 535),\n",
       " ('room', 536),\n",
       " ('knew', 537),\n",
       " ('dis', 538),\n",
       " ('##ittle', 539),\n",
       " ('##age', 540),\n",
       " ('##aking', 541),\n",
       " ('##ath', 542),\n",
       " ('under', 543),\n",
       " ('ho', 544),\n",
       " ('little', 545),\n",
       " ('mom', 546),\n",
       " ('##te', 547),\n",
       " ('##ning', 548),\n",
       " ('##ady', 549),\n",
       " ('##ough', 550),\n",
       " ('day', 551),\n",
       " ('cr', 552),\n",
       " ('ke', 553),\n",
       " ('make', 554),\n",
       " ('##ery', 555),\n",
       " ('come', 556),\n",
       " ('##co', 557),\n",
       " ('take', 558),\n",
       " ('##elt', 559),\n",
       " ('##gg', 560),\n",
       " ('call', 561),\n",
       " ('happ', 562),\n",
       " ('wal', 563),\n",
       " ('em', 564),\n",
       " ('may', 565),\n",
       " ('##ried', 566),\n",
       " ('##ase', 567),\n",
       " ('##ful', 568),\n",
       " ('##ks', 569),\n",
       " ('first', 570),\n",
       " ('##ress', 571),\n",
       " ('cont', 572),\n",
       " ('##side', 573),\n",
       " ('##be', 574),\n",
       " ('##ru', 575),\n",
       " ('##air', 576),\n",
       " ('vo', 577),\n",
       " ('##ber', 578),\n",
       " ('its', 579),\n",
       " ('felt', 580),\n",
       " ('jo', 581),\n",
       " ('##ence', 582),\n",
       " ('sure', 583),\n",
       " ('wo', 584),\n",
       " ('ca', 585),\n",
       " ('##ves', 586),\n",
       " ('start', 587),\n",
       " ('hands', 588),\n",
       " ('##ied', 589),\n",
       " ('tal', 590),\n",
       " ('##ans', 591),\n",
       " ('##ared', 592),\n",
       " ('hel', 593),\n",
       " ('pull', 594),\n",
       " ('wanted', 595),\n",
       " ('took', 596),\n",
       " ('##ah', 597),\n",
       " ('comp', 598),\n",
       " ('##int', 599),\n",
       " ('turned', 600),\n",
       " ('##em', 601),\n",
       " ('night', 602),\n",
       " ('el', 603),\n",
       " ('app', 604),\n",
       " ('sk', 605),\n",
       " ('really', 606),\n",
       " ('##ling', 607),\n",
       " ('##ps', 608),\n",
       " ('per', 609),\n",
       " ('des', 610),\n",
       " ('mar', 611),\n",
       " ('##oth', 612),\n",
       " ('try', 613),\n",
       " ('##ild', 614),\n",
       " ('##ily', 615),\n",
       " ('against', 616),\n",
       " ('voice', 617),\n",
       " ('sur', 618),\n",
       " ('has', 619),\n",
       " ('ad', 620),\n",
       " ('##uck', 621),\n",
       " ('##ia', 622),\n",
       " ('part', 623),\n",
       " ('res', 624),\n",
       " ('bel', 625),\n",
       " ('##ud', 626),\n",
       " ('fin', 627),\n",
       " ('hard', 628),\n",
       " ('##qu', 629),\n",
       " ('peop', 630),\n",
       " ('real', 631),\n",
       " ('##ced', 632),\n",
       " ('left', 633),\n",
       " ('which', 634),\n",
       " ('##ens', 635),\n",
       " ('##bb', 636),\n",
       " ('very', 637),\n",
       " ('beh', 638),\n",
       " ('help', 639),\n",
       " ('cou', 640),\n",
       " ('people', 641),\n",
       " ('came', 642),\n",
       " ('told', 643),\n",
       " ('another', 644),\n",
       " ('##ds', 645),\n",
       " ('last', 646),\n",
       " ('put', 647),\n",
       " ('##owed', 648),\n",
       " ('##ise', 649),\n",
       " ('side', 650),\n",
       " ('life', 651),\n",
       " ('##een', 652),\n",
       " ('##ook', 653),\n",
       " ('##ish', 654),\n",
       " ('while', 655),\n",
       " ('##ross', 656),\n",
       " ('body', 657),\n",
       " ('wom', 658),\n",
       " ('wait', 659),\n",
       " ('##ty', 660),\n",
       " ('find', 661),\n",
       " ('few', 662),\n",
       " ('sn', 663),\n",
       " ('##av', 664),\n",
       " ('spe', 665),\n",
       " ('gra', 666),\n",
       " ('anything', 667),\n",
       " ('rep', 668),\n",
       " ('moment', 669),\n",
       " ('yes', 670),\n",
       " ('##az', 671),\n",
       " ('being', 672),\n",
       " ('##les', 673),\n",
       " ('new', 674),\n",
       " ('##hes', 675),\n",
       " ('war', 676),\n",
       " ('breat', 677),\n",
       " ('nothing', 678),\n",
       " ('##ned', 679),\n",
       " ('##get', 680),\n",
       " ('ph', 681),\n",
       " ('##xt', 682),\n",
       " ('str', 683),\n",
       " ('own', 684),\n",
       " ('di', 685),\n",
       " ('behind', 686),\n",
       " ('ste', 687),\n",
       " ('keep', 688),\n",
       " ('unt', 689),\n",
       " ('##ions', 690),\n",
       " ('year', 691),\n",
       " ('enough', 692),\n",
       " ('toward', 693),\n",
       " ('went', 694),\n",
       " ('light', 695),\n",
       " ('lau', 696),\n",
       " ('does', 697),\n",
       " ('mat', 698),\n",
       " ('might', 699),\n",
       " ('##ouse', 700),\n",
       " ('saw', 701),\n",
       " ('wr', 702),\n",
       " ('hair', 703),\n",
       " ('bed', 704),\n",
       " ('dr', 705),\n",
       " ('##ci', 706),\n",
       " ('##igh', 707),\n",
       " ('##per', 708),\n",
       " ('beg', 709),\n",
       " ('dark', 710),\n",
       " ('clos', 711),\n",
       " ('sound', 712),\n",
       " ('gir', 713),\n",
       " ('old', 714),\n",
       " ('hu', 715),\n",
       " ('until', 716),\n",
       " ('things', 717),\n",
       " ('love', 718),\n",
       " ('mind', 719),\n",
       " ('cle', 720),\n",
       " ('##outh', 721),\n",
       " ('##ways', 722),\n",
       " ('though', 723),\n",
       " ('id', 724),\n",
       " ('##ars', 725),\n",
       " ('maybe', 726),\n",
       " ('bu', 727),\n",
       " ('those', 728),\n",
       " ('set', 729),\n",
       " ('##amp', 730),\n",
       " ('##ching', 731),\n",
       " ('##ail', 732),\n",
       " ('open', 733),\n",
       " ('end', 734),\n",
       " ('##ys', 735),\n",
       " ('act', 736),\n",
       " ('##aring', 737),\n",
       " ('looking', 738),\n",
       " ('fing', 739),\n",
       " ('##ness', 740),\n",
       " ('once', 741),\n",
       " ('##ng', 742),\n",
       " ('##ord', 743),\n",
       " ('##ost', 744),\n",
       " ('##orm', 745),\n",
       " ('most', 746),\n",
       " ('##ream', 747),\n",
       " ('##ince', 748),\n",
       " ('care', 749),\n",
       " ('##ne', 750),\n",
       " ('##iend', 751),\n",
       " ('both', 752),\n",
       " ('gi', 753),\n",
       " ('girl', 754),\n",
       " ('friend', 755),\n",
       " ('##ool', 756),\n",
       " ('always', 757),\n",
       " ('ass', 758),\n",
       " ('##dded', 759),\n",
       " ('next', 760),\n",
       " ('##ons', 761),\n",
       " ('##cked', 762),\n",
       " ('##ner', 763),\n",
       " ('place', 764),\n",
       " ('sil', 765),\n",
       " ('att', 766),\n",
       " ('cur', 767),\n",
       " ('min', 768),\n",
       " ('hold', 769),\n",
       " ('##ower', 770),\n",
       " ('imp', 771),\n",
       " ('oh', 772),\n",
       " ('kill', 773),\n",
       " ('smil', 774),\n",
       " ('house', 775),\n",
       " ('mouth', 776),\n",
       " ('inside', 777),\n",
       " ('sat', 778),\n",
       " ('run', 779),\n",
       " ('found', 780),\n",
       " ('front', 781),\n",
       " ('##row', 782),\n",
       " ('ple', 783),\n",
       " ('##ted', 784),\n",
       " ('heart', 785),\n",
       " ('inst', 786),\n",
       " ('without', 787),\n",
       " ('dec', 788),\n",
       " ('ser', 789),\n",
       " ('gre', 790),\n",
       " ('##less', 791),\n",
       " ('##ating', 792),\n",
       " ('home', 793),\n",
       " ('##ian', 794),\n",
       " ('rel', 795),\n",
       " ('blood', 796),\n",
       " ('same', 797),\n",
       " ('##ory', 798),\n",
       " ('work', 799),\n",
       " ('##aw', 800),\n",
       " ('everything', 801),\n",
       " ('rest', 802),\n",
       " ('##ually', 803),\n",
       " ('someone', 804),\n",
       " ('himself', 805),\n",
       " ('sho', 806),\n",
       " ('stand', 807),\n",
       " ('inter', 808),\n",
       " ('small', 809),\n",
       " ('##ary', 810),\n",
       " ('woman', 811),\n",
       " ('hop', 812),\n",
       " ('bit', 813),\n",
       " ('bre', 814),\n",
       " ('##sw', 815),\n",
       " ('trying', 816),\n",
       " ('heard', 817),\n",
       " ('better', 818),\n",
       " ('ok', 819),\n",
       " ('arms', 820),\n",
       " ('pulled', 821),\n",
       " ('flo', 822),\n",
       " ('cor', 823),\n",
       " ('dra', 824),\n",
       " ('ret', 825),\n",
       " ('cour', 826),\n",
       " ('betw', 827),\n",
       " ('each', 828),\n",
       " ('between', 829),\n",
       " ('father', 830),\n",
       " ('stre', 831),\n",
       " ('give', 832),\n",
       " ('##ised', 833),\n",
       " ('kiss', 834),\n",
       " ('mean', 835),\n",
       " ('wind', 836),\n",
       " ('##cond', 837),\n",
       " ('mother', 838),\n",
       " ('seemed', 839),\n",
       " ('black', 840),\n",
       " ('##ier', 841),\n",
       " ('deep', 842),\n",
       " ('##ung', 843),\n",
       " ('smile', 844),\n",
       " ('##red', 845),\n",
       " ('second', 846),\n",
       " ('answ', 847),\n",
       " ('##pping', 848),\n",
       " ('##if', 849),\n",
       " ('##ange', 850),\n",
       " ('##tered', 851),\n",
       " ('far', 852),\n",
       " ('ang', 853),\n",
       " ('alm', 854),\n",
       " ('##tain', 855),\n",
       " ('across', 856),\n",
       " ('##ably', 857),\n",
       " ('##sp', 858),\n",
       " ('fam', 859),\n",
       " ('pain', 860),\n",
       " ('stop', 861),\n",
       " ('myself', 862),\n",
       " ('hur', 863),\n",
       " ('sto', 864),\n",
       " ('three', 865),\n",
       " ('##ready', 866),\n",
       " ('##sh', 867),\n",
       " ('##used', 868),\n",
       " ('since', 869),\n",
       " ('dri', 870),\n",
       " ('already', 871),\n",
       " ('pass', 872),\n",
       " ('##ever', 873),\n",
       " ('sle', 874),\n",
       " ('##aught', 875),\n",
       " ('started', 876),\n",
       " ('point', 877),\n",
       " ('##ump', 878),\n",
       " ('quick', 879),\n",
       " ('almost', 880),\n",
       " ('gave', 881),\n",
       " ('stu', 882),\n",
       " ('breath', 883),\n",
       " ('##ict', 884),\n",
       " ('dont', 885),\n",
       " ('years', 886),\n",
       " ('must', 887),\n",
       " ('belie', 888),\n",
       " ('stay', 889),\n",
       " ('##ached', 890),\n",
       " ('stood', 891),\n",
       " ('men', 892),\n",
       " ('kind', 893),\n",
       " ('##ix', 894),\n",
       " ('##ank', 895),\n",
       " ('remem', 896),\n",
       " ('ear', 897),\n",
       " ('air', 898),\n",
       " ('else', 899),\n",
       " ('done', 900),\n",
       " ('mor', 901),\n",
       " ('wall', 902),\n",
       " ('dro', 903),\n",
       " ('okay', 904),\n",
       " ('foll', 905),\n",
       " ('##ors', 906),\n",
       " ('doing', 907),\n",
       " ('hell', 908),\n",
       " ('##gether', 909),\n",
       " ('tried', 910),\n",
       " ('tra', 911),\n",
       " ('##umb', 912),\n",
       " ('world', 913),\n",
       " ('ob', 914),\n",
       " ('##ible', 915),\n",
       " ('##ings', 916),\n",
       " ('lips', 917),\n",
       " ('walked', 918),\n",
       " ('close', 919),\n",
       " ('##ane', 920),\n",
       " ('words', 921),\n",
       " ('big', 922),\n",
       " ('ev', 923),\n",
       " ('hum', 924),\n",
       " ('tou', 925),\n",
       " ('mon', 926),\n",
       " ('slow', 927),\n",
       " ('##ater', 928),\n",
       " ('minut', 929),\n",
       " ('together', 930),\n",
       " ('##rew', 931),\n",
       " ('##ized', 932),\n",
       " ('feet', 933),\n",
       " ('nodded', 934),\n",
       " ('ye', 935),\n",
       " ('sig', 936),\n",
       " ('##ott', 937),\n",
       " ('##so', 938),\n",
       " ('##ple', 939),\n",
       " ('##ense', 940),\n",
       " ('didnt', 941),\n",
       " ('##ushed', 942),\n",
       " ('wonder', 943),\n",
       " ('held', 944),\n",
       " ('near', 945),\n",
       " ('seen', 946),\n",
       " ('##ph', 947),\n",
       " ('met', 948),\n",
       " ('many', 949),\n",
       " ('miss', 950),\n",
       " ('##ured', 951),\n",
       " ('##ren', 952),\n",
       " ('these', 953),\n",
       " ('lot', 954),\n",
       " ('ra', 955),\n",
       " ('quest', 956),\n",
       " ('underst', 957),\n",
       " ('##ully', 958),\n",
       " ('##anc', 959),\n",
       " ('##ash', 960),\n",
       " ('bar', 961),\n",
       " ('diff', 962),\n",
       " ('##ently', 963),\n",
       " ('ma', 964),\n",
       " ('supp', 965),\n",
       " ('int', 966),\n",
       " ('##gs', 967),\n",
       " ('##irt', 968),\n",
       " ('##ason', 969),\n",
       " ('tre', 970),\n",
       " ('sim', 971),\n",
       " ('whis', 972),\n",
       " ('pers', 973),\n",
       " ('ent', 974),\n",
       " ('##ond', 975),\n",
       " ('##icked', 976),\n",
       " ('##inking', 977),\n",
       " ('hear', 978),\n",
       " ('##pr', 979),\n",
       " ('play', 980),\n",
       " ('yet', 981),\n",
       " ('talk', 982),\n",
       " ('**', 983),\n",
       " ('says', 984),\n",
       " ('move', 985),\n",
       " ('##ither', 986),\n",
       " ('conf', 987),\n",
       " ('bad', 988),\n",
       " ('##ows', 989),\n",
       " ('##akes', 990),\n",
       " ('floor', 991),\n",
       " ('water', 992),\n",
       " ('expl', 993),\n",
       " ('leave', 994),\n",
       " ('ed', 995),\n",
       " ('smiled', 996),\n",
       " ('mr', 997),\n",
       " ('##ib', 998),\n",
       " ('##day', 999),\n",
       " ...]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = tokenizer.get_vocab()\n",
    "vocab_sorted = sorted(vocab.items(), key=lambda item: item[1])\n",
    "vocab_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample: usually , he would be tearing around the living room , playing with his toys .\n",
      "Encoding(num_tokens=16, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n"
     ]
    }
   ],
   "source": [
    "sample = ds[0]['text']\n",
    "print(f'sample: {sample}')\n",
    "encoding = tokenizer.encode(sample)\n",
    "print(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <html>\n",
       "        <head>\n",
       "            <style>\n",
       "                .tokenized-text {\n",
       "    width:100%;\n",
       "    padding:2rem;\n",
       "    max-height: 400px;\n",
       "    overflow-y: auto;\n",
       "    box-sizing:border-box;\n",
       "    line-height:4rem; /* Lots of space between lines */\n",
       "    font-family: \"Roboto Light\", \"Ubuntu Light\", \"Ubuntu\", monospace;\n",
       "    box-shadow: 2px 2px 2px rgba(0,0,0,0.2);\n",
       "    background-color: rgba(0,0,0,0.01);\n",
       "    letter-spacing:2px; /* Give some extra separation between chars */\n",
       "}\n",
       ".non-token{\n",
       "    /* White space and other things the tokenizer ignores*/\n",
       "    white-space: pre;\n",
       "    letter-spacing:4px;\n",
       "    border-top:1px solid #A0A0A0; /* A gentle border on top and bottom makes tabs more ovious*/\n",
       "    border-bottom:1px solid #A0A0A0;\n",
       "    line-height: 1rem;\n",
       "    height: calc(100% - 2px);\n",
       "}\n",
       "\n",
       ".token {\n",
       "    white-space: pre;\n",
       "    position:relative;\n",
       "    color:black;\n",
       "    letter-spacing:2px;\n",
       "}\n",
       "\n",
       ".annotation{\n",
       "    white-space:nowrap; /* Important - ensures that annotations appears even if the annotated text wraps a line */\n",
       "    border-radius:4px;\n",
       "    position:relative;\n",
       "    width:fit-content;\n",
       "}\n",
       ".annotation:before {\n",
       "    /*The before holds the text and the after holds the background*/\n",
       "    z-index:1000; /* Make sure this is above the background */\n",
       "    content:attr(data-label); /* The annotations label is on a data attribute */\n",
       "    color:white;\n",
       "    position:absolute;\n",
       "    font-size:1rem;\n",
       "    text-align:center;\n",
       "    font-weight:bold;\n",
       "\n",
       "    top:1.75rem;\n",
       "    line-height:0;\n",
       "    left:0;\n",
       "    width:100%;\n",
       "    padding:0.5rem 0;\n",
       "    /* These make it so an annotation doesn't stretch beyond the annotated text if the label is longer*/\n",
       "    overflow: hidden;\n",
       "    white-space: nowrap;\n",
       "    text-overflow:ellipsis;\n",
       "}\n",
       "\n",
       ".annotation:after {\n",
       "    content:attr(data-label); /* The content defines the width of the annotation*/\n",
       "    position:absolute;\n",
       "    font-size:0.75rem;\n",
       "    text-align:center;\n",
       "    font-weight:bold;\n",
       "    text-overflow:ellipsis;\n",
       "    top:1.75rem;\n",
       "    line-height:0;\n",
       "    overflow: hidden;\n",
       "    white-space: nowrap;\n",
       "\n",
       "    left:0;\n",
       "    width:100%; /* 100% of the parent, which is the annotation whose width is the tokens inside it*/\n",
       "\n",
       "    padding:0.5rem 0;\n",
       "    /* Nast hack below:\n",
       "    We set the annotations color in code because we don't know the colors at css time.\n",
       "    But you can't pass a color as a data attribute to get it into the pseudo element (this thing)\n",
       "    So to get around that, annotations have the color set on them with a style attribute and then we\n",
       "    can get the color with currentColor.\n",
       "    Annotations wrap tokens and tokens set the color back to black\n",
       "     */\n",
       "    background-color: currentColor;\n",
       "}\n",
       ".annotation:hover::after, .annotation:hover::before{\n",
       "    /* When the user hovers over an annotation expand the label to display in full\n",
       "     */\n",
       "    min-width: fit-content;\n",
       "}\n",
       "\n",
       ".annotation:hover{\n",
       "    /* Emphasize the annotation start end with a border on hover*/\n",
       "    border-color: currentColor;\n",
       "    border: 2px solid;\n",
       "}\n",
       ".special-token:not(:empty){\n",
       "    /*\n",
       "    A none empty special token is like UNK (as opposed to CLS which has no representation in the text )\n",
       "     */\n",
       "    position:relative;\n",
       "}\n",
       ".special-token:empty::before{\n",
       "    /* Special tokens that don't have text are displayed as pseudo elements so we dont select them with the mouse*/\n",
       "    content:attr(data-stok);\n",
       "    background:#202020;\n",
       "    font-size:0.75rem;\n",
       "    color:white;\n",
       "    margin: 0 0.25rem;\n",
       "    padding: 0.25rem;\n",
       "    border-radius:4px\n",
       "}\n",
       "\n",
       ".special-token:not(:empty):before {\n",
       "    /* Special tokens that have text (UNK) are displayed above the actual text*/\n",
       "    content:attr(data-stok);\n",
       "    position:absolute;\n",
       "    bottom:1.75rem;\n",
       "    min-width:100%;\n",
       "    width:100%;\n",
       "    height:1rem;\n",
       "    line-height:1rem;\n",
       "    font-size:1rem;\n",
       "    text-align:center;\n",
       "    color:white;\n",
       "    font-weight:bold;\n",
       "    background:#202020;\n",
       "    border-radius:10%;\n",
       "}\n",
       "/*\n",
       "We want to alternate the color of tokens, but we can't use nth child because tokens might be broken up by annotations\n",
       "instead we apply even and odd class at generation time and color them that way\n",
       " */\n",
       ".even-token{\n",
       "    background:#DCDCDC\t;\n",
       "    border: 1px solid #DCDCDC;\n",
       "}\n",
       ".odd-token{\n",
       "    background:#A0A0A0;\n",
       "    border: 1px solid #A0A0A0;\n",
       "}\n",
       ".even-token.multi-token,.odd-token.multi-token{\n",
       "    background:  repeating-linear-gradient(\n",
       "    45deg,\n",
       "    transparent,\n",
       "    transparent 1px,\n",
       "    #ccc 1px,\n",
       "    #ccc 1px\n",
       "    ),\n",
       "    /* on \"bottom\" */\n",
       "    linear-gradient(\n",
       "    to bottom,\n",
       "    #FFB6C1,\n",
       "    #999\n",
       "    );\n",
       "}\n",
       "\n",
       ".multi-token:hover::after {\n",
       "    content:\"This char has more than 1 token\"; /* The content defines the width of the annotation*/\n",
       "    color:white;\n",
       "    background-color: black;\n",
       "    position:absolute;\n",
       "    font-size:0.75rem;\n",
       "    text-align:center;\n",
       "    font-weight:bold;\n",
       "    text-overflow:ellipsis;\n",
       "    top:1.75rem;\n",
       "    line-height:0;\n",
       "    overflow: hidden;\n",
       "    white-space: nowrap;\n",
       "    left:0;\n",
       "    width:fit-content; /* 100% of the parent, which is the annotation whose width is the tokens inside it*/\n",
       "    padding:0.5rem 0;\n",
       "}\n",
       "\n",
       "            </style>\n",
       "        </head>\n",
       "        <body>\n",
       "            <div class=\"tokenized-text\" dir=auto>\n",
       "            <span class=\"token even-token\"  >usually</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >,</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >he</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >would</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >be</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >tearing</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >around</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >the</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >living</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >room</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >,</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >playing</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >with</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >his</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >toys</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >.</span>\n",
       "            </div>\n",
       "        </body>\n",
       "    </html>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "token_ids = encoding.ids\n",
    "tokens = encoding.tokens\n",
    "type_ids = encoding.type_ids\n",
    "attention_mask = encoding.attention_mask\n",
    "\n",
    "from tokenizers.tools import EncodingVisualizer\n",
    "visualizer = EncodingVisualizer(tokenizer=tokenizer)\n",
    "visualizer(text=sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>ids</th>\n",
       "      <th>type_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>usually</td>\n",
       "      <td>2462</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>,</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>he</td>\n",
       "      <td>149</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>would</td>\n",
       "      <td>277</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>be</td>\n",
       "      <td>162</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tearing</td>\n",
       "      <td>6456</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>around</td>\n",
       "      <td>422</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>the</td>\n",
       "      <td>131</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>living</td>\n",
       "      <td>1559</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>room</td>\n",
       "      <td>536</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>,</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>playing</td>\n",
       "      <td>2301</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>with</td>\n",
       "      <td>201</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>his</td>\n",
       "      <td>177</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>toys</td>\n",
       "      <td>9774</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>.</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tokens   ids  type_ids  attention_mask\n",
       "0   usually  2462         0               1\n",
       "1         ,    19         0               1\n",
       "2        he   149         0               1\n",
       "3     would   277         0               1\n",
       "4        be   162         0               1\n",
       "5   tearing  6456         0               1\n",
       "6    around   422         0               1\n",
       "7       the   131         0               1\n",
       "8    living  1559         0               1\n",
       "9      room   536         0               1\n",
       "10        ,    19         0               1\n",
       "11  playing  2301         0               1\n",
       "12     with   201         0               1\n",
       "13      his   177         0               1\n",
       "14     toys  9774         0               1\n",
       "15        .    21         0               1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "out_dict = {'tokens':tokens,'ids':token_ids,'type_ids':type_ids,'attention_mask':attention_mask}\n",
    "df = pd.DataFrame.from_dict(out_dict)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Encoding(num_tokens=16, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
      " Encoding(num_tokens=14, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
      " Encoding(num_tokens=14, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
      " Encoding(num_tokens=42, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])]\n"
     ]
    }
   ],
   "source": [
    "samples = ds[0:4]['text']\n",
    "batch_encoding = tokenizer.encode_batch(samples)\n",
    "pprint(batch_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all default args\n",
    "tokenizer.enable_padding(direction = 'right',\n",
    "                         pad_id = 0,\n",
    "                         pad_type_id = 0,\n",
    "                         pad_token = '[PAD]',\n",
    "                         length = None, # None default to max_len in the batch\n",
    "                         pad_to_multiple_of = None) \n",
    "\n",
    "tokenizer.enable_truncation(max_length=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Encoding(num_tokens=42, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
      " Encoding(num_tokens=42, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
      " Encoding(num_tokens=42, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
      " Encoding(num_tokens=42, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])]\n"
     ]
    }
   ],
   "source": [
    "batch_encoding = tokenizer.encode_batch(samples)\n",
    "print(batch_encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['all',\n",
      " 'this',\n",
      " 'is',\n",
      " 'so',\n",
      " 'simple',\n",
      " 'to',\n",
      " 'do',\n",
      " 'in',\n",
      " 'h',\n",
      " '##f',\n",
      " '[UNK]',\n",
      " '[UNK]',\n",
      " '##.']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <html>\n",
       "        <head>\n",
       "            <style>\n",
       "                .tokenized-text {\n",
       "    width:100%;\n",
       "    padding:2rem;\n",
       "    max-height: 400px;\n",
       "    overflow-y: auto;\n",
       "    box-sizing:border-box;\n",
       "    line-height:4rem; /* Lots of space between lines */\n",
       "    font-family: \"Roboto Light\", \"Ubuntu Light\", \"Ubuntu\", monospace;\n",
       "    box-shadow: 2px 2px 2px rgba(0,0,0,0.2);\n",
       "    background-color: rgba(0,0,0,0.01);\n",
       "    letter-spacing:2px; /* Give some extra separation between chars */\n",
       "}\n",
       ".non-token{\n",
       "    /* White space and other things the tokenizer ignores*/\n",
       "    white-space: pre;\n",
       "    letter-spacing:4px;\n",
       "    border-top:1px solid #A0A0A0; /* A gentle border on top and bottom makes tabs more ovious*/\n",
       "    border-bottom:1px solid #A0A0A0;\n",
       "    line-height: 1rem;\n",
       "    height: calc(100% - 2px);\n",
       "}\n",
       "\n",
       ".token {\n",
       "    white-space: pre;\n",
       "    position:relative;\n",
       "    color:black;\n",
       "    letter-spacing:2px;\n",
       "}\n",
       "\n",
       ".annotation{\n",
       "    white-space:nowrap; /* Important - ensures that annotations appears even if the annotated text wraps a line */\n",
       "    border-radius:4px;\n",
       "    position:relative;\n",
       "    width:fit-content;\n",
       "}\n",
       ".annotation:before {\n",
       "    /*The before holds the text and the after holds the background*/\n",
       "    z-index:1000; /* Make sure this is above the background */\n",
       "    content:attr(data-label); /* The annotations label is on a data attribute */\n",
       "    color:white;\n",
       "    position:absolute;\n",
       "    font-size:1rem;\n",
       "    text-align:center;\n",
       "    font-weight:bold;\n",
       "\n",
       "    top:1.75rem;\n",
       "    line-height:0;\n",
       "    left:0;\n",
       "    width:100%;\n",
       "    padding:0.5rem 0;\n",
       "    /* These make it so an annotation doesn't stretch beyond the annotated text if the label is longer*/\n",
       "    overflow: hidden;\n",
       "    white-space: nowrap;\n",
       "    text-overflow:ellipsis;\n",
       "}\n",
       "\n",
       ".annotation:after {\n",
       "    content:attr(data-label); /* The content defines the width of the annotation*/\n",
       "    position:absolute;\n",
       "    font-size:0.75rem;\n",
       "    text-align:center;\n",
       "    font-weight:bold;\n",
       "    text-overflow:ellipsis;\n",
       "    top:1.75rem;\n",
       "    line-height:0;\n",
       "    overflow: hidden;\n",
       "    white-space: nowrap;\n",
       "\n",
       "    left:0;\n",
       "    width:100%; /* 100% of the parent, which is the annotation whose width is the tokens inside it*/\n",
       "\n",
       "    padding:0.5rem 0;\n",
       "    /* Nast hack below:\n",
       "    We set the annotations color in code because we don't know the colors at css time.\n",
       "    But you can't pass a color as a data attribute to get it into the pseudo element (this thing)\n",
       "    So to get around that, annotations have the color set on them with a style attribute and then we\n",
       "    can get the color with currentColor.\n",
       "    Annotations wrap tokens and tokens set the color back to black\n",
       "     */\n",
       "    background-color: currentColor;\n",
       "}\n",
       ".annotation:hover::after, .annotation:hover::before{\n",
       "    /* When the user hovers over an annotation expand the label to display in full\n",
       "     */\n",
       "    min-width: fit-content;\n",
       "}\n",
       "\n",
       ".annotation:hover{\n",
       "    /* Emphasize the annotation start end with a border on hover*/\n",
       "    border-color: currentColor;\n",
       "    border: 2px solid;\n",
       "}\n",
       ".special-token:not(:empty){\n",
       "    /*\n",
       "    A none empty special token is like UNK (as opposed to CLS which has no representation in the text )\n",
       "     */\n",
       "    position:relative;\n",
       "}\n",
       ".special-token:empty::before{\n",
       "    /* Special tokens that don't have text are displayed as pseudo elements so we dont select them with the mouse*/\n",
       "    content:attr(data-stok);\n",
       "    background:#202020;\n",
       "    font-size:0.75rem;\n",
       "    color:white;\n",
       "    margin: 0 0.25rem;\n",
       "    padding: 0.25rem;\n",
       "    border-radius:4px\n",
       "}\n",
       "\n",
       ".special-token:not(:empty):before {\n",
       "    /* Special tokens that have text (UNK) are displayed above the actual text*/\n",
       "    content:attr(data-stok);\n",
       "    position:absolute;\n",
       "    bottom:1.75rem;\n",
       "    min-width:100%;\n",
       "    width:100%;\n",
       "    height:1rem;\n",
       "    line-height:1rem;\n",
       "    font-size:1rem;\n",
       "    text-align:center;\n",
       "    color:white;\n",
       "    font-weight:bold;\n",
       "    background:#202020;\n",
       "    border-radius:10%;\n",
       "}\n",
       "/*\n",
       "We want to alternate the color of tokens, but we can't use nth child because tokens might be broken up by annotations\n",
       "instead we apply even and odd class at generation time and color them that way\n",
       " */\n",
       ".even-token{\n",
       "    background:#DCDCDC\t;\n",
       "    border: 1px solid #DCDCDC;\n",
       "}\n",
       ".odd-token{\n",
       "    background:#A0A0A0;\n",
       "    border: 1px solid #A0A0A0;\n",
       "}\n",
       ".even-token.multi-token,.odd-token.multi-token{\n",
       "    background:  repeating-linear-gradient(\n",
       "    45deg,\n",
       "    transparent,\n",
       "    transparent 1px,\n",
       "    #ccc 1px,\n",
       "    #ccc 1px\n",
       "    ),\n",
       "    /* on \"bottom\" */\n",
       "    linear-gradient(\n",
       "    to bottom,\n",
       "    #FFB6C1,\n",
       "    #999\n",
       "    );\n",
       "}\n",
       "\n",
       ".multi-token:hover::after {\n",
       "    content:\"This char has more than 1 token\"; /* The content defines the width of the annotation*/\n",
       "    color:white;\n",
       "    background-color: black;\n",
       "    position:absolute;\n",
       "    font-size:0.75rem;\n",
       "    text-align:center;\n",
       "    font-weight:bold;\n",
       "    text-overflow:ellipsis;\n",
       "    top:1.75rem;\n",
       "    line-height:0;\n",
       "    overflow: hidden;\n",
       "    white-space: nowrap;\n",
       "    left:0;\n",
       "    width:fit-content; /* 100% of the parent, which is the annotation whose width is the tokens inside it*/\n",
       "    padding:0.5rem 0;\n",
       "}\n",
       "\n",
       "            </style>\n",
       "        </head>\n",
       "        <body>\n",
       "            <div class=\"tokenized-text\" dir=auto>\n",
       "            <span class=\"token even-token\"  >All</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >this</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >is</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >so</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >simple</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >to</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >do</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >in</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >H</span><span class=\"token odd-token\"  >F</span><span class=\"non-token\"  > </span><span class=\"token even-token special-token\"  data-stok=\"[UNK]\" >இ</span><span class=\"token odd-token special-token\"  data-stok=\"[UNK]\" >😊</span><span class=\"token even-token\"  >.</span>\n",
       "            </div>\n",
       "        </body>\n",
       "    </html>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = \"All this is so simple to do in HF இ😊.\"\n",
    "encoded = tokenizer.encode(text).tokens\n",
    "print(encoded)\n",
    "visualizer(text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"ERROR: argument 'pretty': 'str' object cannot be converted to 'PyBool'\"\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tokenizer.save(save_dir, 'hopper.json')\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the BookCorpus dataset. Take every 7-th sample (the indices are multiple of 7:[0,7,14,21,...]) from the entire dataset. This will result in a dataset with 10 million samples (exactly, 10,572,033). Use these samples to build a tokenizer with the BPE tokenization algorithm by varying the vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 74004228\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "try:\n",
    "    all_ds = load_dataset('bookcorpus',split='all')\n",
    "    print(all_ds)\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 10572033\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = all_ds.select(range(0, all_ds.num_rows, 7))\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Normalizer: LowerCase\n",
    "* PreTokenizer: WhiteSpace\n",
    "* Model: BPE\n",
    "* Special tokens: [GO],[UNK],[PAD],[EOS]\n",
    "* PostProcessing: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.normalizers import Lowercase\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.models import  BPE\n",
    "from tokenizers.trainers import BpeTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the input text: “SEBI study finds 93% of individual F&O traders made losses between FY22 and FY24.” using the following configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 ['SEBI', 'study', 'finds', '93%', 'of', 'individual', 'F&O', 'traders', 'made', 'losses', 'between', 'FY22', 'and', 'FY24.']\n"
     ]
    }
   ],
   "source": [
    "text = \"SEBI study finds 93% of individual F&O traders made losses between FY22 and FY24.\"\n",
    "tokens = text.split()\n",
    "print(len(tokens),tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Keep the vocabulary size at 5000 and tokenize the input text using the learned vocabulary. Choose the number of tokens returned by the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BPE(unk_token= \"[UNK]\")\n",
    "tokenizer = Tokenizer(model)\n",
    "tokenizer.normalizer = Lowercase()\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "trainer = BpeTrainer(vocab_size = 5000,\n",
    "    special_tokens = [\"[GO]\",\"[UNK]\",\"[PAD]\",\"[EOS]\"],\n",
    "    continuing_subword_prefix='##')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def samples(batch_size = 1000):\n",
    "    for i in range(0, len(ds), batch_size):\n",
    "        yield ds[i : i+ batch_size]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizer Training: 10573it [01:57, 89.79it/s]                           \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "bsize = 1000\n",
    "samples_iterator = tqdm(samples(bsize), total = len(ds) // bsize, desc=\"Tokenizer Training\")\n",
    "tokenizer.train_from_iterator(samples_iterator, trainer= trainer, length=len(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 ['seb', '##i', 'study', 'find', '##s', '9', '##3', '%', 'of', 'ind', '##ivid', '##ual', 'f', '&', 'o', 'tr', '##ad', '##ers', 'made', 'loss', '##es', 'between', 'f', '##y', '##2', '##2', 'and', 'f', '##y', '##2', '##4', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.encode(text).tokens\n",
    "print(len(tokens), tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2:** Increase the vocabulary size to 10K, 15K and 32K. For each case, tokenize the same input with the newly learned vocabulary. Choose all the correct statements\n",
    "\n",
    "Do change the `vocab_size` and retrain the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches Trained for Vocab size 10000: 10573it [01:55, 91.61it/s]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token size = 28\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <html>\n",
       "        <head>\n",
       "            <style>\n",
       "                .tokenized-text {\n",
       "    width:100%;\n",
       "    padding:2rem;\n",
       "    max-height: 400px;\n",
       "    overflow-y: auto;\n",
       "    box-sizing:border-box;\n",
       "    line-height:4rem; /* Lots of space between lines */\n",
       "    font-family: \"Roboto Light\", \"Ubuntu Light\", \"Ubuntu\", monospace;\n",
       "    box-shadow: 2px 2px 2px rgba(0,0,0,0.2);\n",
       "    background-color: rgba(0,0,0,0.01);\n",
       "    letter-spacing:2px; /* Give some extra separation between chars */\n",
       "}\n",
       ".non-token{\n",
       "    /* White space and other things the tokenizer ignores*/\n",
       "    white-space: pre;\n",
       "    letter-spacing:4px;\n",
       "    border-top:1px solid #A0A0A0; /* A gentle border on top and bottom makes tabs more ovious*/\n",
       "    border-bottom:1px solid #A0A0A0;\n",
       "    line-height: 1rem;\n",
       "    height: calc(100% - 2px);\n",
       "}\n",
       "\n",
       ".token {\n",
       "    white-space: pre;\n",
       "    position:relative;\n",
       "    color:black;\n",
       "    letter-spacing:2px;\n",
       "}\n",
       "\n",
       ".annotation{\n",
       "    white-space:nowrap; /* Important - ensures that annotations appears even if the annotated text wraps a line */\n",
       "    border-radius:4px;\n",
       "    position:relative;\n",
       "    width:fit-content;\n",
       "}\n",
       ".annotation:before {\n",
       "    /*The before holds the text and the after holds the background*/\n",
       "    z-index:1000; /* Make sure this is above the background */\n",
       "    content:attr(data-label); /* The annotations label is on a data attribute */\n",
       "    color:white;\n",
       "    position:absolute;\n",
       "    font-size:1rem;\n",
       "    text-align:center;\n",
       "    font-weight:bold;\n",
       "\n",
       "    top:1.75rem;\n",
       "    line-height:0;\n",
       "    left:0;\n",
       "    width:100%;\n",
       "    padding:0.5rem 0;\n",
       "    /* These make it so an annotation doesn't stretch beyond the annotated text if the label is longer*/\n",
       "    overflow: hidden;\n",
       "    white-space: nowrap;\n",
       "    text-overflow:ellipsis;\n",
       "}\n",
       "\n",
       ".annotation:after {\n",
       "    content:attr(data-label); /* The content defines the width of the annotation*/\n",
       "    position:absolute;\n",
       "    font-size:0.75rem;\n",
       "    text-align:center;\n",
       "    font-weight:bold;\n",
       "    text-overflow:ellipsis;\n",
       "    top:1.75rem;\n",
       "    line-height:0;\n",
       "    overflow: hidden;\n",
       "    white-space: nowrap;\n",
       "\n",
       "    left:0;\n",
       "    width:100%; /* 100% of the parent, which is the annotation whose width is the tokens inside it*/\n",
       "\n",
       "    padding:0.5rem 0;\n",
       "    /* Nast hack below:\n",
       "    We set the annotations color in code because we don't know the colors at css time.\n",
       "    But you can't pass a color as a data attribute to get it into the pseudo element (this thing)\n",
       "    So to get around that, annotations have the color set on them with a style attribute and then we\n",
       "    can get the color with currentColor.\n",
       "    Annotations wrap tokens and tokens set the color back to black\n",
       "     */\n",
       "    background-color: currentColor;\n",
       "}\n",
       ".annotation:hover::after, .annotation:hover::before{\n",
       "    /* When the user hovers over an annotation expand the label to display in full\n",
       "     */\n",
       "    min-width: fit-content;\n",
       "}\n",
       "\n",
       ".annotation:hover{\n",
       "    /* Emphasize the annotation start end with a border on hover*/\n",
       "    border-color: currentColor;\n",
       "    border: 2px solid;\n",
       "}\n",
       ".special-token:not(:empty){\n",
       "    /*\n",
       "    A none empty special token is like UNK (as opposed to CLS which has no representation in the text )\n",
       "     */\n",
       "    position:relative;\n",
       "}\n",
       ".special-token:empty::before{\n",
       "    /* Special tokens that don't have text are displayed as pseudo elements so we dont select them with the mouse*/\n",
       "    content:attr(data-stok);\n",
       "    background:#202020;\n",
       "    font-size:0.75rem;\n",
       "    color:white;\n",
       "    margin: 0 0.25rem;\n",
       "    padding: 0.25rem;\n",
       "    border-radius:4px\n",
       "}\n",
       "\n",
       ".special-token:not(:empty):before {\n",
       "    /* Special tokens that have text (UNK) are displayed above the actual text*/\n",
       "    content:attr(data-stok);\n",
       "    position:absolute;\n",
       "    bottom:1.75rem;\n",
       "    min-width:100%;\n",
       "    width:100%;\n",
       "    height:1rem;\n",
       "    line-height:1rem;\n",
       "    font-size:1rem;\n",
       "    text-align:center;\n",
       "    color:white;\n",
       "    font-weight:bold;\n",
       "    background:#202020;\n",
       "    border-radius:10%;\n",
       "}\n",
       "/*\n",
       "We want to alternate the color of tokens, but we can't use nth child because tokens might be broken up by annotations\n",
       "instead we apply even and odd class at generation time and color them that way\n",
       " */\n",
       ".even-token{\n",
       "    background:#DCDCDC\t;\n",
       "    border: 1px solid #DCDCDC;\n",
       "}\n",
       ".odd-token{\n",
       "    background:#A0A0A0;\n",
       "    border: 1px solid #A0A0A0;\n",
       "}\n",
       ".even-token.multi-token,.odd-token.multi-token{\n",
       "    background:  repeating-linear-gradient(\n",
       "    45deg,\n",
       "    transparent,\n",
       "    transparent 1px,\n",
       "    #ccc 1px,\n",
       "    #ccc 1px\n",
       "    ),\n",
       "    /* on \"bottom\" */\n",
       "    linear-gradient(\n",
       "    to bottom,\n",
       "    #FFB6C1,\n",
       "    #999\n",
       "    );\n",
       "}\n",
       "\n",
       ".multi-token:hover::after {\n",
       "    content:\"This char has more than 1 token\"; /* The content defines the width of the annotation*/\n",
       "    color:white;\n",
       "    background-color: black;\n",
       "    position:absolute;\n",
       "    font-size:0.75rem;\n",
       "    text-align:center;\n",
       "    font-weight:bold;\n",
       "    text-overflow:ellipsis;\n",
       "    top:1.75rem;\n",
       "    line-height:0;\n",
       "    overflow: hidden;\n",
       "    white-space: nowrap;\n",
       "    left:0;\n",
       "    width:fit-content; /* 100% of the parent, which is the annotation whose width is the tokens inside it*/\n",
       "    padding:0.5rem 0;\n",
       "}\n",
       "\n",
       "            </style>\n",
       "        </head>\n",
       "        <body>\n",
       "            <div class=\"tokenized-text\" dir=auto>\n",
       "            <span class=\"token even-token\"  >SEB</span><span class=\"token odd-token\"  >I</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >study</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >find</span><span class=\"token even-token\"  >s</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >9</span><span class=\"token even-token\"  >3</span><span class=\"token odd-token\"  >%</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >of</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >ind</span><span class=\"token even-token\"  >ivid</span><span class=\"token odd-token\"  >ual</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >F</span><span class=\"token odd-token\"  >&</span><span class=\"token even-token\"  >O</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >tr</span><span class=\"token even-token\"  >ad</span><span class=\"token odd-token\"  >ers</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >made</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >loss</span><span class=\"token even-token\"  >es</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >between</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >F</span><span class=\"token odd-token\"  >Y</span><span class=\"token even-token\"  >2</span><span class=\"token odd-token\"  >2</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >and</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >F</span><span class=\"token even-token\"  >Y</span><span class=\"token odd-token\"  >2</span><span class=\"token even-token\"  >4</span><span class=\"token odd-token\"  >.</span>\n",
       "            </div>\n",
       "        </body>\n",
       "    </html>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches Trained for Vocab size 15000: 10573it [03:27, 50.90it/s]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token size = 28\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <html>\n",
       "        <head>\n",
       "            <style>\n",
       "                .tokenized-text {\n",
       "    width:100%;\n",
       "    padding:2rem;\n",
       "    max-height: 400px;\n",
       "    overflow-y: auto;\n",
       "    box-sizing:border-box;\n",
       "    line-height:4rem; /* Lots of space between lines */\n",
       "    font-family: \"Roboto Light\", \"Ubuntu Light\", \"Ubuntu\", monospace;\n",
       "    box-shadow: 2px 2px 2px rgba(0,0,0,0.2);\n",
       "    background-color: rgba(0,0,0,0.01);\n",
       "    letter-spacing:2px; /* Give some extra separation between chars */\n",
       "}\n",
       ".non-token{\n",
       "    /* White space and other things the tokenizer ignores*/\n",
       "    white-space: pre;\n",
       "    letter-spacing:4px;\n",
       "    border-top:1px solid #A0A0A0; /* A gentle border on top and bottom makes tabs more ovious*/\n",
       "    border-bottom:1px solid #A0A0A0;\n",
       "    line-height: 1rem;\n",
       "    height: calc(100% - 2px);\n",
       "}\n",
       "\n",
       ".token {\n",
       "    white-space: pre;\n",
       "    position:relative;\n",
       "    color:black;\n",
       "    letter-spacing:2px;\n",
       "}\n",
       "\n",
       ".annotation{\n",
       "    white-space:nowrap; /* Important - ensures that annotations appears even if the annotated text wraps a line */\n",
       "    border-radius:4px;\n",
       "    position:relative;\n",
       "    width:fit-content;\n",
       "}\n",
       ".annotation:before {\n",
       "    /*The before holds the text and the after holds the background*/\n",
       "    z-index:1000; /* Make sure this is above the background */\n",
       "    content:attr(data-label); /* The annotations label is on a data attribute */\n",
       "    color:white;\n",
       "    position:absolute;\n",
       "    font-size:1rem;\n",
       "    text-align:center;\n",
       "    font-weight:bold;\n",
       "\n",
       "    top:1.75rem;\n",
       "    line-height:0;\n",
       "    left:0;\n",
       "    width:100%;\n",
       "    padding:0.5rem 0;\n",
       "    /* These make it so an annotation doesn't stretch beyond the annotated text if the label is longer*/\n",
       "    overflow: hidden;\n",
       "    white-space: nowrap;\n",
       "    text-overflow:ellipsis;\n",
       "}\n",
       "\n",
       ".annotation:after {\n",
       "    content:attr(data-label); /* The content defines the width of the annotation*/\n",
       "    position:absolute;\n",
       "    font-size:0.75rem;\n",
       "    text-align:center;\n",
       "    font-weight:bold;\n",
       "    text-overflow:ellipsis;\n",
       "    top:1.75rem;\n",
       "    line-height:0;\n",
       "    overflow: hidden;\n",
       "    white-space: nowrap;\n",
       "\n",
       "    left:0;\n",
       "    width:100%; /* 100% of the parent, which is the annotation whose width is the tokens inside it*/\n",
       "\n",
       "    padding:0.5rem 0;\n",
       "    /* Nast hack below:\n",
       "    We set the annotations color in code because we don't know the colors at css time.\n",
       "    But you can't pass a color as a data attribute to get it into the pseudo element (this thing)\n",
       "    So to get around that, annotations have the color set on them with a style attribute and then we\n",
       "    can get the color with currentColor.\n",
       "    Annotations wrap tokens and tokens set the color back to black\n",
       "     */\n",
       "    background-color: currentColor;\n",
       "}\n",
       ".annotation:hover::after, .annotation:hover::before{\n",
       "    /* When the user hovers over an annotation expand the label to display in full\n",
       "     */\n",
       "    min-width: fit-content;\n",
       "}\n",
       "\n",
       ".annotation:hover{\n",
       "    /* Emphasize the annotation start end with a border on hover*/\n",
       "    border-color: currentColor;\n",
       "    border: 2px solid;\n",
       "}\n",
       ".special-token:not(:empty){\n",
       "    /*\n",
       "    A none empty special token is like UNK (as opposed to CLS which has no representation in the text )\n",
       "     */\n",
       "    position:relative;\n",
       "}\n",
       ".special-token:empty::before{\n",
       "    /* Special tokens that don't have text are displayed as pseudo elements so we dont select them with the mouse*/\n",
       "    content:attr(data-stok);\n",
       "    background:#202020;\n",
       "    font-size:0.75rem;\n",
       "    color:white;\n",
       "    margin: 0 0.25rem;\n",
       "    padding: 0.25rem;\n",
       "    border-radius:4px\n",
       "}\n",
       "\n",
       ".special-token:not(:empty):before {\n",
       "    /* Special tokens that have text (UNK) are displayed above the actual text*/\n",
       "    content:attr(data-stok);\n",
       "    position:absolute;\n",
       "    bottom:1.75rem;\n",
       "    min-width:100%;\n",
       "    width:100%;\n",
       "    height:1rem;\n",
       "    line-height:1rem;\n",
       "    font-size:1rem;\n",
       "    text-align:center;\n",
       "    color:white;\n",
       "    font-weight:bold;\n",
       "    background:#202020;\n",
       "    border-radius:10%;\n",
       "}\n",
       "/*\n",
       "We want to alternate the color of tokens, but we can't use nth child because tokens might be broken up by annotations\n",
       "instead we apply even and odd class at generation time and color them that way\n",
       " */\n",
       ".even-token{\n",
       "    background:#DCDCDC\t;\n",
       "    border: 1px solid #DCDCDC;\n",
       "}\n",
       ".odd-token{\n",
       "    background:#A0A0A0;\n",
       "    border: 1px solid #A0A0A0;\n",
       "}\n",
       ".even-token.multi-token,.odd-token.multi-token{\n",
       "    background:  repeating-linear-gradient(\n",
       "    45deg,\n",
       "    transparent,\n",
       "    transparent 1px,\n",
       "    #ccc 1px,\n",
       "    #ccc 1px\n",
       "    ),\n",
       "    /* on \"bottom\" */\n",
       "    linear-gradient(\n",
       "    to bottom,\n",
       "    #FFB6C1,\n",
       "    #999\n",
       "    );\n",
       "}\n",
       "\n",
       ".multi-token:hover::after {\n",
       "    content:\"This char has more than 1 token\"; /* The content defines the width of the annotation*/\n",
       "    color:white;\n",
       "    background-color: black;\n",
       "    position:absolute;\n",
       "    font-size:0.75rem;\n",
       "    text-align:center;\n",
       "    font-weight:bold;\n",
       "    text-overflow:ellipsis;\n",
       "    top:1.75rem;\n",
       "    line-height:0;\n",
       "    overflow: hidden;\n",
       "    white-space: nowrap;\n",
       "    left:0;\n",
       "    width:fit-content; /* 100% of the parent, which is the annotation whose width is the tokens inside it*/\n",
       "    padding:0.5rem 0;\n",
       "}\n",
       "\n",
       "            </style>\n",
       "        </head>\n",
       "        <body>\n",
       "            <div class=\"tokenized-text\" dir=auto>\n",
       "            <span class=\"token even-token\"  >SEB</span><span class=\"token odd-token\"  >I</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >study</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >find</span><span class=\"token even-token\"  >s</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >9</span><span class=\"token even-token\"  >3</span><span class=\"token odd-token\"  >%</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >of</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >ind</span><span class=\"token even-token\"  >ivid</span><span class=\"token odd-token\"  >ual</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >F</span><span class=\"token odd-token\"  >&</span><span class=\"token even-token\"  >O</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >tr</span><span class=\"token even-token\"  >ad</span><span class=\"token odd-token\"  >ers</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >made</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >loss</span><span class=\"token even-token\"  >es</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >between</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >F</span><span class=\"token odd-token\"  >Y</span><span class=\"token even-token\"  >2</span><span class=\"token odd-token\"  >2</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >and</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >F</span><span class=\"token even-token\"  >Y</span><span class=\"token odd-token\"  >2</span><span class=\"token even-token\"  >4</span><span class=\"token odd-token\"  >.</span>\n",
       "            </div>\n",
       "        </body>\n",
       "    </html>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches Trained for Vocab size 32000: 10573it [04:17, 41.12it/s]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token size = 25\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <html>\n",
       "        <head>\n",
       "            <style>\n",
       "                .tokenized-text {\n",
       "    width:100%;\n",
       "    padding:2rem;\n",
       "    max-height: 400px;\n",
       "    overflow-y: auto;\n",
       "    box-sizing:border-box;\n",
       "    line-height:4rem; /* Lots of space between lines */\n",
       "    font-family: \"Roboto Light\", \"Ubuntu Light\", \"Ubuntu\", monospace;\n",
       "    box-shadow: 2px 2px 2px rgba(0,0,0,0.2);\n",
       "    background-color: rgba(0,0,0,0.01);\n",
       "    letter-spacing:2px; /* Give some extra separation between chars */\n",
       "}\n",
       ".non-token{\n",
       "    /* White space and other things the tokenizer ignores*/\n",
       "    white-space: pre;\n",
       "    letter-spacing:4px;\n",
       "    border-top:1px solid #A0A0A0; /* A gentle border on top and bottom makes tabs more ovious*/\n",
       "    border-bottom:1px solid #A0A0A0;\n",
       "    line-height: 1rem;\n",
       "    height: calc(100% - 2px);\n",
       "}\n",
       "\n",
       ".token {\n",
       "    white-space: pre;\n",
       "    position:relative;\n",
       "    color:black;\n",
       "    letter-spacing:2px;\n",
       "}\n",
       "\n",
       ".annotation{\n",
       "    white-space:nowrap; /* Important - ensures that annotations appears even if the annotated text wraps a line */\n",
       "    border-radius:4px;\n",
       "    position:relative;\n",
       "    width:fit-content;\n",
       "}\n",
       ".annotation:before {\n",
       "    /*The before holds the text and the after holds the background*/\n",
       "    z-index:1000; /* Make sure this is above the background */\n",
       "    content:attr(data-label); /* The annotations label is on a data attribute */\n",
       "    color:white;\n",
       "    position:absolute;\n",
       "    font-size:1rem;\n",
       "    text-align:center;\n",
       "    font-weight:bold;\n",
       "\n",
       "    top:1.75rem;\n",
       "    line-height:0;\n",
       "    left:0;\n",
       "    width:100%;\n",
       "    padding:0.5rem 0;\n",
       "    /* These make it so an annotation doesn't stretch beyond the annotated text if the label is longer*/\n",
       "    overflow: hidden;\n",
       "    white-space: nowrap;\n",
       "    text-overflow:ellipsis;\n",
       "}\n",
       "\n",
       ".annotation:after {\n",
       "    content:attr(data-label); /* The content defines the width of the annotation*/\n",
       "    position:absolute;\n",
       "    font-size:0.75rem;\n",
       "    text-align:center;\n",
       "    font-weight:bold;\n",
       "    text-overflow:ellipsis;\n",
       "    top:1.75rem;\n",
       "    line-height:0;\n",
       "    overflow: hidden;\n",
       "    white-space: nowrap;\n",
       "\n",
       "    left:0;\n",
       "    width:100%; /* 100% of the parent, which is the annotation whose width is the tokens inside it*/\n",
       "\n",
       "    padding:0.5rem 0;\n",
       "    /* Nast hack below:\n",
       "    We set the annotations color in code because we don't know the colors at css time.\n",
       "    But you can't pass a color as a data attribute to get it into the pseudo element (this thing)\n",
       "    So to get around that, annotations have the color set on them with a style attribute and then we\n",
       "    can get the color with currentColor.\n",
       "    Annotations wrap tokens and tokens set the color back to black\n",
       "     */\n",
       "    background-color: currentColor;\n",
       "}\n",
       ".annotation:hover::after, .annotation:hover::before{\n",
       "    /* When the user hovers over an annotation expand the label to display in full\n",
       "     */\n",
       "    min-width: fit-content;\n",
       "}\n",
       "\n",
       ".annotation:hover{\n",
       "    /* Emphasize the annotation start end with a border on hover*/\n",
       "    border-color: currentColor;\n",
       "    border: 2px solid;\n",
       "}\n",
       ".special-token:not(:empty){\n",
       "    /*\n",
       "    A none empty special token is like UNK (as opposed to CLS which has no representation in the text )\n",
       "     */\n",
       "    position:relative;\n",
       "}\n",
       ".special-token:empty::before{\n",
       "    /* Special tokens that don't have text are displayed as pseudo elements so we dont select them with the mouse*/\n",
       "    content:attr(data-stok);\n",
       "    background:#202020;\n",
       "    font-size:0.75rem;\n",
       "    color:white;\n",
       "    margin: 0 0.25rem;\n",
       "    padding: 0.25rem;\n",
       "    border-radius:4px\n",
       "}\n",
       "\n",
       ".special-token:not(:empty):before {\n",
       "    /* Special tokens that have text (UNK) are displayed above the actual text*/\n",
       "    content:attr(data-stok);\n",
       "    position:absolute;\n",
       "    bottom:1.75rem;\n",
       "    min-width:100%;\n",
       "    width:100%;\n",
       "    height:1rem;\n",
       "    line-height:1rem;\n",
       "    font-size:1rem;\n",
       "    text-align:center;\n",
       "    color:white;\n",
       "    font-weight:bold;\n",
       "    background:#202020;\n",
       "    border-radius:10%;\n",
       "}\n",
       "/*\n",
       "We want to alternate the color of tokens, but we can't use nth child because tokens might be broken up by annotations\n",
       "instead we apply even and odd class at generation time and color them that way\n",
       " */\n",
       ".even-token{\n",
       "    background:#DCDCDC\t;\n",
       "    border: 1px solid #DCDCDC;\n",
       "}\n",
       ".odd-token{\n",
       "    background:#A0A0A0;\n",
       "    border: 1px solid #A0A0A0;\n",
       "}\n",
       ".even-token.multi-token,.odd-token.multi-token{\n",
       "    background:  repeating-linear-gradient(\n",
       "    45deg,\n",
       "    transparent,\n",
       "    transparent 1px,\n",
       "    #ccc 1px,\n",
       "    #ccc 1px\n",
       "    ),\n",
       "    /* on \"bottom\" */\n",
       "    linear-gradient(\n",
       "    to bottom,\n",
       "    #FFB6C1,\n",
       "    #999\n",
       "    );\n",
       "}\n",
       "\n",
       ".multi-token:hover::after {\n",
       "    content:\"This char has more than 1 token\"; /* The content defines the width of the annotation*/\n",
       "    color:white;\n",
       "    background-color: black;\n",
       "    position:absolute;\n",
       "    font-size:0.75rem;\n",
       "    text-align:center;\n",
       "    font-weight:bold;\n",
       "    text-overflow:ellipsis;\n",
       "    top:1.75rem;\n",
       "    line-height:0;\n",
       "    overflow: hidden;\n",
       "    white-space: nowrap;\n",
       "    left:0;\n",
       "    width:fit-content; /* 100% of the parent, which is the annotation whose width is the tokens inside it*/\n",
       "    padding:0.5rem 0;\n",
       "}\n",
       "\n",
       "            </style>\n",
       "        </head>\n",
       "        <body>\n",
       "            <div class=\"tokenized-text\" dir=auto>\n",
       "            <span class=\"token even-token\"  >SEB</span><span class=\"token odd-token\"  >I</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >study</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >find</span><span class=\"token even-token\"  >s</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >9</span><span class=\"token even-token\"  >3</span><span class=\"token odd-token\"  >%</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >of</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >ind</span><span class=\"token even-token\"  >ivid</span><span class=\"token odd-token\"  >ual</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >F</span><span class=\"token odd-token\"  >&</span><span class=\"token even-token\"  >O</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >tr</span><span class=\"token even-token\"  >ad</span><span class=\"token odd-token\"  >ers</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >made</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >loss</span><span class=\"token even-token\"  >es</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >between</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >F</span><span class=\"token odd-token\"  >Y</span><span class=\"token even-token\"  >2</span><span class=\"token odd-token\"  >2</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >and</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >F</span><span class=\"token even-token\"  >Y</span><span class=\"token odd-token\"  >2</span><span class=\"token even-token\"  >4</span><span class=\"token odd-token\"  >.</span>\n",
       "            </div>\n",
       "        </body>\n",
       "    </html>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vsizes = [10_000, 15_000, 32_000]\n",
    "for i in vsizes:\n",
    "    model = BPE(unk_token= \"[UNK]\")\n",
    "    tokenizer = Tokenizer(model)\n",
    "    tokenizer.normalizer = Lowercase()\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "    trainer = BpeTrainer(vocab_size = i,\n",
    "    special_tokens = [\"[GO]\",\"[UNK]\",\"[PAD]\",\"[EOS]\"],\n",
    "    continuing_subword_prefix='##')\n",
    "    \n",
    "    samples_iterator = tqdm(samples(bsize), total = len(ds) // bsize, desc=f\"Batches Trained for Vocab size {i}\")\n",
    "    tokenizer.train_from_iterator(samples_iterator, trainer= trainer, length=len(ds))\n",
    "    print(\"token size =\", len(tokenizer.encode(text).tokens))\n",
    "    visualizer = EncodingVisualizer(tokenizer=tokenizer)\n",
    "    visualizer(text=text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3**: Download the pre-trained tokenizer file “hopper.json” used in the lecture, from [here](https://drive.google.com/file/d/1QNnyh8iMN-IqW_h1w8gAMtw09Em7-e1e/view?usp=sharing). The tokenizer was trained on all 70 million samples in the BookCorpus dataset. Tokenize the same input text using this “hopper” tokenizer. How many tokens are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <html>\n",
       "        <head>\n",
       "            <style>\n",
       "                .tokenized-text {\n",
       "    width:100%;\n",
       "    padding:2rem;\n",
       "    max-height: 400px;\n",
       "    overflow-y: auto;\n",
       "    box-sizing:border-box;\n",
       "    line-height:4rem; /* Lots of space between lines */\n",
       "    font-family: \"Roboto Light\", \"Ubuntu Light\", \"Ubuntu\", monospace;\n",
       "    box-shadow: 2px 2px 2px rgba(0,0,0,0.2);\n",
       "    background-color: rgba(0,0,0,0.01);\n",
       "    letter-spacing:2px; /* Give some extra separation between chars */\n",
       "}\n",
       ".non-token{\n",
       "    /* White space and other things the tokenizer ignores*/\n",
       "    white-space: pre;\n",
       "    letter-spacing:4px;\n",
       "    border-top:1px solid #A0A0A0; /* A gentle border on top and bottom makes tabs more ovious*/\n",
       "    border-bottom:1px solid #A0A0A0;\n",
       "    line-height: 1rem;\n",
       "    height: calc(100% - 2px);\n",
       "}\n",
       "\n",
       ".token {\n",
       "    white-space: pre;\n",
       "    position:relative;\n",
       "    color:black;\n",
       "    letter-spacing:2px;\n",
       "}\n",
       "\n",
       ".annotation{\n",
       "    white-space:nowrap; /* Important - ensures that annotations appears even if the annotated text wraps a line */\n",
       "    border-radius:4px;\n",
       "    position:relative;\n",
       "    width:fit-content;\n",
       "}\n",
       ".annotation:before {\n",
       "    /*The before holds the text and the after holds the background*/\n",
       "    z-index:1000; /* Make sure this is above the background */\n",
       "    content:attr(data-label); /* The annotations label is on a data attribute */\n",
       "    color:white;\n",
       "    position:absolute;\n",
       "    font-size:1rem;\n",
       "    text-align:center;\n",
       "    font-weight:bold;\n",
       "\n",
       "    top:1.75rem;\n",
       "    line-height:0;\n",
       "    left:0;\n",
       "    width:100%;\n",
       "    padding:0.5rem 0;\n",
       "    /* These make it so an annotation doesn't stretch beyond the annotated text if the label is longer*/\n",
       "    overflow: hidden;\n",
       "    white-space: nowrap;\n",
       "    text-overflow:ellipsis;\n",
       "}\n",
       "\n",
       ".annotation:after {\n",
       "    content:attr(data-label); /* The content defines the width of the annotation*/\n",
       "    position:absolute;\n",
       "    font-size:0.75rem;\n",
       "    text-align:center;\n",
       "    font-weight:bold;\n",
       "    text-overflow:ellipsis;\n",
       "    top:1.75rem;\n",
       "    line-height:0;\n",
       "    overflow: hidden;\n",
       "    white-space: nowrap;\n",
       "\n",
       "    left:0;\n",
       "    width:100%; /* 100% of the parent, which is the annotation whose width is the tokens inside it*/\n",
       "\n",
       "    padding:0.5rem 0;\n",
       "    /* Nast hack below:\n",
       "    We set the annotations color in code because we don't know the colors at css time.\n",
       "    But you can't pass a color as a data attribute to get it into the pseudo element (this thing)\n",
       "    So to get around that, annotations have the color set on them with a style attribute and then we\n",
       "    can get the color with currentColor.\n",
       "    Annotations wrap tokens and tokens set the color back to black\n",
       "     */\n",
       "    background-color: currentColor;\n",
       "}\n",
       ".annotation:hover::after, .annotation:hover::before{\n",
       "    /* When the user hovers over an annotation expand the label to display in full\n",
       "     */\n",
       "    min-width: fit-content;\n",
       "}\n",
       "\n",
       ".annotation:hover{\n",
       "    /* Emphasize the annotation start end with a border on hover*/\n",
       "    border-color: currentColor;\n",
       "    border: 2px solid;\n",
       "}\n",
       ".special-token:not(:empty){\n",
       "    /*\n",
       "    A none empty special token is like UNK (as opposed to CLS which has no representation in the text )\n",
       "     */\n",
       "    position:relative;\n",
       "}\n",
       ".special-token:empty::before{\n",
       "    /* Special tokens that don't have text are displayed as pseudo elements so we dont select them with the mouse*/\n",
       "    content:attr(data-stok);\n",
       "    background:#202020;\n",
       "    font-size:0.75rem;\n",
       "    color:white;\n",
       "    margin: 0 0.25rem;\n",
       "    padding: 0.25rem;\n",
       "    border-radius:4px\n",
       "}\n",
       "\n",
       ".special-token:not(:empty):before {\n",
       "    /* Special tokens that have text (UNK) are displayed above the actual text*/\n",
       "    content:attr(data-stok);\n",
       "    position:absolute;\n",
       "    bottom:1.75rem;\n",
       "    min-width:100%;\n",
       "    width:100%;\n",
       "    height:1rem;\n",
       "    line-height:1rem;\n",
       "    font-size:1rem;\n",
       "    text-align:center;\n",
       "    color:white;\n",
       "    font-weight:bold;\n",
       "    background:#202020;\n",
       "    border-radius:10%;\n",
       "}\n",
       "/*\n",
       "We want to alternate the color of tokens, but we can't use nth child because tokens might be broken up by annotations\n",
       "instead we apply even and odd class at generation time and color them that way\n",
       " */\n",
       ".even-token{\n",
       "    background:#DCDCDC\t;\n",
       "    border: 1px solid #DCDCDC;\n",
       "}\n",
       ".odd-token{\n",
       "    background:#A0A0A0;\n",
       "    border: 1px solid #A0A0A0;\n",
       "}\n",
       ".even-token.multi-token,.odd-token.multi-token{\n",
       "    background:  repeating-linear-gradient(\n",
       "    45deg,\n",
       "    transparent,\n",
       "    transparent 1px,\n",
       "    #ccc 1px,\n",
       "    #ccc 1px\n",
       "    ),\n",
       "    /* on \"bottom\" */\n",
       "    linear-gradient(\n",
       "    to bottom,\n",
       "    #FFB6C1,\n",
       "    #999\n",
       "    );\n",
       "}\n",
       "\n",
       ".multi-token:hover::after {\n",
       "    content:\"This char has more than 1 token\"; /* The content defines the width of the annotation*/\n",
       "    color:white;\n",
       "    background-color: black;\n",
       "    position:absolute;\n",
       "    font-size:0.75rem;\n",
       "    text-align:center;\n",
       "    font-weight:bold;\n",
       "    text-overflow:ellipsis;\n",
       "    top:1.75rem;\n",
       "    line-height:0;\n",
       "    overflow: hidden;\n",
       "    white-space: nowrap;\n",
       "    left:0;\n",
       "    width:fit-content; /* 100% of the parent, which is the annotation whose width is the tokens inside it*/\n",
       "    padding:0.5rem 0;\n",
       "}\n",
       "\n",
       "            </style>\n",
       "        </head>\n",
       "        <body>\n",
       "            <div class=\"tokenized-text\" dir=auto>\n",
       "            <span class=\"token even-token\"  >SEB</span><span class=\"token odd-token\"  >I</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >study</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >finds</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >9</span><span class=\"token odd-token\"  >3</span><span class=\"token even-token\"  >%</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >of</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >individual</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >F</span><span class=\"token even-token\"  >&</span><span class=\"token odd-token\"  >O</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >traders</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >made</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >losses</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >between</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >F</span><span class=\"token odd-token\"  >Y</span><span class=\"token even-token\"  >22</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >and</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >F</span><span class=\"token odd-token\"  >Y</span><span class=\"token even-token\"  >2</span><span class=\"token odd-token\"  >4</span><span class=\"token even-token\"  >.</span>\n",
       "            </div>\n",
       "        </body>\n",
       "    </html>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trained_tokenizer = Tokenizer(BPE())\n",
    "trained_tokenizer = trained_tokenizer.from_file('hopper.json')\n",
    "tokens = trained_tokenizer.encode(text).tokens\n",
    "print(len(tokens))\n",
    "EncodingVisualizer(tokenizer=trained_tokenizer)(text=text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4**: Suppose we know that the acronym “FY” will likely appear very frequently in most of the input text (assume the text comes from the financial domain). Therefore, we hope that adding it manually to the vocabulary might help. Add the token “FY” to the vocabulary and tokenize the input text. Enter the number of tokens produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32000\n",
      "32001\n",
      "22\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <html>\n",
       "        <head>\n",
       "            <style>\n",
       "                .tokenized-text {\n",
       "    width:100%;\n",
       "    padding:2rem;\n",
       "    max-height: 400px;\n",
       "    overflow-y: auto;\n",
       "    box-sizing:border-box;\n",
       "    line-height:4rem; /* Lots of space between lines */\n",
       "    font-family: \"Roboto Light\", \"Ubuntu Light\", \"Ubuntu\", monospace;\n",
       "    box-shadow: 2px 2px 2px rgba(0,0,0,0.2);\n",
       "    background-color: rgba(0,0,0,0.01);\n",
       "    letter-spacing:2px; /* Give some extra separation between chars */\n",
       "}\n",
       ".non-token{\n",
       "    /* White space and other things the tokenizer ignores*/\n",
       "    white-space: pre;\n",
       "    letter-spacing:4px;\n",
       "    border-top:1px solid #A0A0A0; /* A gentle border on top and bottom makes tabs more ovious*/\n",
       "    border-bottom:1px solid #A0A0A0;\n",
       "    line-height: 1rem;\n",
       "    height: calc(100% - 2px);\n",
       "}\n",
       "\n",
       ".token {\n",
       "    white-space: pre;\n",
       "    position:relative;\n",
       "    color:black;\n",
       "    letter-spacing:2px;\n",
       "}\n",
       "\n",
       ".annotation{\n",
       "    white-space:nowrap; /* Important - ensures that annotations appears even if the annotated text wraps a line */\n",
       "    border-radius:4px;\n",
       "    position:relative;\n",
       "    width:fit-content;\n",
       "}\n",
       ".annotation:before {\n",
       "    /*The before holds the text and the after holds the background*/\n",
       "    z-index:1000; /* Make sure this is above the background */\n",
       "    content:attr(data-label); /* The annotations label is on a data attribute */\n",
       "    color:white;\n",
       "    position:absolute;\n",
       "    font-size:1rem;\n",
       "    text-align:center;\n",
       "    font-weight:bold;\n",
       "\n",
       "    top:1.75rem;\n",
       "    line-height:0;\n",
       "    left:0;\n",
       "    width:100%;\n",
       "    padding:0.5rem 0;\n",
       "    /* These make it so an annotation doesn't stretch beyond the annotated text if the label is longer*/\n",
       "    overflow: hidden;\n",
       "    white-space: nowrap;\n",
       "    text-overflow:ellipsis;\n",
       "}\n",
       "\n",
       ".annotation:after {\n",
       "    content:attr(data-label); /* The content defines the width of the annotation*/\n",
       "    position:absolute;\n",
       "    font-size:0.75rem;\n",
       "    text-align:center;\n",
       "    font-weight:bold;\n",
       "    text-overflow:ellipsis;\n",
       "    top:1.75rem;\n",
       "    line-height:0;\n",
       "    overflow: hidden;\n",
       "    white-space: nowrap;\n",
       "\n",
       "    left:0;\n",
       "    width:100%; /* 100% of the parent, which is the annotation whose width is the tokens inside it*/\n",
       "\n",
       "    padding:0.5rem 0;\n",
       "    /* Nast hack below:\n",
       "    We set the annotations color in code because we don't know the colors at css time.\n",
       "    But you can't pass a color as a data attribute to get it into the pseudo element (this thing)\n",
       "    So to get around that, annotations have the color set on them with a style attribute and then we\n",
       "    can get the color with currentColor.\n",
       "    Annotations wrap tokens and tokens set the color back to black\n",
       "     */\n",
       "    background-color: currentColor;\n",
       "}\n",
       ".annotation:hover::after, .annotation:hover::before{\n",
       "    /* When the user hovers over an annotation expand the label to display in full\n",
       "     */\n",
       "    min-width: fit-content;\n",
       "}\n",
       "\n",
       ".annotation:hover{\n",
       "    /* Emphasize the annotation start end with a border on hover*/\n",
       "    border-color: currentColor;\n",
       "    border: 2px solid;\n",
       "}\n",
       ".special-token:not(:empty){\n",
       "    /*\n",
       "    A none empty special token is like UNK (as opposed to CLS which has no representation in the text )\n",
       "     */\n",
       "    position:relative;\n",
       "}\n",
       ".special-token:empty::before{\n",
       "    /* Special tokens that don't have text are displayed as pseudo elements so we dont select them with the mouse*/\n",
       "    content:attr(data-stok);\n",
       "    background:#202020;\n",
       "    font-size:0.75rem;\n",
       "    color:white;\n",
       "    margin: 0 0.25rem;\n",
       "    padding: 0.25rem;\n",
       "    border-radius:4px\n",
       "}\n",
       "\n",
       ".special-token:not(:empty):before {\n",
       "    /* Special tokens that have text (UNK) are displayed above the actual text*/\n",
       "    content:attr(data-stok);\n",
       "    position:absolute;\n",
       "    bottom:1.75rem;\n",
       "    min-width:100%;\n",
       "    width:100%;\n",
       "    height:1rem;\n",
       "    line-height:1rem;\n",
       "    font-size:1rem;\n",
       "    text-align:center;\n",
       "    color:white;\n",
       "    font-weight:bold;\n",
       "    background:#202020;\n",
       "    border-radius:10%;\n",
       "}\n",
       "/*\n",
       "We want to alternate the color of tokens, but we can't use nth child because tokens might be broken up by annotations\n",
       "instead we apply even and odd class at generation time and color them that way\n",
       " */\n",
       ".even-token{\n",
       "    background:#DCDCDC\t;\n",
       "    border: 1px solid #DCDCDC;\n",
       "}\n",
       ".odd-token{\n",
       "    background:#A0A0A0;\n",
       "    border: 1px solid #A0A0A0;\n",
       "}\n",
       ".even-token.multi-token,.odd-token.multi-token{\n",
       "    background:  repeating-linear-gradient(\n",
       "    45deg,\n",
       "    transparent,\n",
       "    transparent 1px,\n",
       "    #ccc 1px,\n",
       "    #ccc 1px\n",
       "    ),\n",
       "    /* on \"bottom\" */\n",
       "    linear-gradient(\n",
       "    to bottom,\n",
       "    #FFB6C1,\n",
       "    #999\n",
       "    );\n",
       "}\n",
       "\n",
       ".multi-token:hover::after {\n",
       "    content:\"This char has more than 1 token\"; /* The content defines the width of the annotation*/\n",
       "    color:white;\n",
       "    background-color: black;\n",
       "    position:absolute;\n",
       "    font-size:0.75rem;\n",
       "    text-align:center;\n",
       "    font-weight:bold;\n",
       "    text-overflow:ellipsis;\n",
       "    top:1.75rem;\n",
       "    line-height:0;\n",
       "    overflow: hidden;\n",
       "    white-space: nowrap;\n",
       "    left:0;\n",
       "    width:fit-content; /* 100% of the parent, which is the annotation whose width is the tokens inside it*/\n",
       "    padding:0.5rem 0;\n",
       "}\n",
       "\n",
       "            </style>\n",
       "        </head>\n",
       "        <body>\n",
       "            <div class=\"tokenized-text\" dir=auto>\n",
       "            <span class=\"token even-token\"  >SEB</span><span class=\"token odd-token\"  >I</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >study</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >finds</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >9</span><span class=\"token odd-token\"  >3</span><span class=\"token even-token\"  >%</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >of</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >individual</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >F</span><span class=\"token even-token\"  >&</span><span class=\"token odd-token\"  >O</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >traders</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >made</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >losses</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >between</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >FY</span><span class=\"token odd-token\"  >22</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >and</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >FY</span><span class=\"token even-token\"  >24</span><span class=\"token odd-token\"  >.</span>\n",
       "            </div>\n",
       "        </body>\n",
       "    </html>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(trained_tokenizer.get_vocab_size())\n",
    "trained_tokenizer.add_tokens(['FY'])\n",
    "print(trained_tokenizer.get_vocab_size())\n",
    "tokens = trained_tokenizer.encode(text).tokens\n",
    "print(len(tokens))\n",
    "EncodingVisualizer(tokenizer=trained_tokenizer)(text=text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5** Load the “bert-base-uncased” and \"gpt2” tokenizers (use AutoTokenizer function from transformers). Which of the following special tokens are used in these tokenizers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bbu special tokens - ['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']\n",
      "gpt2 special tokens - ['<|endoftext|>']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "print(f\"bbu special tokens - {bert_tokenizer.all_special_tokens}\")\n",
    "print(f\"gpt2 special tokens - {gpt2_tokenizer.all_special_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6** By now, we have four tokenizers. <br>\n",
    "\n",
    "1. Custom tokenizer (vocab size 32K, trained on 10 million samples) <br>\n",
    "2. bert-base-uncased <br>\n",
    "3. gpt2 <br>\n",
    "4. hopper <br>\n",
    "\n",
    "Use these four tokenizers to count the number of tokens for the entire “imdb” dataset (drop the “unsupervised” part of the dataset). Enter the tokenizers in order such that the size of the dataset (measured in tokens) as returned by the tokenizers is in decreasing order. For example, if the first tokenizer yields the smallest number of tokens and the fourth tokenizer yields the largest, you would enter 1234 (without any spaces).”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\dell\\.cache\\huggingface\\hub\\datasets--stanfordnlp--imdb. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Generating train split: 100%|██████████| 25000/25000 [00:00<00:00, 149950.81 examples/s]\n",
      "Generating test split: 100%|██████████| 25000/25000 [00:00<00:00, 363102.96 examples/s]\n",
      "Generating unsupervised split: 100%|██████████| 50000/50000 [00:00<00:00, 364136.93 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 50000\n",
       "})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "hopper_tokenizer = Tokenizer(BPE())\n",
    "hopper_tokenizer = trained_tokenizer.from_file('hopper.json')\n",
    "\n",
    "imdb = load_dataset(\"stanfordnlp/imdb\", split='train+test')\n",
    "imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15352840\n",
      "13526933\n"
     ]
    }
   ],
   "source": [
    "for t in [tokenizer, hopper_tokenizer]:\n",
    "    num_tokens =0\n",
    "    for sample in imdb:\n",
    "        tokens = t.encode(sample['text']).tokens\n",
    "        num_tokens += len(tokens)\n",
    "    print(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50000 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (720 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 50000/50000 [00:48<00:00, 1025.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15516058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50000 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1168 > 1024). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 50000/50000 [00:50<00:00, 998.79it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14812432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for t in [bert_tokenizer, gpt2_tokenizer]:\n",
    "    num_tokens = 0\n",
    "    for sample in tqdm(imdb, total= len(imdb)):\n",
    "        token_ids = t(sample['text'])['input_ids']\n",
    "        num_tokens += len(token_ids)\n",
    "    print(num_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
